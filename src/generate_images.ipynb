{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from utils import weights_init, compute_acc\n",
    "from network import _netG, _netD, _netD_CIFAR10, _netG_CIFAR10\n",
    "from folder import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  6747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py27/lib/python2.7/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar10'\n",
    "dataroot = 'data'\n",
    "batchSize = 64\n",
    "workers = 2\n",
    "imageSize = 32\n",
    "nz = 110\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "niter = 200\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "cuda = True\n",
    "netG_ckpt = './output2/netG.pth'\n",
    "netD_ckpt = './output2/netD.pth'\n",
    "num_classes = 10\n",
    "outf = './output2'\n",
    "manualSeed = None\n",
    "ngpu = 1\n",
    "\n",
    "manualSeed = None\n",
    "\n",
    "try:\n",
    "    os.makedirs(outf)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if manualSeed is None:\n",
    "    manualSeed = random.randint(1, 10000)\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed_all(manualSeed)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "\n",
    "\n",
    "dataset = dset.CIFAR10(root=dataroot, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Scale(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "\n",
    "assert dataset\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=int(workers))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = _netG_CIFAR10(ngpu, nz)\n",
    "netG.apply(weights_init)\n",
    "if netG_ckpt != '':\n",
    "    netG.load_state_dict(torch.load(netG_ckpt))\n",
    "\n",
    "netD = _netD_CIFAR10(ngpu, num_classes)\n",
    "netD.apply(weights_init)\n",
    "if netD_ckpt != '':\n",
    "    netD.load_state_dict(torch.load(netD_ckpt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss functions\n",
    "dis_criterion = nn.BCELoss()\n",
    "aux_criterion = nn.NLLLoss()\n",
    "\n",
    "# tensor placeholders\n",
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "eval_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "dis_label = torch.FloatTensor(batchSize)\n",
    "aux_label = torch.LongTensor(batchSize)\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "netD.cuda()\n",
    "netG.cuda()\n",
    "dis_criterion.cuda()\n",
    "aux_criterion.cuda()\n",
    "input, dis_label, aux_label = input.cuda(), dis_label.cuda(), aux_label.cuda()\n",
    "noise, eval_noise = noise.cuda(), eval_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables\n",
    "input = Variable(input)\n",
    "noise = Variable(noise)\n",
    "eval_noise = Variable(eval_noise)\n",
    "dis_label = Variable(dis_label)\n",
    "aux_label = Variable(aux_label)\n",
    "# noise for evaluation\n",
    "eval_noise_ = np.random.normal(0, 1, (batchSize, nz))\n",
    "eval_label = np.random.randint(0, num_classes, batchSize)\n",
    "eval_onehot = np.zeros((batchSize, num_classes))\n",
    "eval_onehot[np.arange(batchSize), eval_label] = 1\n",
    "eval_noise_[np.arange(batchSize), :num_classes] = eval_onehot[np.arange(batchSize)]\n",
    "eval_noise_ = (torch.from_numpy(eval_noise_))\n",
    "with torch.no_grad():\n",
    "    eval_noise.data.copy_(eval_noise_.view(batchSize, nz, 1, 1))\n",
    "    \n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "avg_loss_D = 0.0\n",
    "avg_loss_G = 0.0\n",
    "avg_loss_A = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "network.py:276: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  classes = self.softmax(fc_aux)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][0/782] Loss_D: 1.2223 (1.2223) Loss_G: 0.6493 (0.6493) D(x): 0.4839 D(G(z)): 0.4740 / 0.4854 Acc: 10.9375 (10.9375)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][1/782] Loss_D: 1.3637 (1.2930) Loss_G: 0.6573 (0.6533) D(x): 0.4432 D(G(z)): 0.4933 / 0.4897 Acc: 14.0625 (12.5000)\n",
      "[0/200][2/782] Loss_D: 1.2296 (1.2718) Loss_G: 0.5942 (0.6336) D(x): 0.4999 D(G(z)): 0.4706 / 0.5185 Acc: 6.2500 (10.4167)\n",
      "[0/200][3/782] Loss_D: 1.2018 (1.2543) Loss_G: 0.6878 (0.6471) D(x): 0.5381 D(G(z)): 0.5113 / 0.4740 Acc: 10.9375 (10.5469)\n",
      "[0/200][4/782] Loss_D: 1.1671 (1.2369) Loss_G: 0.7248 (0.6627) D(x): 0.5110 D(G(z)): 0.4725 / 0.4597 Acc: 7.8125 (10.0000)\n",
      "[0/200][5/782] Loss_D: 1.1239 (1.2181) Loss_G: 0.7567 (0.6783) D(x): 0.5155 D(G(z)): 0.4570 / 0.4460 Acc: 10.9375 (10.1562)\n",
      "[0/200][6/782] Loss_D: 1.2276 (1.2194) Loss_G: 0.7516 (0.6888) D(x): 0.4924 D(G(z)): 0.4702 / 0.4505 Acc: 6.2500 (9.5982)\n",
      "[0/200][7/782] Loss_D: 1.2063 (1.2178) Loss_G: 0.6965 (0.6897) D(x): 0.4835 D(G(z)): 0.4568 / 0.4744 Acc: 14.0625 (10.1562)\n",
      "[0/200][8/782] Loss_D: 1.1445 (1.2096) Loss_G: 0.6849 (0.6892) D(x): 0.5441 D(G(z)): 0.4953 / 0.4634 Acc: 18.7500 (11.1111)\n",
      "[0/200][9/782] Loss_D: 1.1079 (1.1995) Loss_G: 0.7485 (0.6951) D(x): 0.5145 D(G(z)): 0.4444 / 0.4490 Acc: 9.3750 (10.9375)\n",
      "[0/200][10/782] Loss_D: 1.1417 (1.1942) Loss_G: 0.8255 (0.7070) D(x): 0.5040 D(G(z)): 0.4466 / 0.4226 Acc: 14.0625 (11.2216)\n",
      "[0/200][11/782] Loss_D: 1.1975 (1.1945) Loss_G: 0.7679 (0.7121) D(x): 0.5133 D(G(z)): 0.4742 / 0.4399 Acc: 6.2500 (10.8073)\n",
      "[0/200][12/782] Loss_D: 1.1634 (1.1921) Loss_G: 0.7324 (0.7136) D(x): 0.5233 D(G(z)): 0.4692 / 0.4544 Acc: 4.6875 (10.3365)\n",
      "[0/200][13/782] Loss_D: 1.1554 (1.1895) Loss_G: 0.7683 (0.7175) D(x): 0.5158 D(G(z)): 0.4570 / 0.4429 Acc: 9.3750 (10.2679)\n",
      "[0/200][14/782] Loss_D: 1.2759 (1.1952) Loss_G: 0.6819 (0.7152) D(x): 0.4914 D(G(z)): 0.4860 / 0.4836 Acc: 10.9375 (10.3125)\n",
      "[0/200][15/782] Loss_D: 1.1713 (1.1937) Loss_G: 0.7486 (0.7172) D(x): 0.5297 D(G(z)): 0.4829 / 0.4597 Acc: 7.8125 (10.1562)\n",
      "[0/200][16/782] Loss_D: 1.2035 (1.1943) Loss_G: 0.7910 (0.7216) D(x): 0.5095 D(G(z)): 0.4769 / 0.4350 Acc: 10.9375 (10.2022)\n",
      "[0/200][17/782] Loss_D: 1.1771 (1.1934) Loss_G: 0.8465 (0.7285) D(x): 0.5087 D(G(z)): 0.4581 / 0.4134 Acc: 4.6875 (9.8958)\n",
      "[0/200][18/782] Loss_D: 1.1804 (1.1927) Loss_G: 0.7268 (0.7284) D(x): 0.4866 D(G(z)): 0.4484 / 0.4654 Acc: 15.6250 (10.1974)\n",
      "[0/200][19/782] Loss_D: 1.1677 (1.1914) Loss_G: 0.7540 (0.7297) D(x): 0.5261 D(G(z)): 0.4712 / 0.4521 Acc: 10.9375 (10.2344)\n",
      "[0/200][20/782] Loss_D: 1.2047 (1.1921) Loss_G: 0.7975 (0.7329) D(x): 0.4978 D(G(z)): 0.4743 / 0.4226 Acc: 7.8125 (10.1190)\n",
      "[0/200][21/782] Loss_D: 1.1225 (1.1889) Loss_G: 0.7525 (0.7338) D(x): 0.5140 D(G(z)): 0.4413 / 0.4513 Acc: 15.6250 (10.3693)\n",
      "[0/200][22/782] Loss_D: 1.2180 (1.1902) Loss_G: 0.6636 (0.7308) D(x): 0.5114 D(G(z)): 0.4747 / 0.4896 Acc: 9.3750 (10.3261)\n",
      "[0/200][23/782] Loss_D: 1.2665 (1.1933) Loss_G: 0.7337 (0.7309) D(x): 0.4948 D(G(z)): 0.4941 / 0.4504 Acc: 10.9375 (10.3516)\n",
      "[0/200][24/782] Loss_D: 1.1630 (1.1921) Loss_G: 0.7851 (0.7331) D(x): 0.5235 D(G(z)): 0.4807 / 0.4345 Acc: 14.0625 (10.5000)\n",
      "[0/200][25/782] Loss_D: 1.0927 (1.1883) Loss_G: 0.8933 (0.7392) D(x): 0.5438 D(G(z)): 0.4549 / 0.3921 Acc: 10.9375 (10.5168)\n",
      "[0/200][26/782] Loss_D: 1.1001 (1.1850) Loss_G: 0.8618 (0.7438) D(x): 0.5118 D(G(z)): 0.4312 / 0.4123 Acc: 3.1250 (10.2431)\n",
      "[0/200][27/782] Loss_D: 1.2145 (1.1861) Loss_G: 0.7703 (0.7447) D(x): 0.4833 D(G(z)): 0.4644 / 0.4382 Acc: 10.9375 (10.2679)\n",
      "[0/200][28/782] Loss_D: 1.0834 (1.1825) Loss_G: 0.7929 (0.7464) D(x): 0.5298 D(G(z)): 0.4248 / 0.4278 Acc: 3.1250 (10.0216)\n",
      "[0/200][29/782] Loss_D: 1.0453 (1.1780) Loss_G: 0.7327 (0.7459) D(x): 0.5391 D(G(z)): 0.4278 / 0.4575 Acc: 9.3750 (10.0000)\n",
      "[0/200][30/782] Loss_D: 1.1540 (1.1772) Loss_G: 0.7508 (0.7461) D(x): 0.5464 D(G(z)): 0.4954 / 0.4461 Acc: 6.2500 (9.8790)\n",
      "[0/200][31/782] Loss_D: 1.0688 (1.1738) Loss_G: 0.8407 (0.7490) D(x): 0.5129 D(G(z)): 0.4363 / 0.4119 Acc: 18.7500 (10.1562)\n",
      "[0/200][32/782] Loss_D: 1.1136 (1.1720) Loss_G: 0.7963 (0.7505) D(x): 0.5278 D(G(z)): 0.4481 / 0.4276 Acc: 4.6875 (9.9905)\n",
      "[0/200][33/782] Loss_D: 1.0363 (1.1680) Loss_G: 0.7348 (0.7500) D(x): 0.5643 D(G(z)): 0.4528 / 0.4438 Acc: 9.3750 (9.9724)\n",
      "[0/200][34/782] Loss_D: 1.0395 (1.1643) Loss_G: 0.7831 (0.7509) D(x): 0.5466 D(G(z)): 0.4533 / 0.4254 Acc: 7.8125 (9.9107)\n",
      "[0/200][35/782] Loss_D: 1.2455 (1.1666) Loss_G: 0.7578 (0.7511) D(x): 0.4952 D(G(z)): 0.4920 / 0.4368 Acc: 10.9375 (9.9392)\n",
      "[0/200][36/782] Loss_D: 1.1969 (1.1674) Loss_G: 0.7643 (0.7515) D(x): 0.5020 D(G(z)): 0.4606 / 0.4355 Acc: 6.2500 (9.8395)\n",
      "[0/200][37/782] Loss_D: 1.2178 (1.1687) Loss_G: 0.8447 (0.7539) D(x): 0.4883 D(G(z)): 0.4446 / 0.4204 Acc: 7.8125 (9.7862)\n",
      "[0/200][38/782] Loss_D: 1.2125 (1.1699) Loss_G: 0.7524 (0.7539) D(x): 0.5346 D(G(z)): 0.4929 / 0.4520 Acc: 6.2500 (9.6955)\n",
      "[0/200][39/782] Loss_D: 1.1258 (1.1687) Loss_G: 0.7088 (0.7528) D(x): 0.5221 D(G(z)): 0.4544 / 0.4726 Acc: 10.9375 (9.7266)\n",
      "[0/200][40/782] Loss_D: 1.1439 (1.1681) Loss_G: 0.7883 (0.7536) D(x): 0.5038 D(G(z)): 0.4567 / 0.4343 Acc: 15.6250 (9.8704)\n",
      "[0/200][41/782] Loss_D: 1.0897 (1.1663) Loss_G: 0.7209 (0.7529) D(x): 0.5033 D(G(z)): 0.4104 / 0.4576 Acc: 14.0625 (9.9702)\n",
      "[0/200][42/782] Loss_D: 1.1533 (1.1660) Loss_G: 0.8292 (0.7546) D(x): 0.5459 D(G(z)): 0.5158 / 0.4105 Acc: 14.0625 (10.0654)\n",
      "[0/200][43/782] Loss_D: 1.1508 (1.1656) Loss_G: 0.8888 (0.7577) D(x): 0.5187 D(G(z)): 0.4734 / 0.4068 Acc: 18.7500 (10.2628)\n",
      "[0/200][44/782] Loss_D: 1.1180 (1.1646) Loss_G: 0.7981 (0.7586) D(x): 0.5227 D(G(z)): 0.4376 / 0.4325 Acc: 6.2500 (10.1736)\n",
      "[0/200][45/782] Loss_D: 1.1802 (1.1649) Loss_G: 0.8562 (0.7607) D(x): 0.5014 D(G(z)): 0.4677 / 0.4023 Acc: 15.6250 (10.2921)\n",
      "[0/200][46/782] Loss_D: 1.1468 (1.1645) Loss_G: 0.7746 (0.7610) D(x): 0.4816 D(G(z)): 0.4078 / 0.4496 Acc: 7.8125 (10.2394)\n",
      "[0/200][47/782] Loss_D: 1.1824 (1.1649) Loss_G: 0.8031 (0.7619) D(x): 0.5166 D(G(z)): 0.4631 / 0.4350 Acc: 10.9375 (10.2539)\n",
      "[0/200][48/782] Loss_D: 1.1478 (1.1645) Loss_G: 0.8357 (0.7634) D(x): 0.5549 D(G(z)): 0.5035 / 0.4136 Acc: 9.3750 (10.2360)\n",
      "[0/200][49/782] Loss_D: 1.1804 (1.1649) Loss_G: 0.8456 (0.7650) D(x): 0.5053 D(G(z)): 0.4646 / 0.4087 Acc: 12.5000 (10.2813)\n",
      "[0/200][50/782] Loss_D: 1.1119 (1.1638) Loss_G: 0.8710 (0.7671) D(x): 0.5172 D(G(z)): 0.4225 / 0.4047 Acc: 4.6875 (10.1716)\n",
      "[0/200][51/782] Loss_D: 1.1350 (1.1633) Loss_G: 0.9774 (0.7712) D(x): 0.5040 D(G(z)): 0.4530 / 0.3662 Acc: 18.7500 (10.3365)\n",
      "[0/200][52/782] Loss_D: 1.1868 (1.1637) Loss_G: 0.8578 (0.7728) D(x): 0.4933 D(G(z)): 0.4492 / 0.4204 Acc: 15.6250 (10.4363)\n",
      "[0/200][53/782] Loss_D: 1.1658 (1.1638) Loss_G: 0.8112 (0.7735) D(x): 0.5353 D(G(z)): 0.4722 / 0.4221 Acc: 12.5000 (10.4745)\n",
      "[0/200][54/782] Loss_D: 1.0440 (1.1616) Loss_G: 0.8879 (0.7756) D(x): 0.5390 D(G(z)): 0.4412 / 0.3933 Acc: 12.5000 (10.5114)\n",
      "[0/200][55/782] Loss_D: 1.0251 (1.1591) Loss_G: 0.8351 (0.7766) D(x): 0.5217 D(G(z)): 0.4238 / 0.4069 Acc: 14.0625 (10.5748)\n",
      "[0/200][56/782] Loss_D: 1.0519 (1.1573) Loss_G: 0.9355 (0.7794) D(x): 0.5194 D(G(z)): 0.4250 / 0.3900 Acc: 20.3125 (10.7456)\n",
      "[0/200][57/782] Loss_D: 1.0674 (1.1557) Loss_G: 0.9533 (0.7824) D(x): 0.5490 D(G(z)): 0.4517 / 0.3757 Acc: 14.0625 (10.8028)\n",
      "[0/200][58/782] Loss_D: 1.0863 (1.1545) Loss_G: 0.9962 (0.7861) D(x): 0.5247 D(G(z)): 0.4365 / 0.3680 Acc: 17.1875 (10.9110)\n",
      "[0/200][59/782] Loss_D: 0.9677 (1.1514) Loss_G: 0.9438 (0.7887) D(x): 0.5472 D(G(z)): 0.4012 / 0.3821 Acc: 14.0625 (10.9635)\n",
      "[0/200][60/782] Loss_D: 1.0069 (1.1491) Loss_G: 0.9401 (0.7912) D(x): 0.4951 D(G(z)): 0.3692 / 0.3835 Acc: 10.9375 (10.9631)\n",
      "[0/200][61/782] Loss_D: 0.9868 (1.1464) Loss_G: 0.8934 (0.7928) D(x): 0.5462 D(G(z)): 0.4027 / 0.3937 Acc: 15.6250 (11.0383)\n",
      "[0/200][62/782] Loss_D: 0.9547 (1.1434) Loss_G: 1.0255 (0.7965) D(x): 0.5777 D(G(z)): 0.4146 / 0.3641 Acc: 10.9375 (11.0367)\n",
      "[0/200][63/782] Loss_D: 0.8492 (1.1388) Loss_G: 1.2202 (0.8031) D(x): 0.5842 D(G(z)): 0.4058 / 0.2964 Acc: 21.8750 (11.2061)\n",
      "[0/200][64/782] Loss_D: 0.8258 (1.1340) Loss_G: 1.1295 (0.8081) D(x): 0.5603 D(G(z)): 0.3573 / 0.3205 Acc: 12.5000 (11.2260)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][65/782] Loss_D: 0.9368 (1.1310) Loss_G: 1.0523 (0.8118) D(x): 0.5278 D(G(z)): 0.3662 / 0.3514 Acc: 18.7500 (11.3400)\n",
      "[0/200][66/782] Loss_D: 0.8762 (1.1272) Loss_G: 1.1791 (0.8173) D(x): 0.5941 D(G(z)): 0.3805 / 0.3176 Acc: 9.3750 (11.3106)\n",
      "[0/200][67/782] Loss_D: 0.8134 (1.1226) Loss_G: 1.0396 (0.8206) D(x): 0.5865 D(G(z)): 0.3730 / 0.3596 Acc: 20.3125 (11.4430)\n",
      "[0/200][68/782] Loss_D: 0.9311 (1.1198) Loss_G: 1.1486 (0.8253) D(x): 0.6144 D(G(z)): 0.4271 / 0.3153 Acc: 10.9375 (11.4357)\n",
      "[0/200][69/782] Loss_D: 1.1190 (1.1198) Loss_G: 1.1575 (0.8301) D(x): 0.5186 D(G(z)): 0.3948 / 0.3216 Acc: 9.3750 (11.4063)\n",
      "[0/200][70/782] Loss_D: 1.0520 (1.1188) Loss_G: 1.1093 (0.8340) D(x): 0.5210 D(G(z)): 0.3927 / 0.3423 Acc: 12.5000 (11.4217)\n",
      "[0/200][71/782] Loss_D: 0.8930 (1.1157) Loss_G: 1.0759 (0.8374) D(x): 0.5709 D(G(z)): 0.3995 / 0.3490 Acc: 15.6250 (11.4800)\n",
      "[0/200][72/782] Loss_D: 1.0797 (1.1152) Loss_G: 1.1654 (0.8419) D(x): 0.5296 D(G(z)): 0.4170 / 0.3152 Acc: 12.5000 (11.4940)\n",
      "[0/200][73/782] Loss_D: 1.0614 (1.1145) Loss_G: 1.2949 (0.8480) D(x): 0.4998 D(G(z)): 0.3847 / 0.2896 Acc: 14.0625 (11.5287)\n",
      "[0/200][74/782] Loss_D: 0.9372 (1.1121) Loss_G: 1.1454 (0.8520) D(x): 0.5279 D(G(z)): 0.3791 / 0.3310 Acc: 23.4375 (11.6875)\n",
      "[0/200][75/782] Loss_D: 0.9724 (1.1103) Loss_G: 1.1261 (0.8556) D(x): 0.5424 D(G(z)): 0.3790 / 0.3299 Acc: 9.3750 (11.6571)\n",
      "[0/200][76/782] Loss_D: 0.8616 (1.1070) Loss_G: 1.1603 (0.8595) D(x): 0.5649 D(G(z)): 0.3612 / 0.3318 Acc: 18.7500 (11.7492)\n",
      "[0/200][77/782] Loss_D: 0.8235 (1.1034) Loss_G: 1.4423 (0.8670) D(x): 0.5925 D(G(z)): 0.3609 / 0.2539 Acc: 15.6250 (11.7989)\n",
      "[0/200][78/782] Loss_D: 0.6831 (1.0981) Loss_G: 1.4567 (0.8745) D(x): 0.6037 D(G(z)): 0.2925 / 0.2475 Acc: 14.0625 (11.8275)\n",
      "[0/200][79/782] Loss_D: 0.7832 (1.0942) Loss_G: 1.4230 (0.8813) D(x): 0.5668 D(G(z)): 0.3218 / 0.2564 Acc: 20.3125 (11.9336)\n",
      "[0/200][80/782] Loss_D: 0.7826 (1.0903) Loss_G: 1.5085 (0.8891) D(x): 0.6105 D(G(z)): 0.3626 / 0.2369 Acc: 18.7500 (12.0177)\n",
      "[0/200][81/782] Loss_D: 0.6233 (1.0846) Loss_G: 1.4061 (0.8954) D(x): 0.6016 D(G(z)): 0.2924 / 0.2485 Acc: 20.3125 (12.1189)\n",
      "[0/200][82/782] Loss_D: 0.5950 (1.0787) Loss_G: 1.5236 (0.9029) D(x): 0.6391 D(G(z)): 0.2889 / 0.2197 Acc: 17.1875 (12.1800)\n",
      "[0/200][83/782] Loss_D: 0.7577 (1.0749) Loss_G: 1.4892 (0.9099) D(x): 0.6317 D(G(z)): 0.3404 / 0.2462 Acc: 12.5000 (12.1838)\n",
      "[0/200][84/782] Loss_D: 0.6739 (1.0702) Loss_G: 1.4929 (0.9168) D(x): 0.6280 D(G(z)): 0.3354 / 0.2411 Acc: 20.3125 (12.2794)\n",
      "[0/200][85/782] Loss_D: 0.7862 (1.0669) Loss_G: 1.2530 (0.9207) D(x): 0.6146 D(G(z)): 0.3527 / 0.3083 Acc: 14.0625 (12.3001)\n",
      "[0/200][86/782] Loss_D: 0.7440 (1.0632) Loss_G: 1.5278 (0.9277) D(x): 0.6348 D(G(z)): 0.3635 / 0.2377 Acc: 14.0625 (12.3204)\n",
      "[0/200][87/782] Loss_D: 0.8179 (1.0604) Loss_G: 1.5857 (0.9351) D(x): 0.6293 D(G(z)): 0.3582 / 0.2288 Acc: 10.9375 (12.3047)\n",
      "[0/200][88/782] Loss_D: 0.8699 (1.0582) Loss_G: 1.6506 (0.9432) D(x): 0.5828 D(G(z)): 0.3622 / 0.2146 Acc: 10.9375 (12.2893)\n",
      "[0/200][89/782] Loss_D: 0.8596 (1.0560) Loss_G: 1.6116 (0.9506) D(x): 0.6098 D(G(z)): 0.3693 / 0.2225 Acc: 10.9375 (12.2743)\n",
      "[0/200][90/782] Loss_D: 0.9483 (1.0548) Loss_G: 2.0095 (0.9622) D(x): 0.6263 D(G(z)): 0.4357 / 0.1595 Acc: 12.5000 (12.2768)\n",
      "[0/200][91/782] Loss_D: 1.1082 (1.0554) Loss_G: 1.7996 (0.9713) D(x): 0.4843 D(G(z)): 0.3083 / 0.1989 Acc: 15.6250 (12.3132)\n",
      "[0/200][92/782] Loss_D: 1.3559 (1.0587) Loss_G: 1.7187 (0.9794) D(x): 0.4285 D(G(z)): 0.3984 / 0.2052 Acc: 15.6250 (12.3488)\n",
      "[0/200][93/782] Loss_D: 1.2326 (1.0605) Loss_G: 1.7117 (0.9872) D(x): 0.5227 D(G(z)): 0.4587 / 0.1925 Acc: 14.0625 (12.3670)\n",
      "[0/200][94/782] Loss_D: 0.9758 (1.0596) Loss_G: 2.2271 (1.0002) D(x): 0.5071 D(G(z)): 0.3436 / 0.1420 Acc: 23.4375 (12.4836)\n",
      "[0/200][95/782] Loss_D: 0.8666 (1.0576) Loss_G: 1.8691 (1.0093) D(x): 0.4768 D(G(z)): 0.2246 / 0.1731 Acc: 17.1875 (12.5326)\n",
      "[0/200][96/782] Loss_D: 0.6832 (1.0537) Loss_G: 1.6788 (1.0162) D(x): 0.5490 D(G(z)): 0.2113 / 0.2001 Acc: 17.1875 (12.5805)\n",
      "[0/200][97/782] Loss_D: 0.4630 (1.0477) Loss_G: 1.9382 (1.0256) D(x): 0.6627 D(G(z)): 0.2445 / 0.1644 Acc: 23.4375 (12.6913)\n",
      "[0/200][98/782] Loss_D: 0.4481 (1.0417) Loss_G: 2.1212 (1.0367) D(x): 0.6827 D(G(z)): 0.2062 / 0.1509 Acc: 15.6250 (12.7210)\n",
      "[0/200][99/782] Loss_D: 0.2972 (1.0342) Loss_G: 2.1479 (1.0478) D(x): 0.7219 D(G(z)): 0.1865 / 0.1559 Acc: 29.6875 (12.8906)\n",
      "[0/200][100/782] Loss_D: 0.3992 (1.0279) Loss_G: 2.1831 (1.0590) D(x): 0.7500 D(G(z)): 0.1839 / 0.1514 Acc: 12.5000 (12.8868)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][101/782] Loss_D: 0.4500 (1.0223) Loss_G: 1.9045 (1.0673) D(x): 0.7400 D(G(z)): 0.2246 / 0.1759 Acc: 15.6250 (12.9136)\n",
      "[0/200][102/782] Loss_D: 0.6530 (1.0187) Loss_G: 2.0345 (1.0767) D(x): 0.7376 D(G(z)): 0.3484 / 0.1863 Acc: 10.9375 (12.8944)\n",
      "[0/200][103/782] Loss_D: 0.9365 (1.0179) Loss_G: 1.9984 (1.0855) D(x): 0.7169 D(G(z)): 0.4431 / 0.2086 Acc: 12.5000 (12.8906)\n",
      "[0/200][104/782] Loss_D: 1.1882 (1.0195) Loss_G: 1.4560 (1.0891) D(x): 0.6865 D(G(z)): 0.5340 / 0.3072 Acc: 18.7500 (12.9464)\n",
      "[0/200][105/782] Loss_D: 1.5995 (1.0250) Loss_G: 1.9321 (1.0970) D(x): 0.5882 D(G(z)): 0.6048 / 0.2086 Acc: 14.0625 (12.9570)\n",
      "[0/200][106/782] Loss_D: 1.0533 (1.0252) Loss_G: 2.1712 (1.1071) D(x): 0.5006 D(G(z)): 0.3337 / 0.1373 Acc: 21.8750 (13.0403)\n",
      "[0/200][107/782] Loss_D: 1.0755 (1.0257) Loss_G: 1.9750 (1.1151) D(x): 0.4521 D(G(z)): 0.2692 / 0.1580 Acc: 20.3125 (13.1076)\n",
      "[0/200][108/782] Loss_D: 1.1338 (1.0267) Loss_G: 1.5029 (1.1187) D(x): 0.4314 D(G(z)): 0.2479 / 0.2454 Acc: 28.1250 (13.2454)\n",
      "[0/200][109/782] Loss_D: 0.5133 (1.0220) Loss_G: 1.4724 (1.1219) D(x): 0.6429 D(G(z)): 0.2803 / 0.2490 Acc: 23.4375 (13.3381)\n",
      "[0/200][110/782] Loss_D: 0.6720 (1.0189) Loss_G: 1.6212 (1.1264) D(x): 0.6978 D(G(z)): 0.3892 / 0.2303 Acc: 15.6250 (13.3587)\n",
      "[0/200][111/782] Loss_D: 0.4586 (1.0139) Loss_G: 1.7797 (1.1322) D(x): 0.6644 D(G(z)): 0.2353 / 0.1976 Acc: 15.6250 (13.3789)\n",
      "[0/200][112/782] Loss_D: 0.6099 (1.0103) Loss_G: 1.6932 (1.1372) D(x): 0.6825 D(G(z)): 0.3304 / 0.2281 Acc: 15.6250 (13.3988)\n",
      "[0/200][113/782] Loss_D: 0.7016 (1.0076) Loss_G: 1.6220 (1.1414) D(x): 0.6246 D(G(z)): 0.2988 / 0.2384 Acc: 14.0625 (13.4046)\n",
      "[0/200][114/782] Loss_D: 0.8541 (1.0063) Loss_G: 1.1238 (1.1413) D(x): 0.6399 D(G(z)): 0.4317 / 0.3515 Acc: 20.3125 (13.4647)\n",
      "[0/200][115/782] Loss_D: 0.9109 (1.0054) Loss_G: 1.1032 (1.1409) D(x): 0.6290 D(G(z)): 0.4581 / 0.3627 Acc: 15.6250 (13.4833)\n",
      "[0/200][116/782] Loss_D: 1.0308 (1.0057) Loss_G: 1.1919 (1.1414) D(x): 0.5712 D(G(z)): 0.3652 / 0.3536 Acc: 7.8125 (13.4348)\n",
      "[0/200][117/782] Loss_D: 1.0012 (1.0056) Loss_G: 1.0193 (1.1403) D(x): 0.5289 D(G(z)): 0.3686 / 0.3637 Acc: 10.9375 (13.4137)\n",
      "[0/200][118/782] Loss_D: 1.0476 (1.0060) Loss_G: 0.7850 (1.1374) D(x): 0.5835 D(G(z)): 0.4541 / 0.4310 Acc: 18.7500 (13.4585)\n",
      "[0/200][119/782] Loss_D: 1.2332 (1.0079) Loss_G: 0.9081 (1.1354) D(x): 0.5319 D(G(z)): 0.4697 / 0.4132 Acc: 18.7500 (13.5026)\n",
      "[0/200][120/782] Loss_D: 1.0680 (1.0084) Loss_G: 1.0417 (1.1347) D(x): 0.6311 D(G(z)): 0.4714 / 0.3907 Acc: 9.3750 (13.4685)\n",
      "[0/200][121/782] Loss_D: 1.3918 (1.0115) Loss_G: 0.9177 (1.1329) D(x): 0.4315 D(G(z)): 0.4055 / 0.4331 Acc: 18.7500 (13.5118)\n",
      "[0/200][122/782] Loss_D: 1.0160 (1.0115) Loss_G: 0.7906 (1.1301) D(x): 0.6160 D(G(z)): 0.5079 / 0.4585 Acc: 17.1875 (13.5417)\n",
      "[0/200][123/782] Loss_D: 1.0365 (1.0117) Loss_G: 0.8145 (1.1276) D(x): 0.5476 D(G(z)): 0.4686 / 0.4384 Acc: 17.1875 (13.5711)\n",
      "[0/200][124/782] Loss_D: 0.9598 (1.0113) Loss_G: 0.8352 (1.1252) D(x): 0.5826 D(G(z)): 0.4487 / 0.4446 Acc: 26.5625 (13.6750)\n",
      "[0/200][125/782] Loss_D: 1.0797 (1.0119) Loss_G: 0.8007 (1.1227) D(x): 0.5612 D(G(z)): 0.4529 / 0.4611 Acc: 14.0625 (13.6781)\n",
      "[0/200][126/782] Loss_D: 1.0424 (1.0121) Loss_G: 0.9260 (1.1211) D(x): 0.5924 D(G(z)): 0.4922 / 0.4355 Acc: 20.3125 (13.7303)\n",
      "[0/200][127/782] Loss_D: 0.7568 (1.0101) Loss_G: 1.0775 (1.1208) D(x): 0.6265 D(G(z)): 0.3867 / 0.3675 Acc: 17.1875 (13.7573)\n",
      "[0/200][128/782] Loss_D: 0.9159 (1.0094) Loss_G: 0.8058 (1.1183) D(x): 0.6231 D(G(z)): 0.4357 / 0.4415 Acc: 12.5000 (13.7476)\n",
      "[0/200][129/782] Loss_D: 0.7423 (1.0073) Loss_G: 0.8402 (1.1162) D(x): 0.6328 D(G(z)): 0.4299 / 0.4366 Acc: 29.6875 (13.8702)\n",
      "[0/200][130/782] Loss_D: 0.9291 (1.0067) Loss_G: 0.8851 (1.1144) D(x): 0.5811 D(G(z)): 0.4102 / 0.4213 Acc: 18.7500 (13.9074)\n",
      "[0/200][131/782] Loss_D: 0.9801 (1.0065) Loss_G: 0.7154 (1.1114) D(x): 0.6422 D(G(z)): 0.4599 / 0.4717 Acc: 15.6250 (13.9205)\n",
      "[0/200][132/782] Loss_D: 1.0552 (1.0069) Loss_G: 0.8843 (1.1097) D(x): 0.6144 D(G(z)): 0.4578 / 0.4326 Acc: 9.3750 (13.8863)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][133/782] Loss_D: 0.9499 (1.0065) Loss_G: 0.8634 (1.1078) D(x): 0.6602 D(G(z)): 0.4886 / 0.4373 Acc: 10.9375 (13.8643)\n",
      "[0/200][134/782] Loss_D: 1.2547 (1.0083) Loss_G: 1.0042 (1.1071) D(x): 0.5721 D(G(z)): 0.4997 / 0.4095 Acc: 10.9375 (13.8426)\n",
      "[0/200][135/782] Loss_D: 1.3457 (1.0108) Loss_G: 0.8752 (1.1054) D(x): 0.4974 D(G(z)): 0.4968 / 0.4416 Acc: 23.4375 (13.9131)\n",
      "[0/200][136/782] Loss_D: 1.2201 (1.0123) Loss_G: 0.5420 (1.1013) D(x): 0.5746 D(G(z)): 0.5308 / 0.5318 Acc: 15.6250 (13.9256)\n",
      "[0/200][137/782] Loss_D: 1.2752 (1.0142) Loss_G: 0.5966 (1.0976) D(x): 0.5112 D(G(z)): 0.4829 / 0.5094 Acc: 12.5000 (13.9153)\n",
      "[0/200][138/782] Loss_D: 1.2931 (1.0162) Loss_G: 0.7624 (1.0952) D(x): 0.5913 D(G(z)): 0.5463 / 0.4796 Acc: 10.9375 (13.8939)\n",
      "[0/200][139/782] Loss_D: 1.3378 (1.0185) Loss_G: 0.6210 (1.0918) D(x): 0.5109 D(G(z)): 0.5345 / 0.4879 Acc: 17.1875 (13.9174)\n",
      "[0/200][140/782] Loss_D: 1.3351 (1.0208) Loss_G: 0.6462 (1.0886) D(x): 0.5119 D(G(z)): 0.5329 / 0.4891 Acc: 17.1875 (13.9406)\n",
      "[0/200][141/782] Loss_D: 1.3323 (1.0230) Loss_G: 0.6508 (1.0856) D(x): 0.5364 D(G(z)): 0.5733 / 0.4894 Acc: 23.4375 (14.0075)\n",
      "[0/200][142/782] Loss_D: 0.9821 (1.0227) Loss_G: 0.9062 (1.0843) D(x): 0.5357 D(G(z)): 0.4696 / 0.4103 Acc: 23.4375 (14.0734)\n",
      "[0/200][143/782] Loss_D: 1.2193 (1.0240) Loss_G: 0.6130 (1.0810) D(x): 0.5331 D(G(z)): 0.5233 / 0.5041 Acc: 17.1875 (14.0951)\n",
      "[0/200][144/782] Loss_D: 1.0901 (1.0245) Loss_G: 0.6702 (1.0782) D(x): 0.5647 D(G(z)): 0.5156 / 0.4791 Acc: 17.1875 (14.1164)\n",
      "[0/200][145/782] Loss_D: 1.4806 (1.0276) Loss_G: 0.6394 (1.0752) D(x): 0.4538 D(G(z)): 0.5137 / 0.5020 Acc: 14.0625 (14.1160)\n",
      "[0/200][146/782] Loss_D: 1.3072 (1.0295) Loss_G: 0.6015 (1.0720) D(x): 0.5825 D(G(z)): 0.5629 / 0.5250 Acc: 14.0625 (14.1156)\n",
      "[0/200][147/782] Loss_D: 1.1236 (1.0302) Loss_G: 0.8065 (1.0702) D(x): 0.5825 D(G(z)): 0.5384 / 0.4528 Acc: 23.4375 (14.1786)\n",
      "[0/200][148/782] Loss_D: 1.1489 (1.0310) Loss_G: 0.5668 (1.0668) D(x): 0.5361 D(G(z)): 0.5406 / 0.5177 Acc: 23.4375 (14.2408)\n",
      "[0/200][149/782] Loss_D: 1.1542 (1.0318) Loss_G: 0.6780 (1.0642) D(x): 0.5582 D(G(z)): 0.4893 / 0.5082 Acc: 17.1875 (14.2604)\n",
      "[0/200][150/782] Loss_D: 1.1924 (1.0328) Loss_G: 0.5640 (1.0609) D(x): 0.5490 D(G(z)): 0.5293 / 0.5236 Acc: 14.0625 (14.2591)\n",
      "[0/200][151/782] Loss_D: 1.2066 (1.0340) Loss_G: 0.5550 (1.0576) D(x): 0.5353 D(G(z)): 0.5365 / 0.5284 Acc: 21.8750 (14.3092)\n",
      "[0/200][152/782] Loss_D: 1.2443 (1.0354) Loss_G: 0.6319 (1.0548) D(x): 0.5631 D(G(z)): 0.5353 / 0.5167 Acc: 23.4375 (14.3689)\n",
      "[0/200][153/782] Loss_D: 1.2346 (1.0367) Loss_G: 0.5134 (1.0513) D(x): 0.5660 D(G(z)): 0.5448 / 0.5541 Acc: 14.0625 (14.3669)\n",
      "[0/200][154/782] Loss_D: 1.4158 (1.0391) Loss_G: 0.7415 (1.0493) D(x): 0.5725 D(G(z)): 0.5917 / 0.5110 Acc: 18.7500 (14.3952)\n",
      "[0/200][155/782] Loss_D: 1.2655 (1.0406) Loss_G: 0.7124 (1.0471) D(x): 0.5468 D(G(z)): 0.5537 / 0.4980 Acc: 18.7500 (14.4231)\n",
      "[0/200][156/782] Loss_D: 1.2429 (1.0418) Loss_G: 0.5653 (1.0440) D(x): 0.5322 D(G(z)): 0.5548 / 0.5257 Acc: 20.3125 (14.4606)\n",
      "[0/200][157/782] Loss_D: 1.2025 (1.0429) Loss_G: 0.5959 (1.0412) D(x): 0.5353 D(G(z)): 0.5452 / 0.5172 Acc: 20.3125 (14.4976)\n",
      "[0/200][158/782] Loss_D: 1.3975 (1.0451) Loss_G: 0.5228 (1.0379) D(x): 0.4438 D(G(z)): 0.4876 / 0.5339 Acc: 10.9375 (14.4752)\n",
      "[0/200][159/782] Loss_D: 1.2494 (1.0464) Loss_G: 0.5463 (1.0349) D(x): 0.5438 D(G(z)): 0.5397 / 0.5479 Acc: 20.3125 (14.5117)\n",
      "[0/200][160/782] Loss_D: 0.9977 (1.0461) Loss_G: 0.5386 (1.0318) D(x): 0.5875 D(G(z)): 0.5462 / 0.5304 Acc: 31.2500 (14.6157)\n",
      "[0/200][161/782] Loss_D: 1.1202 (1.0465) Loss_G: 0.6170 (1.0292) D(x): 0.5529 D(G(z)): 0.5386 / 0.5189 Acc: 23.4375 (14.6701)\n",
      "[0/200][162/782] Loss_D: 1.0999 (1.0468) Loss_G: 0.6067 (1.0266) D(x): 0.5601 D(G(z)): 0.5423 / 0.5020 Acc: 21.8750 (14.7143)\n",
      "[0/200][163/782] Loss_D: 1.0847 (1.0471) Loss_G: 0.5442 (1.0237) D(x): 0.5214 D(G(z)): 0.4956 / 0.5308 Acc: 20.3125 (14.7485)\n",
      "[0/200][164/782] Loss_D: 1.0849 (1.0473) Loss_G: 0.5190 (1.0206) D(x): 0.6051 D(G(z)): 0.5512 / 0.5305 Acc: 17.1875 (14.7633)\n",
      "[0/200][165/782] Loss_D: 1.0952 (1.0476) Loss_G: 0.7105 (1.0188) D(x): 0.5666 D(G(z)): 0.5176 / 0.4862 Acc: 23.4375 (14.8155)\n",
      "[0/200][166/782] Loss_D: 0.9700 (1.0471) Loss_G: 0.6383 (1.0165) D(x): 0.5879 D(G(z)): 0.5120 / 0.4817 Acc: 20.3125 (14.8484)\n",
      "[0/200][167/782] Loss_D: 1.0158 (1.0469) Loss_G: 0.7091 (1.0147) D(x): 0.6016 D(G(z)): 0.5046 / 0.4828 Acc: 17.1875 (14.8624)\n",
      "[0/200][168/782] Loss_D: 1.1045 (1.0473) Loss_G: 0.7405 (1.0130) D(x): 0.5734 D(G(z)): 0.5159 / 0.4563 Acc: 23.4375 (14.9131)\n",
      "[0/200][169/782] Loss_D: 1.1022 (1.0476) Loss_G: 0.6752 (1.0111) D(x): 0.5538 D(G(z)): 0.4771 / 0.4745 Acc: 9.3750 (14.8805)\n",
      "[0/200][170/782] Loss_D: 1.1848 (1.0484) Loss_G: 0.6294 (1.0088) D(x): 0.5294 D(G(z)): 0.5037 / 0.5134 Acc: 21.8750 (14.9214)\n",
      "[0/200][171/782] Loss_D: 1.0901 (1.0487) Loss_G: 0.6490 (1.0067) D(x): 0.5760 D(G(z)): 0.5251 / 0.4995 Acc: 26.5625 (14.9891)\n",
      "[0/200][172/782] Loss_D: 1.0204 (1.0485) Loss_G: 0.7070 (1.0050) D(x): 0.6224 D(G(z)): 0.5079 / 0.4907 Acc: 17.1875 (15.0018)\n",
      "[0/200][173/782] Loss_D: 0.9327 (1.0478) Loss_G: 0.7157 (1.0033) D(x): 0.6182 D(G(z)): 0.5041 / 0.4818 Acc: 26.5625 (15.0682)\n",
      "[0/200][174/782] Loss_D: 1.0574 (1.0479) Loss_G: 0.8163 (1.0023) D(x): 0.5809 D(G(z)): 0.4825 / 0.4378 Acc: 14.0625 (15.0625)\n",
      "[0/200][175/782] Loss_D: 0.9911 (1.0476) Loss_G: 0.7057 (1.0006) D(x): 0.5562 D(G(z)): 0.4797 / 0.4753 Acc: 25.0000 (15.1190)\n",
      "[0/200][176/782] Loss_D: 1.0397 (1.0475) Loss_G: 0.7138 (0.9990) D(x): 0.5943 D(G(z)): 0.5090 / 0.4824 Acc: 17.1875 (15.1306)\n",
      "[0/200][177/782] Loss_D: 0.9806 (1.0471) Loss_G: 0.6566 (0.9970) D(x): 0.6014 D(G(z)): 0.4899 / 0.4944 Acc: 17.1875 (15.1422)\n",
      "[0/200][178/782] Loss_D: 1.0705 (1.0473) Loss_G: 0.6838 (0.9953) D(x): 0.5804 D(G(z)): 0.5007 / 0.4994 Acc: 20.3125 (15.1711)\n",
      "[0/200][179/782] Loss_D: 0.7705 (1.0457) Loss_G: 0.6904 (0.9936) D(x): 0.6270 D(G(z)): 0.4888 / 0.4802 Acc: 29.6875 (15.2517)\n",
      "[0/200][180/782] Loss_D: 1.1178 (1.0461) Loss_G: 0.7395 (0.9922) D(x): 0.5654 D(G(z)): 0.5375 / 0.4515 Acc: 20.3125 (15.2797)\n",
      "[0/200][181/782] Loss_D: 1.1042 (1.0464) Loss_G: 0.7281 (0.9907) D(x): 0.5857 D(G(z)): 0.5118 / 0.4554 Acc: 21.8750 (15.3159)\n",
      "[0/200][182/782] Loss_D: 1.0268 (1.0463) Loss_G: 0.8077 (0.9897) D(x): 0.5589 D(G(z)): 0.4617 / 0.4548 Acc: 17.1875 (15.3262)\n",
      "[0/200][183/782] Loss_D: 0.9956 (1.0461) Loss_G: 0.6755 (0.9880) D(x): 0.5576 D(G(z)): 0.4607 / 0.4685 Acc: 17.1875 (15.3363)\n",
      "[0/200][184/782] Loss_D: 0.8701 (1.0451) Loss_G: 0.5684 (0.9858) D(x): 0.5790 D(G(z)): 0.4827 / 0.4795 Acc: 23.4375 (15.3801)\n",
      "[0/200][185/782] Loss_D: 0.9804 (1.0448) Loss_G: 0.6633 (0.9840) D(x): 0.6111 D(G(z)): 0.5027 / 0.4840 Acc: 17.1875 (15.3898)\n",
      "[0/200][186/782] Loss_D: 0.9456 (1.0442) Loss_G: 0.7473 (0.9828) D(x): 0.5968 D(G(z)): 0.4717 / 0.4877 Acc: 20.3125 (15.4161)\n",
      "[0/200][187/782] Loss_D: 0.7167 (1.0425) Loss_G: 0.8115 (0.9819) D(x): 0.6505 D(G(z)): 0.4813 / 0.4537 Acc: 34.3750 (15.5170)\n",
      "[0/200][188/782] Loss_D: 0.8986 (1.0417) Loss_G: 0.7466 (0.9806) D(x): 0.6012 D(G(z)): 0.4597 / 0.4714 Acc: 25.0000 (15.5671)\n",
      "[0/200][189/782] Loss_D: 0.9595 (1.0413) Loss_G: 0.6264 (0.9787) D(x): 0.5862 D(G(z)): 0.4902 / 0.4745 Acc: 17.1875 (15.5757)\n",
      "[0/200][190/782] Loss_D: 0.9455 (1.0408) Loss_G: 0.7069 (0.9773) D(x): 0.6027 D(G(z)): 0.4791 / 0.4741 Acc: 26.5625 (15.6332)\n",
      "[0/200][191/782] Loss_D: 0.7138 (1.0391) Loss_G: 0.5863 (0.9753) D(x): 0.6293 D(G(z)): 0.4276 / 0.4921 Acc: 20.3125 (15.6576)\n",
      "[0/200][192/782] Loss_D: 0.9842 (1.0388) Loss_G: 0.7531 (0.9741) D(x): 0.6382 D(G(z)): 0.4940 / 0.4462 Acc: 20.3125 (15.6817)\n",
      "[0/200][193/782] Loss_D: 0.8872 (1.0380) Loss_G: 0.6575 (0.9725) D(x): 0.6119 D(G(z)): 0.4593 / 0.4726 Acc: 17.1875 (15.6894)\n",
      "[0/200][194/782] Loss_D: 0.9195 (1.0374) Loss_G: 0.7079 (0.9711) D(x): 0.5972 D(G(z)): 0.4849 / 0.4858 Acc: 26.5625 (15.7452)\n",
      "[0/200][195/782] Loss_D: 0.8934 (1.0367) Loss_G: 0.5874 (0.9692) D(x): 0.5740 D(G(z)): 0.4754 / 0.4925 Acc: 25.0000 (15.7924)\n",
      "[0/200][196/782] Loss_D: 0.9177 (1.0361) Loss_G: 0.6228 (0.9674) D(x): 0.6293 D(G(z)): 0.4995 / 0.4941 Acc: 21.8750 (15.8233)\n",
      "[0/200][197/782] Loss_D: 0.9769 (1.0358) Loss_G: 0.7067 (0.9661) D(x): 0.6312 D(G(z)): 0.4908 / 0.4832 Acc: 18.7500 (15.8381)\n",
      "[0/200][198/782] Loss_D: 0.8718 (1.0350) Loss_G: 0.5694 (0.9641) D(x): 0.6206 D(G(z)): 0.4822 / 0.4950 Acc: 15.6250 (15.8370)\n",
      "[0/200][199/782] Loss_D: 1.0462 (1.0350) Loss_G: 0.6379 (0.9625) D(x): 0.5747 D(G(z)): 0.4927 / 0.4866 Acc: 20.3125 (15.8594)\n",
      "[0/200][200/782] Loss_D: 0.9612 (1.0346) Loss_G: 0.7026 (0.9612) D(x): 0.6053 D(G(z)): 0.4737 / 0.4834 Acc: 21.8750 (15.8893)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][201/782] Loss_D: 0.9540 (1.0342) Loss_G: 0.7833 (0.9603) D(x): 0.6027 D(G(z)): 0.5008 / 0.4284 Acc: 21.8750 (15.9189)\n",
      "[0/200][202/782] Loss_D: 0.9998 (1.0341) Loss_G: 0.9304 (0.9602) D(x): 0.6214 D(G(z)): 0.4747 / 0.4234 Acc: 17.1875 (15.9252)\n",
      "[0/200][203/782] Loss_D: 0.9639 (1.0337) Loss_G: 0.6728 (0.9588) D(x): 0.5616 D(G(z)): 0.4483 / 0.4688 Acc: 21.8750 (15.9544)\n",
      "[0/200][204/782] Loss_D: 0.8140 (1.0327) Loss_G: 0.8668 (0.9583) D(x): 0.6520 D(G(z)): 0.4381 / 0.4152 Acc: 20.3125 (15.9756)\n",
      "[0/200][205/782] Loss_D: 0.9259 (1.0321) Loss_G: 0.7333 (0.9572) D(x): 0.5579 D(G(z)): 0.4241 / 0.4519 Acc: 18.7500 (15.9891)\n",
      "[0/200][206/782] Loss_D: 0.9504 (1.0317) Loss_G: 0.6289 (0.9556) D(x): 0.5688 D(G(z)): 0.4294 / 0.4809 Acc: 15.6250 (15.9873)\n",
      "[0/200][207/782] Loss_D: 1.0739 (1.0320) Loss_G: 0.8313 (0.9550) D(x): 0.5996 D(G(z)): 0.4605 / 0.4360 Acc: 15.6250 (15.9856)\n",
      "[0/200][208/782] Loss_D: 0.8556 (1.0311) Loss_G: 0.7899 (0.9542) D(x): 0.6231 D(G(z)): 0.4638 / 0.4326 Acc: 23.4375 (16.0212)\n",
      "[0/200][209/782] Loss_D: 0.9766 (1.0308) Loss_G: 0.8209 (0.9536) D(x): 0.6381 D(G(z)): 0.4937 / 0.4332 Acc: 14.0625 (16.0119)\n",
      "[0/200][210/782] Loss_D: 0.8518 (1.0300) Loss_G: 0.8809 (0.9533) D(x): 0.6175 D(G(z)): 0.4742 / 0.3812 Acc: 23.4375 (16.0471)\n",
      "[0/200][211/782] Loss_D: 0.9920 (1.0298) Loss_G: 0.7095 (0.9521) D(x): 0.5818 D(G(z)): 0.4532 / 0.4678 Acc: 14.0625 (16.0377)\n",
      "[0/200][212/782] Loss_D: 1.2026 (1.0306) Loss_G: 0.7274 (0.9511) D(x): 0.5572 D(G(z)): 0.5055 / 0.4616 Acc: 17.1875 (16.0431)\n",
      "[0/200][213/782] Loss_D: 0.9523 (1.0303) Loss_G: 0.6531 (0.9497) D(x): 0.6310 D(G(z)): 0.4750 / 0.4668 Acc: 14.0625 (16.0339)\n",
      "[0/200][214/782] Loss_D: 0.9090 (1.0297) Loss_G: 0.9202 (0.9495) D(x): 0.6061 D(G(z)): 0.4502 / 0.4247 Acc: 20.3125 (16.0538)\n",
      "[0/200][215/782] Loss_D: 1.0176 (1.0296) Loss_G: 0.7083 (0.9484) D(x): 0.6011 D(G(z)): 0.4966 / 0.4720 Acc: 18.7500 (16.0663)\n",
      "[0/200][216/782] Loss_D: 1.0368 (1.0297) Loss_G: 0.8132 (0.9478) D(x): 0.5982 D(G(z)): 0.5103 / 0.4204 Acc: 17.1875 (16.0714)\n",
      "[0/200][217/782] Loss_D: 0.8950 (1.0291) Loss_G: 0.7313 (0.9468) D(x): 0.5767 D(G(z)): 0.4387 / 0.4333 Acc: 17.1875 (16.0765)\n",
      "[0/200][218/782] Loss_D: 0.9182 (1.0286) Loss_G: 0.7173 (0.9457) D(x): 0.5641 D(G(z)): 0.4335 / 0.4480 Acc: 17.1875 (16.0816)\n",
      "[0/200][219/782] Loss_D: 0.8148 (1.0276) Loss_G: 0.7713 (0.9450) D(x): 0.6134 D(G(z)): 0.4219 / 0.4468 Acc: 20.3125 (16.1009)\n",
      "[0/200][220/782] Loss_D: 0.8444 (1.0268) Loss_G: 0.6827 (0.9438) D(x): 0.6162 D(G(z)): 0.4640 / 0.4806 Acc: 26.5625 (16.1482)\n",
      "[0/200][221/782] Loss_D: 0.9027 (1.0262) Loss_G: 0.7747 (0.9430) D(x): 0.6637 D(G(z)): 0.4949 / 0.4468 Acc: 25.0000 (16.1881)\n",
      "[0/200][222/782] Loss_D: 0.9184 (1.0257) Loss_G: 0.8033 (0.9424) D(x): 0.6208 D(G(z)): 0.4647 / 0.4280 Acc: 17.1875 (16.1925)\n",
      "[0/200][223/782] Loss_D: 0.9692 (1.0255) Loss_G: 0.7957 (0.9417) D(x): 0.6168 D(G(z)): 0.4834 / 0.4356 Acc: 18.7500 (16.2040)\n",
      "[0/200][224/782] Loss_D: 0.9560 (1.0251) Loss_G: 0.9219 (0.9416) D(x): 0.6437 D(G(z)): 0.4720 / 0.4209 Acc: 18.7500 (16.2153)\n",
      "[0/200][225/782] Loss_D: 0.8778 (1.0245) Loss_G: 0.7181 (0.9406) D(x): 0.5912 D(G(z)): 0.4405 / 0.4523 Acc: 21.8750 (16.2403)\n",
      "[0/200][226/782] Loss_D: 0.8921 (1.0239) Loss_G: 0.8273 (0.9402) D(x): 0.6066 D(G(z)): 0.4430 / 0.4370 Acc: 26.5625 (16.2858)\n",
      "[0/200][227/782] Loss_D: 0.8107 (1.0230) Loss_G: 0.7527 (0.9393) D(x): 0.6107 D(G(z)): 0.4515 / 0.4422 Acc: 28.1250 (16.3377)\n",
      "[0/200][228/782] Loss_D: 1.0082 (1.0229) Loss_G: 0.6776 (0.9382) D(x): 0.6107 D(G(z)): 0.4705 / 0.4750 Acc: 12.5000 (16.3210)\n",
      "[0/200][229/782] Loss_D: 0.9102 (1.0224) Loss_G: 0.9169 (0.9381) D(x): 0.6334 D(G(z)): 0.5014 / 0.4272 Acc: 26.5625 (16.3655)\n",
      "[0/200][230/782] Loss_D: 0.9687 (1.0222) Loss_G: 0.8155 (0.9376) D(x): 0.5722 D(G(z)): 0.4838 / 0.4322 Acc: 21.8750 (16.3893)\n",
      "[0/200][231/782] Loss_D: 0.9188 (1.0217) Loss_G: 0.7735 (0.9369) D(x): 0.6292 D(G(z)): 0.5078 / 0.4332 Acc: 20.3125 (16.4062)\n",
      "[0/200][232/782] Loss_D: 1.1398 (1.0223) Loss_G: 0.8012 (0.9363) D(x): 0.5164 D(G(z)): 0.4456 / 0.4407 Acc: 17.1875 (16.4096)\n",
      "[0/200][233/782] Loss_D: 1.0897 (1.0225) Loss_G: 0.8508 (0.9359) D(x): 0.5811 D(G(z)): 0.4855 / 0.4191 Acc: 15.6250 (16.4062)\n",
      "[0/200][234/782] Loss_D: 1.1394 (1.0230) Loss_G: 0.7090 (0.9349) D(x): 0.5576 D(G(z)): 0.5154 / 0.4889 Acc: 20.3125 (16.4229)\n",
      "[0/200][235/782] Loss_D: 0.8711 (1.0224) Loss_G: 0.7060 (0.9340) D(x): 0.6660 D(G(z)): 0.5270 / 0.4659 Acc: 31.2500 (16.4857)\n",
      "[0/200][236/782] Loss_D: 1.1023 (1.0227) Loss_G: 0.9659 (0.9341) D(x): 0.6011 D(G(z)): 0.5090 / 0.4099 Acc: 15.6250 (16.4821)\n",
      "[0/200][237/782] Loss_D: 1.1357 (1.0232) Loss_G: 0.8487 (0.9337) D(x): 0.5034 D(G(z)): 0.4418 / 0.4332 Acc: 20.3125 (16.4982)\n",
      "[0/200][238/782] Loss_D: 1.0313 (1.0232) Loss_G: 0.6325 (0.9325) D(x): 0.5664 D(G(z)): 0.4593 / 0.4780 Acc: 15.6250 (16.4945)\n",
      "[0/200][239/782] Loss_D: 1.1033 (1.0236) Loss_G: 0.8132 (0.9320) D(x): 0.6138 D(G(z)): 0.5407 / 0.4308 Acc: 17.1875 (16.4974)\n",
      "[0/200][240/782] Loss_D: 1.0607 (1.0237) Loss_G: 0.8691 (0.9317) D(x): 0.5599 D(G(z)): 0.4802 / 0.4050 Acc: 17.1875 (16.5003)\n",
      "[0/200][241/782] Loss_D: 1.0688 (1.0239) Loss_G: 0.8724 (0.9315) D(x): 0.5563 D(G(z)): 0.4856 / 0.4167 Acc: 18.7500 (16.5096)\n",
      "[0/200][242/782] Loss_D: 1.0553 (1.0240) Loss_G: 0.8032 (0.9310) D(x): 0.5722 D(G(z)): 0.4700 / 0.4695 Acc: 18.7500 (16.5188)\n",
      "[0/200][243/782] Loss_D: 1.1137 (1.0244) Loss_G: 0.7453 (0.9302) D(x): 0.5258 D(G(z)): 0.4746 / 0.4636 Acc: 25.0000 (16.5535)\n",
      "[0/200][244/782] Loss_D: 1.0480 (1.0245) Loss_G: 0.7304 (0.9294) D(x): 0.6114 D(G(z)): 0.5191 / 0.4628 Acc: 17.1875 (16.5561)\n",
      "[0/200][245/782] Loss_D: 0.9176 (1.0241) Loss_G: 0.7414 (0.9286) D(x): 0.5854 D(G(z)): 0.4876 / 0.4295 Acc: 28.1250 (16.6032)\n",
      "[0/200][246/782] Loss_D: 1.2575 (1.0250) Loss_G: 1.0058 (0.9289) D(x): 0.5505 D(G(z)): 0.5382 / 0.3662 Acc: 12.5000 (16.5865)\n",
      "[0/200][247/782] Loss_D: 1.1319 (1.0254) Loss_G: 0.8525 (0.9286) D(x): 0.4918 D(G(z)): 0.4563 / 0.4314 Acc: 25.0000 (16.6205)\n",
      "[0/200][248/782] Loss_D: 1.1951 (1.0261) Loss_G: 0.7225 (0.9278) D(x): 0.5162 D(G(z)): 0.4563 / 0.4836 Acc: 12.5000 (16.6039)\n",
      "[0/200][249/782] Loss_D: 1.0292 (1.0261) Loss_G: 0.8302 (0.9274) D(x): 0.5930 D(G(z)): 0.5134 / 0.4245 Acc: 20.3125 (16.6187)\n",
      "[0/200][250/782] Loss_D: 0.8476 (1.0254) Loss_G: 0.7848 (0.9268) D(x): 0.5675 D(G(z)): 0.4296 / 0.3979 Acc: 20.3125 (16.6335)\n",
      "[0/200][251/782] Loss_D: 1.0911 (1.0257) Loss_G: 0.9747 (0.9270) D(x): 0.5421 D(G(z)): 0.4593 / 0.3749 Acc: 17.1875 (16.6357)\n",
      "[0/200][252/782] Loss_D: 0.9945 (1.0256) Loss_G: 0.9617 (0.9272) D(x): 0.6264 D(G(z)): 0.4342 / 0.3821 Acc: 4.6875 (16.5884)\n",
      "[0/200][253/782] Loss_D: 1.1357 (1.0260) Loss_G: 0.7481 (0.9265) D(x): 0.5269 D(G(z)): 0.4565 / 0.4558 Acc: 15.6250 (16.5846)\n",
      "[0/200][254/782] Loss_D: 1.1530 (1.0265) Loss_G: 0.9450 (0.9265) D(x): 0.5373 D(G(z)): 0.4985 / 0.3854 Acc: 17.1875 (16.5870)\n",
      "[0/200][255/782] Loss_D: 1.0566 (1.0266) Loss_G: 1.0134 (0.9269) D(x): 0.5836 D(G(z)): 0.5194 / 0.3773 Acc: 23.4375 (16.6138)\n",
      "[0/200][256/782] Loss_D: 1.1673 (1.0272) Loss_G: 0.9377 (0.9269) D(x): 0.5251 D(G(z)): 0.4531 / 0.3923 Acc: 18.7500 (16.6221)\n",
      "[0/200][257/782] Loss_D: 1.0767 (1.0274) Loss_G: 0.7285 (0.9261) D(x): 0.5379 D(G(z)): 0.4717 / 0.4428 Acc: 14.0625 (16.6122)\n",
      "[0/200][258/782] Loss_D: 1.3567 (1.0286) Loss_G: 0.8503 (0.9258) D(x): 0.4972 D(G(z)): 0.5234 / 0.4202 Acc: 18.7500 (16.6204)\n",
      "[0/200][259/782] Loss_D: 0.9414 (1.0283) Loss_G: 0.9751 (0.9260) D(x): 0.6175 D(G(z)): 0.4836 / 0.3699 Acc: 21.8750 (16.6406)\n",
      "[0/200][260/782] Loss_D: 1.1958 (1.0289) Loss_G: 0.9734 (0.9262) D(x): 0.5260 D(G(z)): 0.5080 / 0.3874 Acc: 26.5625 (16.6786)\n",
      "[0/200][261/782] Loss_D: 1.0401 (1.0290) Loss_G: 0.9988 (0.9265) D(x): 0.5622 D(G(z)): 0.4141 / 0.3635 Acc: 10.9375 (16.6567)\n",
      "[0/200][262/782] Loss_D: 1.2319 (1.0297) Loss_G: 0.9388 (0.9265) D(x): 0.5034 D(G(z)): 0.4464 / 0.3964 Acc: 10.9375 (16.6350)\n",
      "[0/200][263/782] Loss_D: 1.1711 (1.0303) Loss_G: 0.7748 (0.9260) D(x): 0.5482 D(G(z)): 0.4752 / 0.4214 Acc: 12.5000 (16.6193)\n",
      "[0/200][264/782] Loss_D: 0.9689 (1.0301) Loss_G: 0.9748 (0.9262) D(x): 0.6258 D(G(z)): 0.4717 / 0.3701 Acc: 7.8125 (16.5861)\n",
      "[0/200][265/782] Loss_D: 0.9697 (1.0298) Loss_G: 1.0154 (0.9265) D(x): 0.5729 D(G(z)): 0.4701 / 0.3686 Acc: 28.1250 (16.6295)\n",
      "[0/200][266/782] Loss_D: 1.1715 (1.0304) Loss_G: 1.1000 (0.9271) D(x): 0.4803 D(G(z)): 0.4182 / 0.3515 Acc: 17.1875 (16.6316)\n",
      "[0/200][267/782] Loss_D: 0.9383 (1.0300) Loss_G: 0.8762 (0.9269) D(x): 0.5636 D(G(z)): 0.4610 / 0.3840 Acc: 23.4375 (16.6569)\n",
      "[0/200][268/782] Loss_D: 0.8808 (1.0295) Loss_G: 0.9500 (0.9270) D(x): 0.5861 D(G(z)): 0.4766 / 0.3683 Acc: 26.5625 (16.6938)\n",
      "[0/200][269/782] Loss_D: 1.0074 (1.0294) Loss_G: 0.9709 (0.9272) D(x): 0.5364 D(G(z)): 0.4601 / 0.3737 Acc: 28.1250 (16.7361)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][270/782] Loss_D: 1.1297 (1.0297) Loss_G: 1.0583 (0.9277) D(x): 0.5590 D(G(z)): 0.4728 / 0.3581 Acc: 12.5000 (16.7205)\n",
      "[0/200][271/782] Loss_D: 1.2658 (1.0306) Loss_G: 0.8841 (0.9275) D(x): 0.4860 D(G(z)): 0.4933 / 0.4019 Acc: 20.3125 (16.7337)\n",
      "[0/200][272/782] Loss_D: 1.0554 (1.0307) Loss_G: 0.9756 (0.9277) D(x): 0.5265 D(G(z)): 0.4508 / 0.3830 Acc: 18.7500 (16.7411)\n",
      "[0/200][273/782] Loss_D: 1.0814 (1.0309) Loss_G: 1.0348 (0.9281) D(x): 0.6258 D(G(z)): 0.5240 / 0.3615 Acc: 14.0625 (16.7313)\n",
      "[0/200][274/782] Loss_D: 1.0391 (1.0309) Loss_G: 1.1321 (0.9288) D(x): 0.5452 D(G(z)): 0.4597 / 0.3222 Acc: 15.6250 (16.7273)\n",
      "[0/200][275/782] Loss_D: 1.1359 (1.0313) Loss_G: 0.9205 (0.9288) D(x): 0.4697 D(G(z)): 0.4229 / 0.3841 Acc: 25.0000 (16.7572)\n",
      "[0/200][276/782] Loss_D: 1.0706 (1.0314) Loss_G: 0.9288 (0.9288) D(x): 0.5220 D(G(z)): 0.4442 / 0.3806 Acc: 17.1875 (16.7588)\n",
      "[0/200][277/782] Loss_D: 1.0727 (1.0316) Loss_G: 1.0560 (0.9293) D(x): 0.5949 D(G(z)): 0.5251 / 0.3472 Acc: 21.8750 (16.7772)\n",
      "[0/200][278/782] Loss_D: 0.8737 (1.0310) Loss_G: 0.7894 (0.9288) D(x): 0.5596 D(G(z)): 0.4221 / 0.4267 Acc: 23.4375 (16.8011)\n",
      "[0/200][279/782] Loss_D: 1.0217 (1.0310) Loss_G: 0.7963 (0.9283) D(x): 0.4952 D(G(z)): 0.4763 / 0.4153 Acc: 31.2500 (16.8527)\n",
      "[0/200][280/782] Loss_D: 0.9608 (1.0307) Loss_G: 0.9462 (0.9283) D(x): 0.5822 D(G(z)): 0.4185 / 0.4047 Acc: 18.7500 (16.8594)\n",
      "[0/200][281/782] Loss_D: 0.9516 (1.0305) Loss_G: 0.9243 (0.9283) D(x): 0.5499 D(G(z)): 0.4698 / 0.3635 Acc: 17.1875 (16.8606)\n",
      "[0/200][282/782] Loss_D: 1.2210 (1.0311) Loss_G: 0.7482 (0.9277) D(x): 0.4820 D(G(z)): 0.4661 / 0.4412 Acc: 20.3125 (16.8728)\n",
      "[0/200][283/782] Loss_D: 1.1901 (1.0317) Loss_G: 0.8507 (0.9274) D(x): 0.5565 D(G(z)): 0.5030 / 0.4186 Acc: 17.1875 (16.8739)\n",
      "[0/200][284/782] Loss_D: 1.0719 (1.0318) Loss_G: 0.8637 (0.9272) D(x): 0.5642 D(G(z)): 0.5168 / 0.4032 Acc: 21.8750 (16.8914)\n",
      "[0/200][285/782] Loss_D: 1.1620 (1.0323) Loss_G: 0.6793 (0.9263) D(x): 0.4977 D(G(z)): 0.4734 / 0.4629 Acc: 18.7500 (16.8979)\n",
      "[0/200][286/782] Loss_D: 1.2985 (1.0332) Loss_G: 0.7724 (0.9258) D(x): 0.4915 D(G(z)): 0.4737 / 0.4544 Acc: 18.7500 (16.9044)\n",
      "[0/200][287/782] Loss_D: 1.0988 (1.0334) Loss_G: 0.8110 (0.9254) D(x): 0.5055 D(G(z)): 0.4169 / 0.4463 Acc: 20.3125 (16.9162)\n",
      "[0/200][288/782] Loss_D: 1.0704 (1.0336) Loss_G: 0.7215 (0.9247) D(x): 0.5761 D(G(z)): 0.4753 / 0.4897 Acc: 17.1875 (16.9172)\n",
      "[0/200][289/782] Loss_D: 1.0856 (1.0338) Loss_G: 0.8442 (0.9244) D(x): 0.5743 D(G(z)): 0.5101 / 0.4294 Acc: 25.0000 (16.9450)\n",
      "[0/200][290/782] Loss_D: 1.0886 (1.0339) Loss_G: 0.9120 (0.9244) D(x): 0.5327 D(G(z)): 0.4707 / 0.4171 Acc: 26.5625 (16.9781)\n",
      "[0/200][291/782] Loss_D: 1.1411 (1.0343) Loss_G: 0.7085 (0.9236) D(x): 0.4921 D(G(z)): 0.4589 / 0.4676 Acc: 21.8750 (16.9949)\n",
      "[0/200][292/782] Loss_D: 1.0954 (1.0345) Loss_G: 0.8260 (0.9233) D(x): 0.5676 D(G(z)): 0.4930 / 0.4188 Acc: 18.7500 (17.0009)\n",
      "[0/200][293/782] Loss_D: 1.1836 (1.0350) Loss_G: 0.7935 (0.9229) D(x): 0.4575 D(G(z)): 0.4522 / 0.4631 Acc: 23.4375 (17.0227)\n",
      "[0/200][294/782] Loss_D: 1.2298 (1.0357) Loss_G: 0.6276 (0.9219) D(x): 0.4729 D(G(z)): 0.5064 / 0.4904 Acc: 23.4375 (17.0445)\n",
      "[0/200][295/782] Loss_D: 1.1654 (1.0361) Loss_G: 0.9190 (0.9218) D(x): 0.5744 D(G(z)): 0.5420 / 0.3956 Acc: 20.3125 (17.0555)\n",
      "[0/200][296/782] Loss_D: 1.2782 (1.0369) Loss_G: 0.8370 (0.9216) D(x): 0.4817 D(G(z)): 0.5097 / 0.4345 Acc: 26.5625 (17.0875)\n",
      "[0/200][297/782] Loss_D: 1.1946 (1.0375) Loss_G: 0.7729 (0.9211) D(x): 0.4904 D(G(z)): 0.4329 / 0.4531 Acc: 12.5000 (17.0721)\n",
      "[0/200][298/782] Loss_D: 1.2321 (1.0381) Loss_G: 0.6534 (0.9202) D(x): 0.5147 D(G(z)): 0.5134 / 0.4758 Acc: 17.1875 (17.0725)\n",
      "[0/200][299/782] Loss_D: 1.0690 (1.0382) Loss_G: 0.5918 (0.9191) D(x): 0.4850 D(G(z)): 0.4634 / 0.4872 Acc: 29.6875 (17.1146)\n",
      "[0/200][300/782] Loss_D: 1.3180 (1.0391) Loss_G: 0.7342 (0.9185) D(x): 0.5013 D(G(z)): 0.5563 / 0.4485 Acc: 20.3125 (17.1252)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][301/782] Loss_D: 1.0528 (1.0392) Loss_G: 0.8138 (0.9181) D(x): 0.5199 D(G(z)): 0.5024 / 0.4015 Acc: 28.1250 (17.1616)\n",
      "[0/200][302/782] Loss_D: 1.1761 (1.0396) Loss_G: 0.7276 (0.9175) D(x): 0.4721 D(G(z)): 0.4500 / 0.4407 Acc: 14.0625 (17.1514)\n",
      "[0/200][303/782] Loss_D: 1.1603 (1.0400) Loss_G: 0.5976 (0.9164) D(x): 0.4502 D(G(z)): 0.4322 / 0.4995 Acc: 23.4375 (17.1721)\n",
      "[0/200][304/782] Loss_D: 1.1339 (1.0404) Loss_G: 0.6926 (0.9157) D(x): 0.5312 D(G(z)): 0.5345 / 0.4869 Acc: 23.4375 (17.1926)\n",
      "[0/200][305/782] Loss_D: 1.1108 (1.0406) Loss_G: 1.0743 (0.9162) D(x): 0.5658 D(G(z)): 0.5518 / 0.3561 Acc: 25.0000 (17.2181)\n",
      "[0/200][306/782] Loss_D: 1.4349 (1.0419) Loss_G: 0.8840 (0.9161) D(x): 0.4116 D(G(z)): 0.4321 / 0.4189 Acc: 15.6250 (17.2129)\n",
      "[0/200][307/782] Loss_D: 1.1230 (1.0421) Loss_G: 0.7564 (0.9156) D(x): 0.5027 D(G(z)): 0.4463 / 0.4710 Acc: 14.0625 (17.2027)\n",
      "[0/200][308/782] Loss_D: 1.0627 (1.0422) Loss_G: 0.6825 (0.9148) D(x): 0.5108 D(G(z)): 0.4770 / 0.4638 Acc: 25.0000 (17.2280)\n",
      "[0/200][309/782] Loss_D: 1.1959 (1.0427) Loss_G: 0.7640 (0.9144) D(x): 0.5327 D(G(z)): 0.5095 / 0.4626 Acc: 21.8750 (17.2429)\n",
      "[0/200][310/782] Loss_D: 1.1755 (1.0431) Loss_G: 0.7602 (0.9139) D(x): 0.5208 D(G(z)): 0.5129 / 0.4426 Acc: 17.1875 (17.2428)\n",
      "[0/200][311/782] Loss_D: 1.2328 (1.0437) Loss_G: 0.7159 (0.9132) D(x): 0.4517 D(G(z)): 0.4450 / 0.5048 Acc: 23.4375 (17.2626)\n",
      "[0/200][312/782] Loss_D: 1.0372 (1.0437) Loss_G: 0.7485 (0.9127) D(x): 0.5596 D(G(z)): 0.4912 / 0.4330 Acc: 14.0625 (17.2524)\n",
      "[0/200][313/782] Loss_D: 1.0303 (1.0437) Loss_G: 0.8463 (0.9125) D(x): 0.5162 D(G(z)): 0.4515 / 0.4135 Acc: 21.8750 (17.2671)\n",
      "[0/200][314/782] Loss_D: 1.1256 (1.0439) Loss_G: 0.7300 (0.9119) D(x): 0.5263 D(G(z)): 0.4947 / 0.4718 Acc: 23.4375 (17.2867)\n",
      "[0/200][315/782] Loss_D: 1.2584 (1.0446) Loss_G: 0.7837 (0.9115) D(x): 0.5119 D(G(z)): 0.5215 / 0.4613 Acc: 18.7500 (17.2913)\n",
      "[0/200][316/782] Loss_D: 1.0390 (1.0446) Loss_G: 0.8516 (0.9113) D(x): 0.5222 D(G(z)): 0.4704 / 0.3847 Acc: 20.3125 (17.3009)\n",
      "[0/200][317/782] Loss_D: 1.0953 (1.0447) Loss_G: 0.8894 (0.9112) D(x): 0.5080 D(G(z)): 0.4545 / 0.4108 Acc: 20.3125 (17.3103)\n",
      "[0/200][318/782] Loss_D: 1.0621 (1.0448) Loss_G: 0.8138 (0.9109) D(x): 0.5155 D(G(z)): 0.4847 / 0.4192 Acc: 25.0000 (17.3344)\n",
      "[0/200][319/782] Loss_D: 0.9441 (1.0445) Loss_G: 0.6166 (0.9100) D(x): 0.4895 D(G(z)): 0.4159 / 0.5069 Acc: 25.0000 (17.3584)\n",
      "[0/200][320/782] Loss_D: 0.8800 (1.0440) Loss_G: 0.5828 (0.9090) D(x): 0.5450 D(G(z)): 0.4390 / 0.5078 Acc: 28.1250 (17.3919)\n",
      "[0/200][321/782] Loss_D: 1.1701 (1.0444) Loss_G: 0.5835 (0.9080) D(x): 0.5690 D(G(z)): 0.5655 / 0.4992 Acc: 25.0000 (17.4156)\n",
      "[0/200][322/782] Loss_D: 1.2464 (1.0450) Loss_G: 0.6767 (0.9073) D(x): 0.5135 D(G(z)): 0.5135 / 0.4831 Acc: 15.6250 (17.4100)\n",
      "[0/200][323/782] Loss_D: 1.0092 (1.0449) Loss_G: 0.7978 (0.9069) D(x): 0.5274 D(G(z)): 0.4464 / 0.4202 Acc: 15.6250 (17.4045)\n",
      "[0/200][324/782] Loss_D: 0.9550 (1.0446) Loss_G: 0.6671 (0.9062) D(x): 0.4804 D(G(z)): 0.4301 / 0.4478 Acc: 23.4375 (17.4231)\n",
      "[0/200][325/782] Loss_D: 1.2868 (1.0453) Loss_G: 0.6698 (0.9055) D(x): 0.4860 D(G(z)): 0.5431 / 0.5023 Acc: 23.4375 (17.4415)\n",
      "[0/200][326/782] Loss_D: 1.0501 (1.0454) Loss_G: 0.6599 (0.9047) D(x): 0.5342 D(G(z)): 0.4919 / 0.4456 Acc: 15.6250 (17.4360)\n",
      "[0/200][327/782] Loss_D: 0.8959 (1.0449) Loss_G: 0.6219 (0.9039) D(x): 0.4979 D(G(z)): 0.4464 / 0.4672 Acc: 28.1250 (17.4686)\n",
      "[0/200][328/782] Loss_D: 1.0775 (1.0450) Loss_G: 0.7025 (0.9032) D(x): 0.5053 D(G(z)): 0.4629 / 0.4452 Acc: 15.6250 (17.4630)\n",
      "[0/200][329/782] Loss_D: 1.1570 (1.0453) Loss_G: 0.7329 (0.9027) D(x): 0.5032 D(G(z)): 0.4932 / 0.4448 Acc: 20.3125 (17.4716)\n",
      "[0/200][330/782] Loss_D: 0.9601 (1.0451) Loss_G: 0.7353 (0.9022) D(x): 0.5420 D(G(z)): 0.4648 / 0.4299 Acc: 25.0000 (17.4943)\n",
      "[0/200][331/782] Loss_D: 0.7911 (1.0443) Loss_G: 0.8877 (0.9022) D(x): 0.5644 D(G(z)): 0.4332 / 0.4043 Acc: 31.2500 (17.5358)\n",
      "[0/200][332/782] Loss_D: 0.9809 (1.0441) Loss_G: 0.8509 (0.9020) D(x): 0.5412 D(G(z)): 0.4594 / 0.3963 Acc: 20.3125 (17.5441)\n",
      "[0/200][333/782] Loss_D: 0.9359 (1.0438) Loss_G: 0.9933 (0.9023) D(x): 0.5541 D(G(z)): 0.4158 / 0.3708 Acc: 20.3125 (17.5524)\n",
      "[0/200][334/782] Loss_D: 0.8667 (1.0433) Loss_G: 0.9124 (0.9023) D(x): 0.5231 D(G(z)): 0.4053 / 0.3887 Acc: 26.5625 (17.5793)\n",
      "[0/200][335/782] Loss_D: 0.8654 (1.0427) Loss_G: 0.8306 (0.9021) D(x): 0.5576 D(G(z)): 0.4459 / 0.3991 Acc: 25.0000 (17.6014)\n",
      "[0/200][336/782] Loss_D: 1.0406 (1.0427) Loss_G: 0.7250 (0.9016) D(x): 0.5048 D(G(z)): 0.4571 / 0.4517 Acc: 26.5625 (17.6280)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][337/782] Loss_D: 1.0637 (1.0428) Loss_G: 0.7468 (0.9011) D(x): 0.5171 D(G(z)): 0.4899 / 0.4261 Acc: 20.3125 (17.6359)\n",
      "[0/200][338/782] Loss_D: 1.0847 (1.0429) Loss_G: 0.7634 (0.9007) D(x): 0.5133 D(G(z)): 0.4924 / 0.4285 Acc: 21.8750 (17.6484)\n",
      "[0/200][339/782] Loss_D: 0.9119 (1.0425) Loss_G: 0.8033 (0.9004) D(x): 0.5219 D(G(z)): 0.4494 / 0.4070 Acc: 26.5625 (17.6746)\n",
      "[0/200][340/782] Loss_D: 0.9922 (1.0424) Loss_G: 0.6862 (0.8998) D(x): 0.5209 D(G(z)): 0.4579 / 0.4569 Acc: 21.8750 (17.6870)\n",
      "[0/200][341/782] Loss_D: 1.2961 (1.0431) Loss_G: 0.8276 (0.8996) D(x): 0.5114 D(G(z)): 0.4899 / 0.4234 Acc: 9.3750 (17.6626)\n",
      "[0/200][342/782] Loss_D: 1.0706 (1.0432) Loss_G: 0.7802 (0.8992) D(x): 0.5214 D(G(z)): 0.4490 / 0.4549 Acc: 21.8750 (17.6749)\n",
      "[0/200][343/782] Loss_D: 1.1115 (1.0434) Loss_G: 0.6864 (0.8986) D(x): 0.4679 D(G(z)): 0.4485 / 0.4603 Acc: 20.3125 (17.6826)\n",
      "[0/200][344/782] Loss_D: 1.0580 (1.0435) Loss_G: 0.8933 (0.8986) D(x): 0.5332 D(G(z)): 0.4581 / 0.4032 Acc: 21.8750 (17.6947)\n",
      "[0/200][345/782] Loss_D: 0.9765 (1.0433) Loss_G: 0.8938 (0.8986) D(x): 0.5413 D(G(z)): 0.4397 / 0.3976 Acc: 21.8750 (17.7068)\n",
      "[0/200][346/782] Loss_D: 0.7291 (1.0424) Loss_G: 0.7903 (0.8983) D(x): 0.5549 D(G(z)): 0.3914 / 0.4204 Acc: 31.2500 (17.7459)\n",
      "[0/200][347/782] Loss_D: 0.8717 (1.0419) Loss_G: 0.8911 (0.8983) D(x): 0.5539 D(G(z)): 0.4388 / 0.4000 Acc: 26.5625 (17.7712)\n",
      "[0/200][348/782] Loss_D: 1.1555 (1.0422) Loss_G: 0.8974 (0.8983) D(x): 0.4950 D(G(z)): 0.4608 / 0.4185 Acc: 17.1875 (17.7695)\n",
      "[0/200][349/782] Loss_D: 1.0118 (1.0421) Loss_G: 0.8660 (0.8982) D(x): 0.5576 D(G(z)): 0.4583 / 0.4292 Acc: 18.7500 (17.7723)\n",
      "[0/200][350/782] Loss_D: 0.8993 (1.0417) Loss_G: 0.7301 (0.8977) D(x): 0.5177 D(G(z)): 0.4169 / 0.4349 Acc: 26.5625 (17.7974)\n",
      "[0/200][351/782] Loss_D: 0.9803 (1.0415) Loss_G: 0.8900 (0.8977) D(x): 0.5610 D(G(z)): 0.4522 / 0.3783 Acc: 14.0625 (17.7868)\n",
      "[0/200][352/782] Loss_D: 0.8938 (1.0411) Loss_G: 0.9209 (0.8977) D(x): 0.4761 D(G(z)): 0.4065 / 0.3511 Acc: 28.1250 (17.8160)\n",
      "[0/200][353/782] Loss_D: 0.9004 (1.0407) Loss_G: 0.9151 (0.8978) D(x): 0.5302 D(G(z)): 0.4017 / 0.3627 Acc: 21.8750 (17.8275)\n",
      "[0/200][354/782] Loss_D: 0.9247 (1.0404) Loss_G: 1.0962 (0.8983) D(x): 0.5019 D(G(z)): 0.3483 / 0.3433 Acc: 20.3125 (17.8345)\n",
      "[0/200][355/782] Loss_D: 0.6535 (1.0393) Loss_G: 1.1266 (0.8990) D(x): 0.5724 D(G(z)): 0.3835 / 0.3187 Acc: 35.9375 (17.8854)\n",
      "[0/200][356/782] Loss_D: 0.7133 (1.0384) Loss_G: 1.1914 (0.8998) D(x): 0.6009 D(G(z)): 0.3722 / 0.3128 Acc: 25.0000 (17.9053)\n",
      "[0/200][357/782] Loss_D: 0.7449 (1.0376) Loss_G: 0.7305 (0.8993) D(x): 0.5853 D(G(z)): 0.3612 / 0.4459 Acc: 23.4375 (17.9207)\n",
      "[0/200][358/782] Loss_D: 0.6778 (1.0366) Loss_G: 0.9597 (0.8995) D(x): 0.6602 D(G(z)): 0.4266 / 0.3559 Acc: 23.4375 (17.9361)\n",
      "[0/200][359/782] Loss_D: 0.6363 (1.0354) Loss_G: 1.0442 (0.8999) D(x): 0.6309 D(G(z)): 0.4173 / 0.3209 Acc: 21.8750 (17.9470)\n",
      "[0/200][360/782] Loss_D: 0.8194 (1.0348) Loss_G: 0.9640 (0.9001) D(x): 0.5364 D(G(z)): 0.3888 / 0.3397 Acc: 26.5625 (17.9709)\n",
      "[0/200][361/782] Loss_D: 1.0299 (1.0348) Loss_G: 0.9930 (0.9003) D(x): 0.5164 D(G(z)): 0.4247 / 0.3641 Acc: 18.7500 (17.9731)\n",
      "[0/200][362/782] Loss_D: 0.8842 (1.0344) Loss_G: 1.1016 (0.9009) D(x): 0.5481 D(G(z)): 0.4411 / 0.3289 Acc: 23.4375 (17.9881)\n",
      "[0/200][363/782] Loss_D: 0.9755 (1.0343) Loss_G: 0.9396 (0.9010) D(x): 0.5039 D(G(z)): 0.3856 / 0.3765 Acc: 15.6250 (17.9816)\n",
      "[0/200][364/782] Loss_D: 1.0554 (1.0343) Loss_G: 0.9028 (0.9010) D(x): 0.5261 D(G(z)): 0.4488 / 0.4035 Acc: 14.0625 (17.9709)\n",
      "[0/200][365/782] Loss_D: 0.9071 (1.0340) Loss_G: 1.2146 (0.9019) D(x): 0.5678 D(G(z)): 0.4490 / 0.2932 Acc: 18.7500 (17.9730)\n",
      "[0/200][366/782] Loss_D: 0.8765 (1.0335) Loss_G: 1.2128 (0.9027) D(x): 0.5640 D(G(z)): 0.3772 / 0.3260 Acc: 14.0625 (17.9624)\n",
      "[0/200][367/782] Loss_D: 0.9897 (1.0334) Loss_G: 1.2096 (0.9035) D(x): 0.5296 D(G(z)): 0.3687 / 0.2984 Acc: 14.0625 (17.9518)\n",
      "[0/200][368/782] Loss_D: 0.8073 (1.0328) Loss_G: 1.0396 (0.9039) D(x): 0.5792 D(G(z)): 0.4185 / 0.3560 Acc: 25.0000 (17.9709)\n",
      "[0/200][369/782] Loss_D: 0.9418 (1.0326) Loss_G: 1.2312 (0.9048) D(x): 0.5492 D(G(z)): 0.4108 / 0.2974 Acc: 17.1875 (17.9687)\n",
      "[0/200][370/782] Loss_D: 0.8141 (1.0320) Loss_G: 1.3208 (0.9059) D(x): 0.6244 D(G(z)): 0.3682 / 0.2930 Acc: 12.5000 (17.9540)\n",
      "[0/200][371/782] Loss_D: 0.9170 (1.0317) Loss_G: 1.1862 (0.9067) D(x): 0.5706 D(G(z)): 0.3885 / 0.3174 Acc: 14.0625 (17.9435)\n",
      "[0/200][372/782] Loss_D: 0.9437 (1.0314) Loss_G: 1.4832 (0.9082) D(x): 0.5910 D(G(z)): 0.4176 / 0.2578 Acc: 14.0625 (17.9331)\n",
      "[0/200][373/782] Loss_D: 1.0390 (1.0314) Loss_G: 1.4950 (0.9098) D(x): 0.4767 D(G(z)): 0.3762 / 0.2502 Acc: 20.3125 (17.9395)\n",
      "[0/200][374/782] Loss_D: 0.7915 (1.0308) Loss_G: 1.3154 (0.9109) D(x): 0.5776 D(G(z)): 0.3324 / 0.2894 Acc: 20.3125 (17.9458)\n",
      "[0/200][375/782] Loss_D: 1.0142 (1.0308) Loss_G: 1.2917 (0.9119) D(x): 0.6227 D(G(z)): 0.4822 / 0.2883 Acc: 17.1875 (17.9438)\n",
      "[0/200][376/782] Loss_D: 0.8180 (1.0302) Loss_G: 1.2417 (0.9128) D(x): 0.5071 D(G(z)): 0.3253 / 0.3016 Acc: 23.4375 (17.9584)\n",
      "[0/200][377/782] Loss_D: 0.8857 (1.0298) Loss_G: 1.1980 (0.9135) D(x): 0.5254 D(G(z)): 0.3867 / 0.3278 Acc: 20.3125 (17.9646)\n",
      "[0/200][378/782] Loss_D: 1.1036 (1.0300) Loss_G: 0.8601 (0.9134) D(x): 0.5474 D(G(z)): 0.4719 / 0.4067 Acc: 14.0625 (17.9543)\n",
      "[0/200][379/782] Loss_D: 1.0361 (1.0300) Loss_G: 1.3429 (0.9145) D(x): 0.5764 D(G(z)): 0.4874 / 0.2716 Acc: 18.7500 (17.9564)\n",
      "[0/200][380/782] Loss_D: 0.9813 (1.0299) Loss_G: 1.6250 (0.9164) D(x): 0.5952 D(G(z)): 0.4504 / 0.2248 Acc: 15.6250 (17.9503)\n",
      "[0/200][381/782] Loss_D: 0.9785 (1.0298) Loss_G: 1.3651 (0.9175) D(x): 0.5553 D(G(z)): 0.3844 / 0.2743 Acc: 10.9375 (17.9319)\n",
      "[0/200][382/782] Loss_D: 0.9817 (1.0296) Loss_G: 1.4059 (0.9188) D(x): 0.5245 D(G(z)): 0.3825 / 0.2470 Acc: 10.9375 (17.9137)\n",
      "[0/200][383/782] Loss_D: 1.0045 (1.0296) Loss_G: 1.2580 (0.9197) D(x): 0.4891 D(G(z)): 0.3368 / 0.2953 Acc: 17.1875 (17.9118)\n",
      "[0/200][384/782] Loss_D: 0.8354 (1.0291) Loss_G: 1.0936 (0.9201) D(x): 0.6040 D(G(z)): 0.4196 / 0.3302 Acc: 14.0625 (17.9018)\n",
      "[0/200][385/782] Loss_D: 1.0746 (1.0292) Loss_G: 0.9965 (0.9203) D(x): 0.5138 D(G(z)): 0.4236 / 0.3500 Acc: 12.5000 (17.8878)\n",
      "[0/200][386/782] Loss_D: 1.0460 (1.0292) Loss_G: 1.1255 (0.9209) D(x): 0.5696 D(G(z)): 0.4519 / 0.3225 Acc: 10.9375 (17.8698)\n",
      "[0/200][387/782] Loss_D: 1.0666 (1.0293) Loss_G: 1.1952 (0.9216) D(x): 0.5392 D(G(z)): 0.3953 / 0.3185 Acc: 15.6250 (17.8640)\n",
      "[0/200][388/782] Loss_D: 0.9494 (1.0291) Loss_G: 0.9239 (0.9216) D(x): 0.5926 D(G(z)): 0.4151 / 0.3927 Acc: 9.3750 (17.8422)\n",
      "[0/200][389/782] Loss_D: 0.8286 (1.0286) Loss_G: 1.1969 (0.9223) D(x): 0.6301 D(G(z)): 0.4016 / 0.2937 Acc: 7.8125 (17.8165)\n",
      "[0/200][390/782] Loss_D: 0.8569 (1.0282) Loss_G: 1.1310 (0.9228) D(x): 0.5682 D(G(z)): 0.3794 / 0.3234 Acc: 21.8750 (17.8269)\n",
      "[0/200][391/782] Loss_D: 1.1596 (1.0285) Loss_G: 0.9574 (0.9229) D(x): 0.4792 D(G(z)): 0.3860 / 0.3884 Acc: 12.5000 (17.8133)\n",
      "[0/200][392/782] Loss_D: 0.9868 (1.0284) Loss_G: 1.0626 (0.9233) D(x): 0.7154 D(G(z)): 0.4910 / 0.3592 Acc: 9.3750 (17.7918)\n",
      "[0/200][393/782] Loss_D: 1.0026 (1.0283) Loss_G: 1.0159 (0.9235) D(x): 0.5726 D(G(z)): 0.4335 / 0.3439 Acc: 12.5000 (17.7784)\n",
      "[0/200][394/782] Loss_D: 1.0328 (1.0283) Loss_G: 1.1128 (0.9240) D(x): 0.5653 D(G(z)): 0.4083 / 0.3285 Acc: 9.3750 (17.7571)\n",
      "[0/200][395/782] Loss_D: 0.9400 (1.0281) Loss_G: 1.1791 (0.9246) D(x): 0.6000 D(G(z)): 0.4074 / 0.3246 Acc: 17.1875 (17.7557)\n",
      "[0/200][396/782] Loss_D: 0.9418 (1.0279) Loss_G: 1.1105 (0.9251) D(x): 0.6146 D(G(z)): 0.4344 / 0.3311 Acc: 14.0625 (17.7464)\n",
      "[0/200][397/782] Loss_D: 0.8893 (1.0276) Loss_G: 1.2700 (0.9260) D(x): 0.6405 D(G(z)): 0.4502 / 0.3184 Acc: 21.8750 (17.7568)\n",
      "[0/200][398/782] Loss_D: 1.2040 (1.0280) Loss_G: 1.2132 (0.9267) D(x): 0.5508 D(G(z)): 0.4922 / 0.3215 Acc: 15.6250 (17.7514)\n",
      "[0/200][399/782] Loss_D: 1.2304 (1.0285) Loss_G: 1.0369 (0.9270) D(x): 0.5090 D(G(z)): 0.4460 / 0.3527 Acc: 12.5000 (17.7383)\n",
      "[0/200][400/782] Loss_D: 1.1689 (1.0289) Loss_G: 1.1965 (0.9276) D(x): 0.5684 D(G(z)): 0.4857 / 0.3342 Acc: 12.5000 (17.7252)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][401/782] Loss_D: 0.9941 (1.0288) Loss_G: 1.0733 (0.9280) D(x): 0.5873 D(G(z)): 0.4163 / 0.3789 Acc: 20.3125 (17.7317)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][402/782] Loss_D: 1.0677 (1.0289) Loss_G: 1.3026 (0.9289) D(x): 0.6047 D(G(z)): 0.5131 / 0.2962 Acc: 21.8750 (17.7419)\n",
      "[0/200][403/782] Loss_D: 0.9985 (1.0288) Loss_G: 1.3212 (0.9299) D(x): 0.5657 D(G(z)): 0.4051 / 0.2983 Acc: 18.7500 (17.7444)\n",
      "[0/200][404/782] Loss_D: 0.8739 (1.0284) Loss_G: 1.2066 (0.9306) D(x): 0.6105 D(G(z)): 0.4385 / 0.3092 Acc: 25.0000 (17.7623)\n",
      "[0/200][405/782] Loss_D: 1.1820 (1.0288) Loss_G: 0.8995 (0.9305) D(x): 0.5287 D(G(z)): 0.4762 / 0.3942 Acc: 15.6250 (17.7571)\n",
      "[0/200][406/782] Loss_D: 0.9898 (1.0287) Loss_G: 1.0172 (0.9307) D(x): 0.6412 D(G(z)): 0.4830 / 0.3801 Acc: 14.0625 (17.7480)\n",
      "[0/200][407/782] Loss_D: 0.9087 (1.0284) Loss_G: 1.1723 (0.9313) D(x): 0.6571 D(G(z)): 0.4569 / 0.3262 Acc: 10.9375 (17.7313)\n",
      "[0/200][408/782] Loss_D: 0.9814 (1.0283) Loss_G: 1.0709 (0.9316) D(x): 0.5617 D(G(z)): 0.4228 / 0.3456 Acc: 10.9375 (17.7147)\n",
      "[0/200][409/782] Loss_D: 1.0015 (1.0282) Loss_G: 1.0737 (0.9320) D(x): 0.5769 D(G(z)): 0.3617 / 0.3773 Acc: 7.8125 (17.6905)\n",
      "[0/200][410/782] Loss_D: 1.2564 (1.0288) Loss_G: 1.1430 (0.9325) D(x): 0.5575 D(G(z)): 0.5132 / 0.3634 Acc: 12.5000 (17.6779)\n",
      "[0/200][411/782] Loss_D: 1.0042 (1.0287) Loss_G: 1.0650 (0.9328) D(x): 0.5714 D(G(z)): 0.4404 / 0.3546 Acc: 21.8750 (17.6881)\n",
      "[0/200][412/782] Loss_D: 0.9070 (1.0284) Loss_G: 1.1513 (0.9334) D(x): 0.6111 D(G(z)): 0.4412 / 0.3186 Acc: 20.3125 (17.6945)\n",
      "[0/200][413/782] Loss_D: 0.9488 (1.0282) Loss_G: 1.1221 (0.9338) D(x): 0.5785 D(G(z)): 0.4562 / 0.3234 Acc: 25.0000 (17.7121)\n",
      "[0/200][414/782] Loss_D: 0.8399 (1.0278) Loss_G: 0.9574 (0.9339) D(x): 0.5833 D(G(z)): 0.4165 / 0.3923 Acc: 26.5625 (17.7334)\n",
      "[0/200][415/782] Loss_D: 0.8174 (1.0273) Loss_G: 1.1554 (0.9344) D(x): 0.6562 D(G(z)): 0.4146 / 0.3318 Acc: 15.6250 (17.7284)\n",
      "[0/200][416/782] Loss_D: 0.9263 (1.0270) Loss_G: 1.1723 (0.9350) D(x): 0.5678 D(G(z)): 0.4179 / 0.3221 Acc: 23.4375 (17.7421)\n",
      "[0/200][417/782] Loss_D: 0.8949 (1.0267) Loss_G: 1.0170 (0.9352) D(x): 0.5404 D(G(z)): 0.3562 / 0.3577 Acc: 18.7500 (17.7445)\n",
      "[0/200][418/782] Loss_D: 0.8050 (1.0262) Loss_G: 1.0056 (0.9353) D(x): 0.5931 D(G(z)): 0.3982 / 0.3657 Acc: 20.3125 (17.7506)\n",
      "[0/200][419/782] Loss_D: 0.8074 (1.0257) Loss_G: 1.1151 (0.9358) D(x): 0.6407 D(G(z)): 0.4094 / 0.3364 Acc: 15.6250 (17.7455)\n",
      "[0/200][420/782] Loss_D: 0.7856 (1.0251) Loss_G: 1.0143 (0.9360) D(x): 0.5767 D(G(z)): 0.3690 / 0.3376 Acc: 18.7500 (17.7479)\n",
      "[0/200][421/782] Loss_D: 0.8601 (1.0247) Loss_G: 0.9416 (0.9360) D(x): 0.5810 D(G(z)): 0.4229 / 0.3710 Acc: 15.6250 (17.7429)\n",
      "[0/200][422/782] Loss_D: 1.1235 (1.0249) Loss_G: 1.2420 (0.9367) D(x): 0.5712 D(G(z)): 0.4725 / 0.3072 Acc: 12.5000 (17.7305)\n",
      "[0/200][423/782] Loss_D: 1.1163 (1.0251) Loss_G: 0.9714 (0.9368) D(x): 0.5284 D(G(z)): 0.4644 / 0.3870 Acc: 17.1875 (17.7292)\n",
      "[0/200][424/782] Loss_D: 0.8589 (1.0248) Loss_G: 1.1074 (0.9372) D(x): 0.5753 D(G(z)): 0.4250 / 0.3235 Acc: 17.1875 (17.7279)\n",
      "[0/200][425/782] Loss_D: 0.9329 (1.0245) Loss_G: 0.9716 (0.9373) D(x): 0.5494 D(G(z)): 0.4293 / 0.3891 Acc: 23.4375 (17.7413)\n",
      "[0/200][426/782] Loss_D: 1.1089 (1.0247) Loss_G: 0.8961 (0.9372) D(x): 0.5261 D(G(z)): 0.4805 / 0.3993 Acc: 20.3125 (17.7474)\n",
      "[0/200][427/782] Loss_D: 1.0278 (1.0247) Loss_G: 0.9616 (0.9372) D(x): 0.5372 D(G(z)): 0.4348 / 0.3718 Acc: 21.8750 (17.7570)\n",
      "[0/200][428/782] Loss_D: 1.1591 (1.0251) Loss_G: 1.0248 (0.9374) D(x): 0.5217 D(G(z)): 0.5237 / 0.3425 Acc: 20.3125 (17.7630)\n",
      "[0/200][429/782] Loss_D: 1.0514 (1.0251) Loss_G: 0.8441 (0.9372) D(x): 0.5201 D(G(z)): 0.4928 / 0.4028 Acc: 25.0000 (17.7798)\n",
      "[0/200][430/782] Loss_D: 1.1989 (1.0255) Loss_G: 0.7979 (0.9369) D(x): 0.4993 D(G(z)): 0.4904 / 0.4376 Acc: 18.7500 (17.7820)\n",
      "[0/200][431/782] Loss_D: 0.9028 (1.0252) Loss_G: 0.7565 (0.9365) D(x): 0.5577 D(G(z)): 0.4347 / 0.4568 Acc: 21.8750 (17.7915)\n",
      "[0/200][432/782] Loss_D: 1.0760 (1.0254) Loss_G: 0.8681 (0.9363) D(x): 0.5539 D(G(z)): 0.4725 / 0.4153 Acc: 14.0625 (17.7829)\n",
      "[0/200][433/782] Loss_D: 0.8534 (1.0250) Loss_G: 1.1723 (0.9368) D(x): 0.5831 D(G(z)): 0.4344 / 0.3159 Acc: 23.4375 (17.7959)\n",
      "[0/200][434/782] Loss_D: 0.9433 (1.0248) Loss_G: 1.0247 (0.9370) D(x): 0.5535 D(G(z)): 0.3951 / 0.3530 Acc: 15.6250 (17.7909)\n",
      "[0/200][435/782] Loss_D: 0.9905 (1.0247) Loss_G: 1.0774 (0.9374) D(x): 0.5584 D(G(z)): 0.4082 / 0.3505 Acc: 14.0625 (17.7824)\n",
      "[0/200][436/782] Loss_D: 0.8402 (1.0243) Loss_G: 0.8093 (0.9371) D(x): 0.5525 D(G(z)): 0.3747 / 0.3984 Acc: 14.0625 (17.7739)\n",
      "[0/200][437/782] Loss_D: 0.9207 (1.0240) Loss_G: 0.8695 (0.9369) D(x): 0.6172 D(G(z)): 0.5162 / 0.4057 Acc: 25.0000 (17.7904)\n",
      "[0/200][438/782] Loss_D: 0.8938 (1.0237) Loss_G: 0.9990 (0.9371) D(x): 0.5700 D(G(z)): 0.3790 / 0.3708 Acc: 15.6250 (17.7854)\n",
      "[0/200][439/782] Loss_D: 0.8724 (1.0234) Loss_G: 0.9663 (0.9371) D(x): 0.5621 D(G(z)): 0.4186 / 0.3688 Acc: 23.4375 (17.7983)\n",
      "[0/200][440/782] Loss_D: 0.9885 (1.0233) Loss_G: 1.0759 (0.9374) D(x): 0.5477 D(G(z)): 0.4112 / 0.3615 Acc: 17.1875 (17.7969)\n",
      "[0/200][441/782] Loss_D: 1.0686 (1.0234) Loss_G: 0.8964 (0.9374) D(x): 0.5445 D(G(z)): 0.4693 / 0.4045 Acc: 25.0000 (17.8132)\n",
      "[0/200][442/782] Loss_D: 0.9854 (1.0233) Loss_G: 1.0061 (0.9375) D(x): 0.6043 D(G(z)): 0.5087 / 0.3517 Acc: 21.8750 (17.8224)\n",
      "[0/200][443/782] Loss_D: 1.2116 (1.0238) Loss_G: 0.7266 (0.9370) D(x): 0.4519 D(G(z)): 0.4230 / 0.4603 Acc: 15.6250 (17.8174)\n",
      "[0/200][444/782] Loss_D: 1.1484 (1.0240) Loss_G: 0.8293 (0.9368) D(x): 0.5382 D(G(z)): 0.5331 / 0.4089 Acc: 21.8750 (17.8265)\n",
      "[0/200][445/782] Loss_D: 0.9855 (1.0239) Loss_G: 0.9123 (0.9367) D(x): 0.5364 D(G(z)): 0.4672 / 0.3703 Acc: 18.7500 (17.8286)\n",
      "[0/200][446/782] Loss_D: 1.0244 (1.0239) Loss_G: 1.1530 (0.9372) D(x): 0.5560 D(G(z)): 0.4642 / 0.2871 Acc: 18.7500 (17.8307)\n",
      "[0/200][447/782] Loss_D: 1.3013 (1.0246) Loss_G: 1.0401 (0.9375) D(x): 0.4470 D(G(z)): 0.4940 / 0.3630 Acc: 21.8750 (17.8397)\n",
      "[0/200][448/782] Loss_D: 1.3295 (1.0252) Loss_G: 0.8716 (0.9373) D(x): 0.4543 D(G(z)): 0.4892 / 0.3989 Acc: 21.8750 (17.8487)\n",
      "[0/200][449/782] Loss_D: 1.1708 (1.0256) Loss_G: 0.8263 (0.9371) D(x): 0.4836 D(G(z)): 0.4738 / 0.4043 Acc: 17.1875 (17.8472)\n",
      "[0/200][450/782] Loss_D: 1.1036 (1.0257) Loss_G: 1.1008 (0.9374) D(x): 0.6048 D(G(z)): 0.5013 / 0.3371 Acc: 10.9375 (17.8319)\n",
      "[0/200][451/782] Loss_D: 1.1869 (1.0261) Loss_G: 1.0026 (0.9376) D(x): 0.4715 D(G(z)): 0.4152 / 0.3727 Acc: 15.6250 (17.8270)\n",
      "[0/200][452/782] Loss_D: 1.1174 (1.0263) Loss_G: 0.9553 (0.9376) D(x): 0.6199 D(G(z)): 0.5287 / 0.3896 Acc: 12.5000 (17.8153)\n",
      "[0/200][453/782] Loss_D: 0.9679 (1.0262) Loss_G: 1.1158 (0.9380) D(x): 0.5616 D(G(z)): 0.4551 / 0.3501 Acc: 21.8750 (17.8242)\n",
      "[0/200][454/782] Loss_D: 1.2201 (1.0266) Loss_G: 1.0033 (0.9381) D(x): 0.4879 D(G(z)): 0.4359 / 0.3732 Acc: 10.9375 (17.8091)\n",
      "[0/200][455/782] Loss_D: 0.7434 (1.0260) Loss_G: 0.9272 (0.9381) D(x): 0.6425 D(G(z)): 0.4874 / 0.3569 Acc: 29.6875 (17.8351)\n",
      "[0/200][456/782] Loss_D: 1.1218 (1.0262) Loss_G: 0.8567 (0.9379) D(x): 0.5055 D(G(z)): 0.4859 / 0.3882 Acc: 15.6250 (17.8303)\n",
      "[0/200][457/782] Loss_D: 0.9823 (1.0261) Loss_G: 1.1366 (0.9384) D(x): 0.6664 D(G(z)): 0.4721 / 0.3177 Acc: 9.3750 (17.8118)\n",
      "[0/200][458/782] Loss_D: 0.9263 (1.0259) Loss_G: 1.2258 (0.9390) D(x): 0.5500 D(G(z)): 0.3740 / 0.3170 Acc: 17.1875 (17.8105)\n",
      "[0/200][459/782] Loss_D: 0.8593 (1.0255) Loss_G: 0.9482 (0.9390) D(x): 0.5682 D(G(z)): 0.3717 / 0.3976 Acc: 15.6250 (17.8057)\n",
      "[0/200][460/782] Loss_D: 0.9016 (1.0252) Loss_G: 1.3096 (0.9398) D(x): 0.6250 D(G(z)): 0.4575 / 0.2820 Acc: 15.6250 (17.8010)\n",
      "[0/200][461/782] Loss_D: 0.7113 (1.0246) Loss_G: 1.2694 (0.9405) D(x): 0.6369 D(G(z)): 0.3889 / 0.2915 Acc: 18.7500 (17.8030)\n",
      "[0/200][462/782] Loss_D: 0.7623 (1.0240) Loss_G: 1.2484 (0.9412) D(x): 0.5596 D(G(z)): 0.3544 / 0.2985 Acc: 28.1250 (17.8253)\n",
      "[0/200][463/782] Loss_D: 0.7212 (1.0233) Loss_G: 1.2508 (0.9419) D(x): 0.6145 D(G(z)): 0.4037 / 0.2842 Acc: 23.4375 (17.8374)\n",
      "[0/200][464/782] Loss_D: 0.9883 (1.0233) Loss_G: 1.1744 (0.9424) D(x): 0.5418 D(G(z)): 0.4153 / 0.3218 Acc: 12.5000 (17.8259)\n",
      "[0/200][465/782] Loss_D: 0.9440 (1.0231) Loss_G: 1.3340 (0.9432) D(x): 0.6199 D(G(z)): 0.4356 / 0.2750 Acc: 14.0625 (17.8179)\n",
      "[0/200][466/782] Loss_D: 0.8279 (1.0227) Loss_G: 1.4072 (0.9442) D(x): 0.6163 D(G(z)): 0.4195 / 0.2433 Acc: 10.9375 (17.8031)\n",
      "[0/200][467/782] Loss_D: 1.0100 (1.0227) Loss_G: 1.2124 (0.9448) D(x): 0.4457 D(G(z)): 0.3254 / 0.3068 Acc: 20.3125 (17.8085)\n",
      "[0/200][468/782] Loss_D: 0.8380 (1.0223) Loss_G: 1.1004 (0.9451) D(x): 0.6214 D(G(z)): 0.4306 / 0.3562 Acc: 29.6875 (17.8338)\n",
      "[0/200][469/782] Loss_D: 0.8338 (1.0219) Loss_G: 1.1921 (0.9456) D(x): 0.6701 D(G(z)): 0.4578 / 0.3040 Acc: 20.3125 (17.8391)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][470/782] Loss_D: 0.7486 (1.0213) Loss_G: 1.0125 (0.9458) D(x): 0.5587 D(G(z)): 0.3339 / 0.3367 Acc: 17.1875 (17.8377)\n",
      "[0/200][471/782] Loss_D: 0.7171 (1.0206) Loss_G: 1.4199 (0.9468) D(x): 0.6383 D(G(z)): 0.4301 / 0.2517 Acc: 25.0000 (17.8529)\n",
      "[0/200][472/782] Loss_D: 0.8043 (1.0202) Loss_G: 1.3848 (0.9477) D(x): 0.5864 D(G(z)): 0.3246 / 0.2688 Acc: 9.3750 (17.8350)\n",
      "[0/200][473/782] Loss_D: 0.7519 (1.0196) Loss_G: 1.1448 (0.9481) D(x): 0.6461 D(G(z)): 0.3720 / 0.3064 Acc: 10.9375 (17.8204)\n",
      "[0/200][474/782] Loss_D: 0.7393 (1.0190) Loss_G: 1.1287 (0.9485) D(x): 0.5811 D(G(z)): 0.3281 / 0.3443 Acc: 17.1875 (17.8191)\n",
      "[0/200][475/782] Loss_D: 0.7835 (1.0185) Loss_G: 1.2553 (0.9491) D(x): 0.7041 D(G(z)): 0.3885 / 0.2958 Acc: 6.2500 (17.7948)\n",
      "[0/200][476/782] Loss_D: 0.6452 (1.0177) Loss_G: 1.3120 (0.9499) D(x): 0.6701 D(G(z)): 0.3835 / 0.2639 Acc: 10.9375 (17.7804)\n",
      "[0/200][477/782] Loss_D: 0.7541 (1.0172) Loss_G: 1.0918 (0.9502) D(x): 0.5349 D(G(z)): 0.3201 / 0.2997 Acc: 15.6250 (17.7759)\n",
      "[0/200][478/782] Loss_D: 0.9318 (1.0170) Loss_G: 0.8474 (0.9500) D(x): 0.5867 D(G(z)): 0.4300 / 0.4065 Acc: 9.3750 (17.7584)\n",
      "[0/200][479/782] Loss_D: 0.9164 (1.0168) Loss_G: 1.0113 (0.9501) D(x): 0.6542 D(G(z)): 0.4649 / 0.3612 Acc: 14.0625 (17.7507)\n",
      "[0/200][480/782] Loss_D: 0.9729 (1.0167) Loss_G: 1.1177 (0.9505) D(x): 0.6236 D(G(z)): 0.4794 / 0.3323 Acc: 12.5000 (17.7397)\n",
      "[0/200][481/782] Loss_D: 1.0777 (1.0168) Loss_G: 1.2640 (0.9511) D(x): 0.5272 D(G(z)): 0.4450 / 0.2924 Acc: 17.1875 (17.7386)\n",
      "[0/200][482/782] Loss_D: 1.0716 (1.0170) Loss_G: 0.9009 (0.9510) D(x): 0.4332 D(G(z)): 0.3476 / 0.4017 Acc: 15.6250 (17.7342)\n",
      "[0/200][483/782] Loss_D: 1.1939 (1.0173) Loss_G: 0.9160 (0.9509) D(x): 0.6041 D(G(z)): 0.5359 / 0.3774 Acc: 12.5000 (17.7234)\n",
      "[0/200][484/782] Loss_D: 1.3491 (1.0180) Loss_G: 0.7924 (0.9506) D(x): 0.3910 D(G(z)): 0.4266 / 0.4188 Acc: 15.6250 (17.7191)\n",
      "[0/200][485/782] Loss_D: 1.1341 (1.0182) Loss_G: 0.8588 (0.9504) D(x): 0.5751 D(G(z)): 0.5063 / 0.4124 Acc: 14.0625 (17.7115)\n",
      "[0/200][486/782] Loss_D: 0.9110 (1.0180) Loss_G: 1.2155 (0.9510) D(x): 0.6678 D(G(z)): 0.5015 / 0.2994 Acc: 18.7500 (17.7137)\n",
      "[0/200][487/782] Loss_D: 1.0219 (1.0180) Loss_G: 0.9081 (0.9509) D(x): 0.4801 D(G(z)): 0.3288 / 0.3994 Acc: 9.3750 (17.6966)\n",
      "[0/200][488/782] Loss_D: 0.9669 (1.0179) Loss_G: 0.9154 (0.9508) D(x): 0.5822 D(G(z)): 0.4038 / 0.3979 Acc: 9.3750 (17.6796)\n",
      "[0/200][489/782] Loss_D: 0.8278 (1.0175) Loss_G: 0.9678 (0.9508) D(x): 0.6047 D(G(z)): 0.4428 / 0.3707 Acc: 25.0000 (17.6945)\n",
      "[0/200][490/782] Loss_D: 0.9698 (1.0174) Loss_G: 1.0100 (0.9510) D(x): 0.5791 D(G(z)): 0.4215 / 0.3598 Acc: 12.5000 (17.6839)\n",
      "[0/200][491/782] Loss_D: 0.9801 (1.0174) Loss_G: 1.0048 (0.9511) D(x): 0.5371 D(G(z)): 0.4223 / 0.3524 Acc: 21.8750 (17.6925)\n",
      "[0/200][492/782] Loss_D: 0.9688 (1.0173) Loss_G: 1.0024 (0.9512) D(x): 0.5404 D(G(z)): 0.4157 / 0.3593 Acc: 14.0625 (17.6851)\n",
      "[0/200][493/782] Loss_D: 0.9102 (1.0170) Loss_G: 0.9537 (0.9512) D(x): 0.5989 D(G(z)): 0.4365 / 0.3744 Acc: 17.1875 (17.6841)\n",
      "[0/200][494/782] Loss_D: 1.0828 (1.0172) Loss_G: 0.9276 (0.9511) D(x): 0.5284 D(G(z)): 0.4761 / 0.3825 Acc: 15.6250 (17.6799)\n",
      "[0/200][495/782] Loss_D: 0.9913 (1.0171) Loss_G: 0.9621 (0.9512) D(x): 0.5625 D(G(z)): 0.4445 / 0.3841 Acc: 21.8750 (17.6884)\n",
      "[0/200][496/782] Loss_D: 1.0365 (1.0172) Loss_G: 0.7961 (0.9508) D(x): 0.5364 D(G(z)): 0.3918 / 0.4425 Acc: 10.9375 (17.6748)\n",
      "[0/200][497/782] Loss_D: 0.9359 (1.0170) Loss_G: 0.9467 (0.9508) D(x): 0.6095 D(G(z)): 0.5048 / 0.3824 Acc: 25.0000 (17.6895)\n",
      "[0/200][498/782] Loss_D: 0.9922 (1.0170) Loss_G: 0.9391 (0.9508) D(x): 0.5745 D(G(z)): 0.4372 / 0.3881 Acc: 21.8750 (17.6979)\n",
      "[0/200][499/782] Loss_D: 0.7273 (1.0164) Loss_G: 1.1144 (0.9511) D(x): 0.6168 D(G(z)): 0.3838 / 0.3412 Acc: 26.5625 (17.7156)\n",
      "[0/200][500/782] Loss_D: 0.8513 (1.0160) Loss_G: 0.8963 (0.9510) D(x): 0.5508 D(G(z)): 0.3628 / 0.3897 Acc: 21.8750 (17.7239)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][501/782] Loss_D: 0.8738 (1.0158) Loss_G: 0.8814 (0.9509) D(x): 0.5026 D(G(z)): 0.4213 / 0.3796 Acc: 31.2500 (17.7509)\n",
      "[0/200][502/782] Loss_D: 0.8703 (1.0155) Loss_G: 1.3075 (0.9516) D(x): 0.6429 D(G(z)): 0.4707 / 0.2984 Acc: 25.0000 (17.7653)\n",
      "[0/200][503/782] Loss_D: 0.7895 (1.0150) Loss_G: 1.3887 (0.9525) D(x): 0.5612 D(G(z)): 0.3787 / 0.2401 Acc: 20.3125 (17.7703)\n",
      "[0/200][504/782] Loss_D: 0.7699 (1.0145) Loss_G: 1.0694 (0.9527) D(x): 0.5277 D(G(z)): 0.3509 / 0.3357 Acc: 25.0000 (17.7847)\n",
      "[0/200][505/782] Loss_D: 1.0068 (1.0145) Loss_G: 1.0339 (0.9529) D(x): 0.4823 D(G(z)): 0.3696 / 0.3503 Acc: 18.7500 (17.7866)\n",
      "[0/200][506/782] Loss_D: 0.8831 (1.0143) Loss_G: 1.0817 (0.9531) D(x): 0.6326 D(G(z)): 0.4966 / 0.3405 Acc: 25.0000 (17.8008)\n",
      "[0/200][507/782] Loss_D: 0.6308 (1.0135) Loss_G: 1.2654 (0.9537) D(x): 0.5768 D(G(z)): 0.3520 / 0.2752 Acc: 28.1250 (17.8211)\n",
      "[0/200][508/782] Loss_D: 0.7752 (1.0130) Loss_G: 1.2363 (0.9543) D(x): 0.5150 D(G(z)): 0.3547 / 0.2759 Acc: 25.0000 (17.8352)\n",
      "[0/200][509/782] Loss_D: 0.7357 (1.0125) Loss_G: 1.1931 (0.9548) D(x): 0.5740 D(G(z)): 0.3499 / 0.2893 Acc: 18.7500 (17.8370)\n",
      "[0/200][510/782] Loss_D: 0.8009 (1.0121) Loss_G: 1.1519 (0.9551) D(x): 0.6006 D(G(z)): 0.4051 / 0.3017 Acc: 21.8750 (17.8449)\n",
      "[0/200][511/782] Loss_D: 0.6148 (1.0113) Loss_G: 1.3481 (0.9559) D(x): 0.6524 D(G(z)): 0.3680 / 0.2671 Acc: 21.8750 (17.8528)\n",
      "[0/200][512/782] Loss_D: 0.5987 (1.0105) Loss_G: 1.2437 (0.9565) D(x): 0.6209 D(G(z)): 0.3374 / 0.2899 Acc: 21.8750 (17.8606)\n",
      "[0/200][513/782] Loss_D: 0.6340 (1.0098) Loss_G: 1.2923 (0.9571) D(x): 0.6788 D(G(z)): 0.3381 / 0.2703 Acc: 9.3750 (17.8441)\n",
      "[0/200][514/782] Loss_D: 0.8060 (1.0094) Loss_G: 1.2003 (0.9576) D(x): 0.6501 D(G(z)): 0.4538 / 0.3034 Acc: 21.8750 (17.8519)\n",
      "[0/200][515/782] Loss_D: 0.8410 (1.0090) Loss_G: 1.2406 (0.9581) D(x): 0.6376 D(G(z)): 0.4440 / 0.3025 Acc: 23.4375 (17.8628)\n",
      "[0/200][516/782] Loss_D: 0.7674 (1.0086) Loss_G: 1.1018 (0.9584) D(x): 0.6273 D(G(z)): 0.3409 / 0.3588 Acc: 15.6250 (17.8584)\n",
      "[0/200][517/782] Loss_D: 1.1003 (1.0088) Loss_G: 1.4499 (0.9594) D(x): 0.6007 D(G(z)): 0.4540 / 0.2340 Acc: 6.2500 (17.8360)\n",
      "[0/200][518/782] Loss_D: 1.0090 (1.0088) Loss_G: 1.3771 (0.9602) D(x): 0.5047 D(G(z)): 0.3281 / 0.2506 Acc: 14.0625 (17.8288)\n",
      "[0/200][519/782] Loss_D: 0.7666 (1.0083) Loss_G: 1.2532 (0.9607) D(x): 0.5908 D(G(z)): 0.3660 / 0.2994 Acc: 20.3125 (17.8335)\n",
      "[0/200][520/782] Loss_D: 1.1389 (1.0085) Loss_G: 1.1149 (0.9610) D(x): 0.4961 D(G(z)): 0.4148 / 0.3338 Acc: 12.5000 (17.8233)\n",
      "[0/200][521/782] Loss_D: 0.8299 (1.0082) Loss_G: 1.0919 (0.9613) D(x): 0.6291 D(G(z)): 0.4819 / 0.3107 Acc: 25.0000 (17.8370)\n",
      "[0/200][522/782] Loss_D: 0.8810 (1.0080) Loss_G: 0.7577 (0.9609) D(x): 0.5560 D(G(z)): 0.4326 / 0.4291 Acc: 23.4375 (17.8478)\n",
      "[0/200][523/782] Loss_D: 0.8235 (1.0076) Loss_G: 1.1909 (0.9613) D(x): 0.6430 D(G(z)): 0.5010 / 0.2762 Acc: 20.3125 (17.8525)\n",
      "[0/200][524/782] Loss_D: 1.0326 (1.0077) Loss_G: 1.0443 (0.9615) D(x): 0.4685 D(G(z)): 0.3220 / 0.3367 Acc: 9.3750 (17.8363)\n",
      "[0/200][525/782] Loss_D: 0.9315 (1.0075) Loss_G: 1.1744 (0.9619) D(x): 0.5814 D(G(z)): 0.4410 / 0.3127 Acc: 15.6250 (17.8321)\n",
      "[0/200][526/782] Loss_D: 1.0096 (1.0075) Loss_G: 0.9556 (0.9619) D(x): 0.5414 D(G(z)): 0.3875 / 0.3876 Acc: 12.5000 (17.8220)\n",
      "[0/200][527/782] Loss_D: 1.0017 (1.0075) Loss_G: 1.3816 (0.9627) D(x): 0.6237 D(G(z)): 0.4919 / 0.2651 Acc: 15.6250 (17.8178)\n",
      "[0/200][528/782] Loss_D: 0.6111 (1.0068) Loss_G: 1.3494 (0.9634) D(x): 0.6467 D(G(z)): 0.3083 / 0.2714 Acc: 21.8750 (17.8255)\n",
      "[0/200][529/782] Loss_D: 0.6386 (1.0061) Loss_G: 1.2174 (0.9639) D(x): 0.6337 D(G(z)): 0.3927 / 0.2812 Acc: 15.6250 (17.8213)\n",
      "[0/200][530/782] Loss_D: 0.8718 (1.0058) Loss_G: 1.3736 (0.9647) D(x): 0.5596 D(G(z)): 0.4226 / 0.2696 Acc: 25.0000 (17.8349)\n",
      "[0/200][531/782] Loss_D: 1.2569 (1.0063) Loss_G: 1.2533 (0.9652) D(x): 0.5517 D(G(z)): 0.4896 / 0.2994 Acc: 14.0625 (17.8278)\n",
      "[0/200][532/782] Loss_D: 0.8401 (1.0060) Loss_G: 1.4353 (0.9661) D(x): 0.6014 D(G(z)): 0.4038 / 0.2491 Acc: 10.9375 (17.8148)\n",
      "[0/200][533/782] Loss_D: 1.0979 (1.0061) Loss_G: 1.0734 (0.9663) D(x): 0.5037 D(G(z)): 0.4503 / 0.3890 Acc: 28.1250 (17.8342)\n",
      "[0/200][534/782] Loss_D: 1.2184 (1.0065) Loss_G: 1.2727 (0.9669) D(x): 0.6895 D(G(z)): 0.6359 / 0.2952 Acc: 17.1875 (17.8329)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][535/782] Loss_D: 1.8989 (1.0082) Loss_G: 0.9642 (0.9669) D(x): 0.3016 D(G(z)): 0.5175 / 0.3705 Acc: 23.4375 (17.8434)\n",
      "[0/200][536/782] Loss_D: 1.5620 (1.0092) Loss_G: 0.4927 (0.9660) D(x): 0.3550 D(G(z)): 0.5006 / 0.5330 Acc: 15.6250 (17.8393)\n",
      "[0/200][537/782] Loss_D: 1.4134 (1.0100) Loss_G: 0.7351 (0.9655) D(x): 0.5638 D(G(z)): 0.6032 / 0.4685 Acc: 17.1875 (17.8381)\n",
      "[0/200][538/782] Loss_D: 1.1039 (1.0102) Loss_G: 1.3891 (0.9663) D(x): 0.5747 D(G(z)): 0.4898 / 0.2563 Acc: 15.6250 (17.8340)\n",
      "[0/200][539/782] Loss_D: 1.2805 (1.0107) Loss_G: 1.0877 (0.9665) D(x): 0.4005 D(G(z)): 0.3934 / 0.3465 Acc: 17.1875 (17.8328)\n",
      "[0/200][540/782] Loss_D: 0.9771 (1.0106) Loss_G: 0.9491 (0.9665) D(x): 0.5446 D(G(z)): 0.4419 / 0.3526 Acc: 15.6250 (17.8287)\n",
      "[0/200][541/782] Loss_D: 0.8887 (1.0104) Loss_G: 1.1333 (0.9668) D(x): 0.5799 D(G(z)): 0.4264 / 0.3353 Acc: 17.1875 (17.8275)\n",
      "[0/200][542/782] Loss_D: 0.8954 (1.0102) Loss_G: 0.9546 (0.9668) D(x): 0.5617 D(G(z)): 0.4331 / 0.3708 Acc: 20.3125 (17.8321)\n",
      "[0/200][543/782] Loss_D: 0.8173 (1.0098) Loss_G: 1.0114 (0.9669) D(x): 0.5828 D(G(z)): 0.4237 / 0.3391 Acc: 25.0000 (17.8452)\n",
      "[0/200][544/782] Loss_D: 0.7926 (1.0094) Loss_G: 0.8723 (0.9667) D(x): 0.5789 D(G(z)): 0.4219 / 0.4025 Acc: 21.8750 (17.8526)\n",
      "[0/200][545/782] Loss_D: 0.8739 (1.0092) Loss_G: 0.8235 (0.9664) D(x): 0.5892 D(G(z)): 0.4465 / 0.4256 Acc: 21.8750 (17.8600)\n",
      "[0/200][546/782] Loss_D: 0.9377 (1.0090) Loss_G: 0.8717 (0.9663) D(x): 0.5761 D(G(z)): 0.4760 / 0.3988 Acc: 21.8750 (17.8673)\n",
      "[0/200][547/782] Loss_D: 0.9962 (1.0090) Loss_G: 0.8227 (0.9660) D(x): 0.5732 D(G(z)): 0.4751 / 0.4289 Acc: 17.1875 (17.8661)\n",
      "[0/200][548/782] Loss_D: 1.0071 (1.0090) Loss_G: 0.7534 (0.9656) D(x): 0.5090 D(G(z)): 0.4380 / 0.4227 Acc: 20.3125 (17.8706)\n",
      "[0/200][549/782] Loss_D: 1.0452 (1.0091) Loss_G: 0.9686 (0.9656) D(x): 0.5631 D(G(z)): 0.4907 / 0.3711 Acc: 17.1875 (17.8693)\n",
      "[0/200][550/782] Loss_D: 1.0390 (1.0091) Loss_G: 0.9970 (0.9657) D(x): 0.5846 D(G(z)): 0.5058 / 0.3771 Acc: 18.7500 (17.8709)\n",
      "[0/200][551/782] Loss_D: 0.7788 (1.0087) Loss_G: 0.9598 (0.9657) D(x): 0.5969 D(G(z)): 0.4306 / 0.3691 Acc: 29.6875 (17.8923)\n",
      "[0/200][552/782] Loss_D: 0.9269 (1.0086) Loss_G: 0.7269 (0.9652) D(x): 0.5197 D(G(z)): 0.4382 / 0.4596 Acc: 29.6875 (17.9137)\n",
      "[0/200][553/782] Loss_D: 0.9462 (1.0084) Loss_G: 0.9360 (0.9652) D(x): 0.5713 D(G(z)): 0.4733 / 0.3690 Acc: 18.7500 (17.9152)\n",
      "[0/200][554/782] Loss_D: 1.0323 (1.0085) Loss_G: 0.9163 (0.9651) D(x): 0.5515 D(G(z)): 0.5070 / 0.3876 Acc: 26.5625 (17.9307)\n",
      "[0/200][555/782] Loss_D: 0.8109 (1.0081) Loss_G: 0.9223 (0.9650) D(x): 0.5918 D(G(z)): 0.4341 / 0.3934 Acc: 25.0000 (17.9435)\n",
      "[0/200][556/782] Loss_D: 0.9539 (1.0080) Loss_G: 0.7150 (0.9646) D(x): 0.5083 D(G(z)): 0.4294 / 0.4368 Acc: 23.4375 (17.9533)\n",
      "[0/200][557/782] Loss_D: 0.9533 (1.0079) Loss_G: 0.7440 (0.9642) D(x): 0.5394 D(G(z)): 0.4573 / 0.4328 Acc: 20.3125 (17.9575)\n",
      "[0/200][558/782] Loss_D: 0.8150 (1.0076) Loss_G: 1.0597 (0.9644) D(x): 0.6466 D(G(z)): 0.4914 / 0.3225 Acc: 23.4375 (17.9674)\n",
      "[0/200][559/782] Loss_D: 0.8974 (1.0074) Loss_G: 1.0032 (0.9644) D(x): 0.5543 D(G(z)): 0.4151 / 0.3710 Acc: 28.1250 (17.9855)\n",
      "[0/200][560/782] Loss_D: 0.8511 (1.0071) Loss_G: 0.9434 (0.9644) D(x): 0.5472 D(G(z)): 0.4148 / 0.3908 Acc: 31.2500 (18.0091)\n",
      "[0/200][561/782] Loss_D: 0.9159 (1.0070) Loss_G: 0.6013 (0.9637) D(x): 0.5502 D(G(z)): 0.4459 / 0.4757 Acc: 21.8750 (18.0160)\n",
      "[0/200][562/782] Loss_D: 1.0968 (1.0071) Loss_G: 0.8677 (0.9636) D(x): 0.5608 D(G(z)): 0.5218 / 0.3961 Acc: 17.1875 (18.0145)\n",
      "[0/200][563/782] Loss_D: 0.9849 (1.0071) Loss_G: 0.8944 (0.9634) D(x): 0.5467 D(G(z)): 0.4795 / 0.4008 Acc: 31.2500 (18.0380)\n",
      "[0/200][564/782] Loss_D: 0.9648 (1.0070) Loss_G: 0.8936 (0.9633) D(x): 0.5644 D(G(z)): 0.4414 / 0.4057 Acc: 18.7500 (18.0393)\n",
      "[0/200][565/782] Loss_D: 0.9129 (1.0068) Loss_G: 0.8332 (0.9631) D(x): 0.5340 D(G(z)): 0.4004 / 0.4188 Acc: 25.0000 (18.0516)\n",
      "[0/200][566/782] Loss_D: 0.7483 (1.0064) Loss_G: 0.8260 (0.9629) D(x): 0.5679 D(G(z)): 0.3834 / 0.4048 Acc: 21.8750 (18.0583)\n",
      "[0/200][567/782] Loss_D: 0.6726 (1.0058) Loss_G: 0.9287 (0.9628) D(x): 0.6322 D(G(z)): 0.4277 / 0.3821 Acc: 28.1250 (18.0760)\n",
      "[0/200][568/782] Loss_D: 0.7661 (1.0054) Loss_G: 1.2803 (0.9633) D(x): 0.6378 D(G(z)): 0.4096 / 0.3000 Acc: 26.5625 (18.0909)\n",
      "[0/200][569/782] Loss_D: 0.6920 (1.0048) Loss_G: 1.2865 (0.9639) D(x): 0.6135 D(G(z)): 0.4059 / 0.2887 Acc: 28.1250 (18.1086)\n",
      "[0/200][570/782] Loss_D: 0.7013 (1.0043) Loss_G: 0.9950 (0.9640) D(x): 0.5722 D(G(z)): 0.3462 / 0.3544 Acc: 20.3125 (18.1124)\n",
      "[0/200][571/782] Loss_D: 0.7679 (1.0039) Loss_G: 1.1400 (0.9643) D(x): 0.6299 D(G(z)): 0.4304 / 0.3389 Acc: 20.3125 (18.1163)\n",
      "[0/200][572/782] Loss_D: 0.8361 (1.0036) Loss_G: 0.9923 (0.9643) D(x): 0.5291 D(G(z)): 0.3451 / 0.3645 Acc: 15.6250 (18.1119)\n",
      "[0/200][573/782] Loss_D: 0.9485 (1.0035) Loss_G: 1.0184 (0.9644) D(x): 0.5766 D(G(z)): 0.3902 / 0.3738 Acc: 15.6250 (18.1076)\n",
      "[0/200][574/782] Loss_D: 0.8987 (1.0033) Loss_G: 0.9750 (0.9644) D(x): 0.6433 D(G(z)): 0.4709 / 0.3716 Acc: 10.9375 (18.0951)\n",
      "[0/200][575/782] Loss_D: 1.1135 (1.0035) Loss_G: 0.9727 (0.9645) D(x): 0.5709 D(G(z)): 0.5168 / 0.3814 Acc: 25.0000 (18.1071)\n",
      "[0/200][576/782] Loss_D: 0.9047 (1.0033) Loss_G: 1.0946 (0.9647) D(x): 0.5504 D(G(z)): 0.3982 / 0.3258 Acc: 17.1875 (18.1055)\n",
      "[0/200][577/782] Loss_D: 1.1280 (1.0035) Loss_G: 0.6463 (0.9641) D(x): 0.4955 D(G(z)): 0.4518 / 0.4741 Acc: 10.9375 (18.0931)\n",
      "[0/200][578/782] Loss_D: 1.0229 (1.0036) Loss_G: 0.6896 (0.9637) D(x): 0.5531 D(G(z)): 0.5039 / 0.4422 Acc: 23.4375 (18.1023)\n",
      "[0/200][579/782] Loss_D: 1.3177 (1.0041) Loss_G: 0.9424 (0.9636) D(x): 0.4562 D(G(z)): 0.4684 / 0.4004 Acc: 18.7500 (18.1034)\n",
      "[0/200][580/782] Loss_D: 1.1013 (1.0043) Loss_G: 0.8949 (0.9635) D(x): 0.5010 D(G(z)): 0.4774 / 0.4082 Acc: 25.0000 (18.1153)\n",
      "[0/200][581/782] Loss_D: 0.8637 (1.0040) Loss_G: 0.8895 (0.9634) D(x): 0.5996 D(G(z)): 0.4445 / 0.3869 Acc: 21.8750 (18.1218)\n",
      "[0/200][582/782] Loss_D: 0.8390 (1.0038) Loss_G: 0.9856 (0.9634) D(x): 0.6003 D(G(z)): 0.4655 / 0.3321 Acc: 20.3125 (18.1255)\n",
      "[0/200][583/782] Loss_D: 0.7300 (1.0033) Loss_G: 1.1645 (0.9638) D(x): 0.5809 D(G(z)): 0.3549 / 0.3094 Acc: 18.7500 (18.1266)\n",
      "[0/200][584/782] Loss_D: 0.6556 (1.0027) Loss_G: 1.1291 (0.9640) D(x): 0.5308 D(G(z)): 0.3496 / 0.3254 Acc: 32.8125 (18.1517)\n",
      "[0/200][585/782] Loss_D: 0.7999 (1.0023) Loss_G: 0.9046 (0.9639) D(x): 0.5607 D(G(z)): 0.3847 / 0.4224 Acc: 20.3125 (18.1554)\n",
      "[0/200][586/782] Loss_D: 0.8662 (1.0021) Loss_G: 0.9986 (0.9640) D(x): 0.6982 D(G(z)): 0.5335 / 0.3198 Acc: 26.5625 (18.1697)\n",
      "[0/200][587/782] Loss_D: 0.7207 (1.0016) Loss_G: 1.2008 (0.9644) D(x): 0.6175 D(G(z)): 0.3776 / 0.3333 Acc: 28.1250 (18.1866)\n",
      "[0/200][588/782] Loss_D: 0.6041 (1.0010) Loss_G: 1.1495 (0.9647) D(x): 0.6169 D(G(z)): 0.3515 / 0.2919 Acc: 21.8750 (18.1929)\n",
      "[0/200][589/782] Loss_D: 1.0975 (1.0011) Loss_G: 0.9306 (0.9647) D(x): 0.4878 D(G(z)): 0.4095 / 0.3876 Acc: 17.1875 (18.1912)\n",
      "[0/200][590/782] Loss_D: 0.7187 (1.0006) Loss_G: 0.8636 (0.9645) D(x): 0.6361 D(G(z)): 0.4113 / 0.3907 Acc: 18.7500 (18.1922)\n",
      "[0/200][591/782] Loss_D: 0.7891 (1.0003) Loss_G: 0.9088 (0.9644) D(x): 0.6191 D(G(z)): 0.4562 / 0.3623 Acc: 23.4375 (18.2010)\n",
      "[0/200][592/782] Loss_D: 0.8983 (1.0001) Loss_G: 0.9534 (0.9644) D(x): 0.5626 D(G(z)): 0.4285 / 0.3763 Acc: 20.3125 (18.2046)\n",
      "[0/200][593/782] Loss_D: 0.9668 (1.0001) Loss_G: 0.8326 (0.9641) D(x): 0.4967 D(G(z)): 0.3927 / 0.3923 Acc: 18.7500 (18.2055)\n",
      "[0/200][594/782] Loss_D: 0.9042 (0.9999) Loss_G: 0.7692 (0.9638) D(x): 0.5983 D(G(z)): 0.4860 / 0.4271 Acc: 21.8750 (18.2117)\n",
      "[0/200][595/782] Loss_D: 0.9204 (0.9998) Loss_G: 0.8389 (0.9636) D(x): 0.5662 D(G(z)): 0.4789 / 0.3899 Acc: 28.1250 (18.2283)\n",
      "[0/200][596/782] Loss_D: 1.3781 (1.0004) Loss_G: 0.9575 (0.9636) D(x): 0.4346 D(G(z)): 0.4736 / 0.3697 Acc: 9.3750 (18.2135)\n",
      "[0/200][597/782] Loss_D: 1.2952 (1.0009) Loss_G: 0.6287 (0.9630) D(x): 0.4450 D(G(z)): 0.4956 / 0.4958 Acc: 23.4375 (18.2222)\n",
      "[0/200][598/782] Loss_D: 1.0652 (1.0010) Loss_G: 0.9403 (0.9630) D(x): 0.6261 D(G(z)): 0.5590 / 0.3665 Acc: 18.7500 (18.2231)\n",
      "[0/200][599/782] Loss_D: 1.2120 (1.0014) Loss_G: 0.8968 (0.9629) D(x): 0.4402 D(G(z)): 0.4434 / 0.3876 Acc: 14.0625 (18.2161)\n",
      "[0/200][600/782] Loss_D: 1.3207 (1.0019) Loss_G: 0.7993 (0.9626) D(x): 0.4003 D(G(z)): 0.4679 / 0.4235 Acc: 21.8750 (18.2222)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][601/782] Loss_D: 0.8917 (1.0017) Loss_G: 0.8406 (0.9624) D(x): 0.5983 D(G(z)): 0.4728 / 0.3952 Acc: 21.8750 (18.2283)\n",
      "[0/200][602/782] Loss_D: 0.7815 (1.0013) Loss_G: 0.9335 (0.9624) D(x): 0.5742 D(G(z)): 0.4310 / 0.3602 Acc: 29.6875 (18.2473)\n",
      "[0/200][603/782] Loss_D: 0.7823 (1.0010) Loss_G: 1.0680 (0.9625) D(x): 0.5850 D(G(z)): 0.4549 / 0.3393 Acc: 32.8125 (18.2714)\n",
      "[0/200][604/782] Loss_D: 0.8910 (1.0008) Loss_G: 0.8755 (0.9624) D(x): 0.5606 D(G(z)): 0.4170 / 0.4255 Acc: 21.8750 (18.2774)\n",
      "[0/200][605/782] Loss_D: 0.7583 (1.0004) Loss_G: 1.4049 (0.9631) D(x): 0.6030 D(G(z)): 0.4000 / 0.2474 Acc: 20.3125 (18.2807)\n",
      "[0/200][606/782] Loss_D: 0.9165 (1.0003) Loss_G: 1.3153 (0.9637) D(x): 0.5217 D(G(z)): 0.3789 / 0.2751 Acc: 20.3125 (18.2841)\n",
      "[0/200][607/782] Loss_D: 0.9487 (1.0002) Loss_G: 1.1429 (0.9640) D(x): 0.5574 D(G(z)): 0.3923 / 0.3382 Acc: 17.1875 (18.2823)\n",
      "[0/200][608/782] Loss_D: 0.8425 (0.9999) Loss_G: 0.9281 (0.9639) D(x): 0.5719 D(G(z)): 0.4107 / 0.3568 Acc: 17.1875 (18.2805)\n",
      "[0/200][609/782] Loss_D: 0.9638 (0.9998) Loss_G: 1.2802 (0.9645) D(x): 0.6041 D(G(z)): 0.5005 / 0.2790 Acc: 25.0000 (18.2915)\n",
      "[0/200][610/782] Loss_D: 0.6929 (0.9993) Loss_G: 1.1323 (0.9647) D(x): 0.5361 D(G(z)): 0.3361 / 0.3012 Acc: 25.0000 (18.3025)\n",
      "[0/200][611/782] Loss_D: 0.5940 (0.9987) Loss_G: 0.7398 (0.9644) D(x): 0.5204 D(G(z)): 0.3260 / 0.4084 Acc: 28.1250 (18.3185)\n",
      "[0/200][612/782] Loss_D: 0.6693 (0.9981) Loss_G: 0.9555 (0.9644) D(x): 0.6590 D(G(z)): 0.4419 / 0.3485 Acc: 23.4375 (18.3269)\n",
      "[0/200][613/782] Loss_D: 0.5409 (0.9974) Loss_G: 1.0432 (0.9645) D(x): 0.6333 D(G(z)): 0.3841 / 0.3315 Acc: 28.1250 (18.3428)\n",
      "[0/200][614/782] Loss_D: 0.7944 (0.9971) Loss_G: 1.4407 (0.9653) D(x): 0.6597 D(G(z)): 0.4390 / 0.2461 Acc: 18.7500 (18.3435)\n",
      "[0/200][615/782] Loss_D: 0.7900 (0.9967) Loss_G: 1.2540 (0.9657) D(x): 0.5399 D(G(z)): 0.3171 / 0.2814 Acc: 17.1875 (18.3416)\n",
      "[0/200][616/782] Loss_D: 1.0018 (0.9967) Loss_G: 0.6932 (0.9653) D(x): 0.5057 D(G(z)): 0.4788 / 0.4486 Acc: 31.2500 (18.3625)\n",
      "[0/200][617/782] Loss_D: 1.0716 (0.9969) Loss_G: 1.0021 (0.9653) D(x): 0.6170 D(G(z)): 0.5821 / 0.3765 Acc: 31.2500 (18.3834)\n",
      "[0/200][618/782] Loss_D: 1.0810 (0.9970) Loss_G: 0.7597 (0.9650) D(x): 0.4923 D(G(z)): 0.4928 / 0.4284 Acc: 29.6875 (18.4017)\n",
      "[0/200][619/782] Loss_D: 1.1403 (0.9972) Loss_G: 0.8433 (0.9648) D(x): 0.5048 D(G(z)): 0.4902 / 0.4171 Acc: 17.1875 (18.3997)\n",
      "[0/200][620/782] Loss_D: 1.0314 (0.9973) Loss_G: 0.6037 (0.9642) D(x): 0.4738 D(G(z)): 0.4986 / 0.4964 Acc: 37.5000 (18.4305)\n",
      "[0/200][621/782] Loss_D: 1.1218 (0.9975) Loss_G: 0.7318 (0.9639) D(x): 0.5292 D(G(z)): 0.5199 / 0.4462 Acc: 21.8750 (18.4360)\n",
      "[0/200][622/782] Loss_D: 0.8362 (0.9972) Loss_G: 0.9200 (0.9638) D(x): 0.5685 D(G(z)): 0.4621 / 0.3754 Acc: 32.8125 (18.4591)\n",
      "[0/200][623/782] Loss_D: 0.9908 (0.9972) Loss_G: 0.9528 (0.9638) D(x): 0.4924 D(G(z)): 0.4586 / 0.3683 Acc: 28.1250 (18.4746)\n",
      "[0/200][624/782] Loss_D: 0.7961 (0.9969) Loss_G: 0.9010 (0.9637) D(x): 0.5581 D(G(z)): 0.4211 / 0.3503 Acc: 23.4375 (18.4825)\n",
      "[0/200][625/782] Loss_D: 0.9414 (0.9968) Loss_G: 0.9018 (0.9636) D(x): 0.4998 D(G(z)): 0.4345 / 0.4073 Acc: 25.0000 (18.4929)\n",
      "[0/200][626/782] Loss_D: 0.8471 (0.9966) Loss_G: 0.7272 (0.9632) D(x): 0.5609 D(G(z)): 0.4221 / 0.4368 Acc: 17.1875 (18.4908)\n",
      "[0/200][627/782] Loss_D: 0.6880 (0.9961) Loss_G: 0.8580 (0.9630) D(x): 0.6408 D(G(z)): 0.4185 / 0.3974 Acc: 21.8750 (18.4962)\n",
      "[0/200][628/782] Loss_D: 0.8431 (0.9958) Loss_G: 0.8588 (0.9629) D(x): 0.5915 D(G(z)): 0.4119 / 0.3954 Acc: 20.3125 (18.4991)\n",
      "[0/200][629/782] Loss_D: 0.7125 (0.9954) Loss_G: 0.9352 (0.9628) D(x): 0.6562 D(G(z)): 0.4639 / 0.3546 Acc: 25.0000 (18.5094)\n",
      "[0/200][630/782] Loss_D: 0.5982 (0.9948) Loss_G: 1.4670 (0.9636) D(x): 0.6465 D(G(z)): 0.3802 / 0.2366 Acc: 29.6875 (18.5271)\n",
      "[0/200][631/782] Loss_D: 0.7330 (0.9943) Loss_G: 0.9393 (0.9636) D(x): 0.5473 D(G(z)): 0.3001 / 0.3690 Acc: 17.1875 (18.5250)\n",
      "[0/200][632/782] Loss_D: 0.5555 (0.9936) Loss_G: 0.8072 (0.9633) D(x): 0.6386 D(G(z)): 0.4358 / 0.3927 Acc: 31.2500 (18.5451)\n",
      "[0/200][633/782] Loss_D: 0.9031 (0.9935) Loss_G: 1.2936 (0.9639) D(x): 0.5713 D(G(z)): 0.4814 / 0.2718 Acc: 20.3125 (18.5479)\n",
      "[0/200][634/782] Loss_D: 0.9605 (0.9935) Loss_G: 1.1428 (0.9641) D(x): 0.5450 D(G(z)): 0.4313 / 0.3000 Acc: 17.1875 (18.5458)\n",
      "[0/200][635/782] Loss_D: 0.8532 (0.9932) Loss_G: 0.8905 (0.9640) D(x): 0.5164 D(G(z)): 0.3786 / 0.3561 Acc: 15.6250 (18.5412)\n",
      "[0/200][636/782] Loss_D: 0.7782 (0.9929) Loss_G: 1.4378 (0.9648) D(x): 0.6701 D(G(z)): 0.4575 / 0.2536 Acc: 18.7500 (18.5415)\n",
      "[0/200][637/782] Loss_D: 0.8945 (0.9927) Loss_G: 1.1849 (0.9651) D(x): 0.5350 D(G(z)): 0.3983 / 0.2986 Acc: 17.1875 (18.5394)\n",
      "[0/200][638/782] Loss_D: 0.9859 (0.9927) Loss_G: 1.1423 (0.9654) D(x): 0.5795 D(G(z)): 0.4564 / 0.3188 Acc: 15.6250 (18.5348)\n",
      "[0/200][639/782] Loss_D: 1.0195 (0.9928) Loss_G: 1.1415 (0.9657) D(x): 0.4810 D(G(z)): 0.4024 / 0.3126 Acc: 18.7500 (18.5352)\n",
      "[0/200][640/782] Loss_D: 0.8347 (0.9925) Loss_G: 1.2004 (0.9660) D(x): 0.5657 D(G(z)): 0.3886 / 0.2955 Acc: 18.7500 (18.5355)\n",
      "[0/200][641/782] Loss_D: 0.8165 (0.9922) Loss_G: 1.0694 (0.9662) D(x): 0.5993 D(G(z)): 0.4050 / 0.3475 Acc: 18.7500 (18.5358)\n",
      "[0/200][642/782] Loss_D: 0.6924 (0.9918) Loss_G: 1.1825 (0.9665) D(x): 0.6139 D(G(z)): 0.3621 / 0.2995 Acc: 15.6250 (18.5313)\n",
      "[0/200][643/782] Loss_D: 0.6905 (0.9913) Loss_G: 0.8708 (0.9664) D(x): 0.5758 D(G(z)): 0.3687 / 0.3947 Acc: 23.4375 (18.5389)\n",
      "[0/200][644/782] Loss_D: 0.6886 (0.9908) Loss_G: 0.8343 (0.9662) D(x): 0.6913 D(G(z)): 0.3976 / 0.3878 Acc: 14.0625 (18.5320)\n",
      "[0/200][645/782] Loss_D: 0.8740 (0.9907) Loss_G: 1.7051 (0.9673) D(x): 0.6488 D(G(z)): 0.5017 / 0.1857 Acc: 21.8750 (18.5372)\n",
      "[0/200][646/782] Loss_D: 0.7820 (0.9903) Loss_G: 1.0818 (0.9675) D(x): 0.4743 D(G(z)): 0.2615 / 0.3259 Acc: 18.7500 (18.5375)\n",
      "[0/200][647/782] Loss_D: 1.1073 (0.9905) Loss_G: 1.1959 (0.9678) D(x): 0.5837 D(G(z)): 0.5025 / 0.3146 Acc: 17.1875 (18.5354)\n",
      "[0/200][648/782] Loss_D: 0.7954 (0.9902) Loss_G: 1.0908 (0.9680) D(x): 0.5501 D(G(z)): 0.4100 / 0.3253 Acc: 23.4375 (18.5430)\n",
      "[0/200][649/782] Loss_D: 0.8363 (0.9900) Loss_G: 0.6320 (0.9675) D(x): 0.5285 D(G(z)): 0.3997 / 0.4378 Acc: 18.7500 (18.5433)\n",
      "[0/200][650/782] Loss_D: 1.0408 (0.9901) Loss_G: 0.9887 (0.9676) D(x): 0.6076 D(G(z)): 0.5277 / 0.3446 Acc: 15.6250 (18.5388)\n",
      "[0/200][651/782] Loss_D: 0.8987 (0.9899) Loss_G: 1.3818 (0.9682) D(x): 0.6080 D(G(z)): 0.4822 / 0.2455 Acc: 20.3125 (18.5415)\n",
      "[0/200][652/782] Loss_D: 1.1000 (0.9901) Loss_G: 0.7981 (0.9679) D(x): 0.3581 D(G(z)): 0.3357 / 0.3708 Acc: 20.3125 (18.5442)\n",
      "[0/200][653/782] Loss_D: 1.1435 (0.9903) Loss_G: 0.8170 (0.9677) D(x): 0.4768 D(G(z)): 0.4806 / 0.4278 Acc: 29.6875 (18.5613)\n",
      "[0/200][654/782] Loss_D: 0.9565 (0.9903) Loss_G: 1.0169 (0.9678) D(x): 0.6543 D(G(z)): 0.5544 / 0.3628 Acc: 23.4375 (18.5687)\n",
      "[0/200][655/782] Loss_D: 1.0929 (0.9904) Loss_G: 1.3307 (0.9683) D(x): 0.6185 D(G(z)): 0.5155 / 0.2720 Acc: 10.9375 (18.5571)\n",
      "[0/200][656/782] Loss_D: 0.8416 (0.9902) Loss_G: 1.2701 (0.9688) D(x): 0.5597 D(G(z)): 0.3919 / 0.2680 Acc: 12.5000 (18.5479)\n",
      "[0/200][657/782] Loss_D: 0.8605 (0.9900) Loss_G: 1.0168 (0.9689) D(x): 0.4888 D(G(z)): 0.3188 / 0.3370 Acc: 17.1875 (18.5458)\n",
      "[0/200][658/782] Loss_D: 0.9211 (0.9899) Loss_G: 1.2779 (0.9693) D(x): 0.6124 D(G(z)): 0.4801 / 0.2738 Acc: 10.9375 (18.5342)\n",
      "[0/200][659/782] Loss_D: 0.7437 (0.9895) Loss_G: 1.2932 (0.9698) D(x): 0.5898 D(G(z)): 0.3866 / 0.2547 Acc: 12.5000 (18.5251)\n",
      "[0/200][660/782] Loss_D: 0.9692 (0.9895) Loss_G: 1.0638 (0.9700) D(x): 0.4894 D(G(z)): 0.3287 / 0.3369 Acc: 9.3750 (18.5113)\n",
      "[0/200][661/782] Loss_D: 0.8340 (0.9893) Loss_G: 1.2171 (0.9703) D(x): 0.6473 D(G(z)): 0.4994 / 0.2614 Acc: 9.3750 (18.4975)\n",
      "[0/200][662/782] Loss_D: 0.8267 (0.9890) Loss_G: 1.6414 (0.9713) D(x): 0.6016 D(G(z)): 0.4172 / 0.1960 Acc: 14.0625 (18.4908)\n",
      "[0/200][663/782] Loss_D: 0.8548 (0.9888) Loss_G: 1.3202 (0.9719) D(x): 0.5047 D(G(z)): 0.2686 / 0.2673 Acc: 9.3750 (18.4770)\n",
      "[0/200][664/782] Loss_D: 0.6684 (0.9883) Loss_G: 0.9941 (0.9719) D(x): 0.6174 D(G(z)): 0.3544 / 0.3325 Acc: 14.0625 (18.4704)\n",
      "[0/200][665/782] Loss_D: 0.9065 (0.9882) Loss_G: 1.5367 (0.9728) D(x): 0.6903 D(G(z)): 0.4777 / 0.2074 Acc: 6.2500 (18.4520)\n",
      "[0/200][666/782] Loss_D: 0.9488 (0.9882) Loss_G: 1.5512 (0.9736) D(x): 0.5234 D(G(z)): 0.4136 / 0.1943 Acc: 17.1875 (18.4501)\n",
      "[0/200][667/782] Loss_D: 0.7519 (0.9878) Loss_G: 1.2468 (0.9740) D(x): 0.5313 D(G(z)): 0.2999 / 0.2907 Acc: 17.1875 (18.4483)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][668/782] Loss_D: 1.0136 (0.9878) Loss_G: 0.7976 (0.9738) D(x): 0.4582 D(G(z)): 0.4461 / 0.3703 Acc: 20.3125 (18.4510)\n",
      "[0/200][669/782] Loss_D: 0.7961 (0.9876) Loss_G: 1.1603 (0.9740) D(x): 0.6111 D(G(z)): 0.4320 / 0.3061 Acc: 20.3125 (18.4538)\n",
      "[0/200][670/782] Loss_D: 0.6854 (0.9871) Loss_G: 1.2252 (0.9744) D(x): 0.5970 D(G(z)): 0.4130 / 0.2731 Acc: 23.4375 (18.4613)\n",
      "[0/200][671/782] Loss_D: 0.5253 (0.9864) Loss_G: 1.3748 (0.9750) D(x): 0.6491 D(G(z)): 0.3396 / 0.2698 Acc: 25.0000 (18.4710)\n",
      "[0/200][672/782] Loss_D: 0.6459 (0.9859) Loss_G: 1.3277 (0.9755) D(x): 0.6324 D(G(z)): 0.3886 / 0.2490 Acc: 21.8750 (18.4760)\n",
      "[0/200][673/782] Loss_D: 0.5274 (0.9852) Loss_G: 1.2222 (0.9759) D(x): 0.6389 D(G(z)): 0.3216 / 0.2885 Acc: 21.8750 (18.4811)\n",
      "[0/200][674/782] Loss_D: 0.8635 (0.9850) Loss_G: 1.1088 (0.9761) D(x): 0.6156 D(G(z)): 0.4130 / 0.3395 Acc: 18.7500 (18.4815)\n",
      "[0/200][675/782] Loss_D: 0.9010 (0.9849) Loss_G: 1.1172 (0.9763) D(x): 0.6823 D(G(z)): 0.5221 / 0.2946 Acc: 18.7500 (18.4819)\n",
      "[0/200][676/782] Loss_D: 1.0364 (0.9850) Loss_G: 0.9114 (0.9762) D(x): 0.4974 D(G(z)): 0.4135 / 0.3496 Acc: 18.7500 (18.4823)\n",
      "[0/200][677/782] Loss_D: 0.9568 (0.9850) Loss_G: 0.9817 (0.9762) D(x): 0.5923 D(G(z)): 0.4879 / 0.3349 Acc: 14.0625 (18.4758)\n",
      "[0/200][678/782] Loss_D: 1.5612 (0.9858) Loss_G: 0.7212 (0.9758) D(x): 0.3341 D(G(z)): 0.4948 / 0.4312 Acc: 29.6875 (18.4923)\n",
      "[0/200][679/782] Loss_D: 1.2422 (0.9862) Loss_G: 0.7581 (0.9755) D(x): 0.4524 D(G(z)): 0.5272 / 0.4313 Acc: 28.1250 (18.5064)\n",
      "[0/200][680/782] Loss_D: 1.3284 (0.9867) Loss_G: 0.7094 (0.9751) D(x): 0.4806 D(G(z)): 0.5377 / 0.4559 Acc: 18.7500 (18.5068)\n",
      "[0/200][681/782] Loss_D: 1.1138 (0.9869) Loss_G: 0.9548 (0.9751) D(x): 0.5406 D(G(z)): 0.5425 / 0.3494 Acc: 18.7500 (18.5071)\n",
      "[0/200][682/782] Loss_D: 0.6855 (0.9864) Loss_G: 0.9477 (0.9751) D(x): 0.5875 D(G(z)): 0.3738 / 0.3560 Acc: 20.3125 (18.5098)\n",
      "[0/200][683/782] Loss_D: 0.7609 (0.9861) Loss_G: 0.8935 (0.9749) D(x): 0.6069 D(G(z)): 0.4852 / 0.3584 Acc: 28.1250 (18.5238)\n",
      "[0/200][684/782] Loss_D: 0.5841 (0.9855) Loss_G: 0.9992 (0.9750) D(x): 0.5589 D(G(z)): 0.3371 / 0.3267 Acc: 26.5625 (18.5356)\n",
      "[0/200][685/782] Loss_D: 0.7662 (0.9852) Loss_G: 0.9878 (0.9750) D(x): 0.5690 D(G(z)): 0.3822 / 0.3333 Acc: 21.8750 (18.5405)\n",
      "[0/200][686/782] Loss_D: 0.7708 (0.9849) Loss_G: 0.9800 (0.9750) D(x): 0.6188 D(G(z)): 0.4015 / 0.3648 Acc: 18.7500 (18.5408)\n",
      "[0/200][687/782] Loss_D: 0.9001 (0.9848) Loss_G: 0.9445 (0.9750) D(x): 0.5903 D(G(z)): 0.5087 / 0.3591 Acc: 28.1250 (18.5547)\n",
      "[0/200][688/782] Loss_D: 0.6536 (0.9843) Loss_G: 1.1369 (0.9752) D(x): 0.6224 D(G(z)): 0.3932 / 0.3023 Acc: 18.7500 (18.5550)\n",
      "[0/200][689/782] Loss_D: 0.8300 (0.9841) Loss_G: 0.9418 (0.9751) D(x): 0.4997 D(G(z)): 0.3776 / 0.3690 Acc: 28.1250 (18.5688)\n",
      "[0/200][690/782] Loss_D: 0.9448 (0.9840) Loss_G: 0.9892 (0.9752) D(x): 0.5729 D(G(z)): 0.4454 / 0.3760 Acc: 26.5625 (18.5804)\n",
      "[0/200][691/782] Loss_D: 0.7337 (0.9836) Loss_G: 0.8991 (0.9751) D(x): 0.6056 D(G(z)): 0.4709 / 0.3431 Acc: 26.5625 (18.5919)\n",
      "[0/200][692/782] Loss_D: 0.9712 (0.9836) Loss_G: 1.4077 (0.9757) D(x): 0.5982 D(G(z)): 0.4991 / 0.2430 Acc: 23.4375 (18.5989)\n",
      "[0/200][693/782] Loss_D: 1.2501 (0.9840) Loss_G: 1.0160 (0.9757) D(x): 0.4346 D(G(z)): 0.4145 / 0.3410 Acc: 15.6250 (18.5947)\n",
      "[0/200][694/782] Loss_D: 0.8478 (0.9838) Loss_G: 0.9023 (0.9756) D(x): 0.6163 D(G(z)): 0.4929 / 0.3755 Acc: 23.4375 (18.6016)\n",
      "[0/200][695/782] Loss_D: 0.8094 (0.9836) Loss_G: 0.8588 (0.9755) D(x): 0.5292 D(G(z)): 0.4188 / 0.3923 Acc: 35.9375 (18.6265)\n",
      "[0/200][696/782] Loss_D: 1.0137 (0.9836) Loss_G: 0.7974 (0.9752) D(x): 0.5434 D(G(z)): 0.5012 / 0.3848 Acc: 21.8750 (18.6312)\n",
      "[0/200][697/782] Loss_D: 0.9993 (0.9836) Loss_G: 0.8998 (0.9751) D(x): 0.5179 D(G(z)): 0.4459 / 0.3713 Acc: 17.1875 (18.6291)\n",
      "[0/200][698/782] Loss_D: 1.0493 (0.9837) Loss_G: 0.8154 (0.9749) D(x): 0.4956 D(G(z)): 0.4517 / 0.4028 Acc: 20.3125 (18.6315)\n",
      "[0/200][699/782] Loss_D: 0.7595 (0.9834) Loss_G: 0.5685 (0.9743) D(x): 0.5110 D(G(z)): 0.3552 / 0.4580 Acc: 17.1875 (18.6295)\n",
      "[0/200][700/782] Loss_D: 0.9380 (0.9833) Loss_G: 0.8741 (0.9742) D(x): 0.6222 D(G(z)): 0.5175 / 0.3614 Acc: 17.1875 (18.6274)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[0/200][701/782] Loss_D: 0.9202 (0.9832) Loss_G: 1.1009 (0.9743) D(x): 0.5199 D(G(z)): 0.3879 / 0.3169 Acc: 20.3125 (18.6298)\n",
      "[0/200][702/782] Loss_D: 0.9931 (0.9833) Loss_G: 0.9866 (0.9744) D(x): 0.4956 D(G(z)): 0.4093 / 0.3515 Acc: 20.3125 (18.6322)\n",
      "[0/200][703/782] Loss_D: 0.7763 (0.9830) Loss_G: 0.7647 (0.9741) D(x): 0.5381 D(G(z)): 0.3998 / 0.3958 Acc: 18.7500 (18.6324)\n",
      "[0/200][704/782] Loss_D: 0.8627 (0.9828) Loss_G: 0.8588 (0.9739) D(x): 0.5891 D(G(z)): 0.4496 / 0.3593 Acc: 15.6250 (18.6281)\n",
      "[0/200][705/782] Loss_D: 0.7896 (0.9825) Loss_G: 1.0199 (0.9740) D(x): 0.5715 D(G(z)): 0.4667 / 0.3251 Acc: 26.5625 (18.6393)\n",
      "[0/200][706/782] Loss_D: 0.7496 (0.9822) Loss_G: 0.9219 (0.9739) D(x): 0.5297 D(G(z)): 0.3659 / 0.3271 Acc: 21.8750 (18.6439)\n",
      "[0/200][707/782] Loss_D: 0.8228 (0.9820) Loss_G: 0.9300 (0.9738) D(x): 0.6024 D(G(z)): 0.4138 / 0.3840 Acc: 18.7500 (18.6441)\n",
      "[0/200][708/782] Loss_D: 0.9264 (0.9819) Loss_G: 0.7958 (0.9736) D(x): 0.5775 D(G(z)): 0.4781 / 0.4010 Acc: 18.7500 (18.6442)\n",
      "[0/200][709/782] Loss_D: 0.8965 (0.9818) Loss_G: 0.9781 (0.9736) D(x): 0.5712 D(G(z)): 0.4540 / 0.3503 Acc: 20.3125 (18.6466)\n",
      "[0/200][710/782] Loss_D: 0.7316 (0.9814) Loss_G: 0.7380 (0.9732) D(x): 0.4986 D(G(z)): 0.3737 / 0.4013 Acc: 26.5625 (18.6577)\n",
      "[0/200][711/782] Loss_D: 1.0434 (0.9815) Loss_G: 0.6636 (0.9728) D(x): 0.5177 D(G(z)): 0.5195 / 0.4493 Acc: 31.2500 (18.6754)\n",
      "[0/200][712/782] Loss_D: 0.6913 (0.9811) Loss_G: 0.6973 (0.9724) D(x): 0.6034 D(G(z)): 0.4086 / 0.4405 Acc: 26.5625 (18.6864)\n",
      "[0/200][713/782] Loss_D: 0.8301 (0.9809) Loss_G: 0.7520 (0.9721) D(x): 0.5980 D(G(z)): 0.4843 / 0.4320 Acc: 25.0000 (18.6953)\n",
      "[0/200][714/782] Loss_D: 0.8317 (0.9807) Loss_G: 1.0475 (0.9722) D(x): 0.5908 D(G(z)): 0.4403 / 0.3404 Acc: 21.8750 (18.6997)\n",
      "[0/200][715/782] Loss_D: 1.1166 (0.9809) Loss_G: 1.0189 (0.9723) D(x): 0.4582 D(G(z)): 0.3876 / 0.3592 Acc: 15.6250 (18.6954)\n",
      "[0/200][716/782] Loss_D: 1.1697 (0.9811) Loss_G: 0.6969 (0.9719) D(x): 0.4767 D(G(z)): 0.4525 / 0.5045 Acc: 25.0000 (18.7042)\n",
      "[0/200][717/782] Loss_D: 1.0188 (0.9812) Loss_G: 0.7901 (0.9716) D(x): 0.6327 D(G(z)): 0.5932 / 0.4456 Acc: 34.3750 (18.7261)\n",
      "[0/200][718/782] Loss_D: 1.0756 (0.9813) Loss_G: 0.8945 (0.9715) D(x): 0.4529 D(G(z)): 0.4647 / 0.3591 Acc: 26.5625 (18.7370)\n",
      "[0/200][719/782] Loss_D: 1.0998 (0.9815) Loss_G: 0.5371 (0.9709) D(x): 0.4587 D(G(z)): 0.4388 / 0.4790 Acc: 25.0000 (18.7457)\n",
      "[0/200][720/782] Loss_D: 0.9931 (0.9815) Loss_G: 0.7736 (0.9707) D(x): 0.6397 D(G(z)): 0.5512 / 0.3976 Acc: 9.3750 (18.7327)\n",
      "[0/200][721/782] Loss_D: 0.8929 (0.9814) Loss_G: 0.9705 (0.9707) D(x): 0.5330 D(G(z)): 0.4051 / 0.3569 Acc: 18.7500 (18.7327)\n",
      "[0/200][722/782] Loss_D: 0.8154 (0.9811) Loss_G: 1.0268 (0.9707) D(x): 0.5644 D(G(z)): 0.3886 / 0.3433 Acc: 18.7500 (18.7327)\n",
      "[0/200][723/782] Loss_D: 1.0299 (0.9812) Loss_G: 0.7718 (0.9705) D(x): 0.5055 D(G(z)): 0.4576 / 0.3992 Acc: 17.1875 (18.7306)\n",
      "[0/200][724/782] Loss_D: 0.6240 (0.9807) Loss_G: 0.7899 (0.9702) D(x): 0.5875 D(G(z)): 0.3884 / 0.4115 Acc: 29.6875 (18.7457)\n",
      "[0/200][725/782] Loss_D: 0.6687 (0.9803) Loss_G: 0.9311 (0.9702) D(x): 0.6143 D(G(z)): 0.4143 / 0.3530 Acc: 28.1250 (18.7586)\n",
      "[0/200][726/782] Loss_D: 0.7712 (0.9800) Loss_G: 1.0664 (0.9703) D(x): 0.5589 D(G(z)): 0.4377 / 0.3178 Acc: 26.5625 (18.7693)\n",
      "[0/200][727/782] Loss_D: 0.8038 (0.9798) Loss_G: 0.8760 (0.9702) D(x): 0.4979 D(G(z)): 0.3366 / 0.3808 Acc: 23.4375 (18.7758)\n",
      "[0/200][728/782] Loss_D: 0.9281 (0.9797) Loss_G: 0.6630 (0.9697) D(x): 0.5691 D(G(z)): 0.4838 / 0.4760 Acc: 23.4375 (18.7822)\n",
      "[0/200][729/782] Loss_D: 0.6655 (0.9793) Loss_G: 0.8073 (0.9695) D(x): 0.6271 D(G(z)): 0.4683 / 0.3948 Acc: 32.8125 (18.8014)\n",
      "[0/200][730/782] Loss_D: 0.8948 (0.9791) Loss_G: 0.9319 (0.9695) D(x): 0.5645 D(G(z)): 0.4496 / 0.3577 Acc: 18.7500 (18.8013)\n",
      "[0/200][731/782] Loss_D: 0.9656 (0.9791) Loss_G: 0.7195 (0.9691) D(x): 0.4666 D(G(z)): 0.3988 / 0.4091 Acc: 18.7500 (18.8012)\n",
      "[0/200][732/782] Loss_D: 1.1191 (0.9793) Loss_G: 0.3610 (0.9683) D(x): 0.4326 D(G(z)): 0.4475 / 0.5715 Acc: 23.4375 (18.8076)\n",
      "[0/200][733/782] Loss_D: 1.0316 (0.9794) Loss_G: 0.6373 (0.9678) D(x): 0.6570 D(G(z)): 0.6102 / 0.4486 Acc: 20.3125 (18.8096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][734/782] Loss_D: 0.8575 (0.9792) Loss_G: 1.0596 (0.9680) D(x): 0.5738 D(G(z)): 0.4563 / 0.3010 Acc: 20.3125 (18.8116)\n",
      "[0/200][735/782] Loss_D: 0.9222 (0.9791) Loss_G: 0.8637 (0.9678) D(x): 0.4318 D(G(z)): 0.3061 / 0.3610 Acc: 20.3125 (18.8137)\n",
      "[0/200][736/782] Loss_D: 0.6538 (0.9787) Loss_G: 0.6170 (0.9674) D(x): 0.6325 D(G(z)): 0.4429 / 0.4393 Acc: 21.8750 (18.8178)\n",
      "[0/200][737/782] Loss_D: 0.6295 (0.9782) Loss_G: 0.8030 (0.9671) D(x): 0.5964 D(G(z)): 0.4099 / 0.3744 Acc: 17.1875 (18.8156)\n",
      "[0/200][738/782] Loss_D: 0.7126 (0.9779) Loss_G: 1.2450 (0.9675) D(x): 0.6639 D(G(z)): 0.4583 / 0.2636 Acc: 20.3125 (18.8177)\n",
      "[0/200][739/782] Loss_D: 0.7434 (0.9775) Loss_G: 1.0331 (0.9676) D(x): 0.5322 D(G(z)): 0.3338 / 0.3512 Acc: 25.0000 (18.8260)\n",
      "[0/200][740/782] Loss_D: 0.8087 (0.9773) Loss_G: 0.8114 (0.9674) D(x): 0.5670 D(G(z)): 0.4462 / 0.3811 Acc: 23.4375 (18.8322)\n",
      "[0/200][741/782] Loss_D: 0.5197 (0.9767) Loss_G: 0.8280 (0.9672) D(x): 0.6397 D(G(z)): 0.3717 / 0.3779 Acc: 20.3125 (18.8342)\n",
      "[0/200][742/782] Loss_D: 0.8649 (0.9766) Loss_G: 0.9520 (0.9672) D(x): 0.5868 D(G(z)): 0.5158 / 0.3372 Acc: 23.4375 (18.8404)\n",
      "[0/200][743/782] Loss_D: 0.6975 (0.9762) Loss_G: 1.0861 (0.9673) D(x): 0.5510 D(G(z)): 0.3696 / 0.3003 Acc: 23.4375 (18.8466)\n",
      "[0/200][744/782] Loss_D: 0.7889 (0.9759) Loss_G: 0.7410 (0.9670) D(x): 0.5106 D(G(z)): 0.3863 / 0.4135 Acc: 26.5625 (18.8570)\n",
      "[0/200][745/782] Loss_D: 0.5006 (0.9753) Loss_G: 1.0859 (0.9672) D(x): 0.7002 D(G(z)): 0.4794 / 0.3069 Acc: 34.3750 (18.8778)\n",
      "[0/200][746/782] Loss_D: 0.6461 (0.9748) Loss_G: 1.1363 (0.9674) D(x): 0.5600 D(G(z)): 0.3895 / 0.2964 Acc: 28.1250 (18.8901)\n",
      "[0/200][747/782] Loss_D: 0.7745 (0.9746) Loss_G: 0.7838 (0.9672) D(x): 0.5109 D(G(z)): 0.3841 / 0.3850 Acc: 20.3125 (18.8920)\n",
      "[0/200][748/782] Loss_D: 0.6848 (0.9742) Loss_G: 0.7466 (0.9669) D(x): 0.5810 D(G(z)): 0.4365 / 0.3939 Acc: 29.6875 (18.9065)\n",
      "[0/200][749/782] Loss_D: 0.4760 (0.9735) Loss_G: 1.1506 (0.9671) D(x): 0.6609 D(G(z)): 0.3697 / 0.2983 Acc: 26.5625 (18.9167)\n",
      "[0/200][750/782] Loss_D: 0.7638 (0.9732) Loss_G: 0.8509 (0.9670) D(x): 0.5515 D(G(z)): 0.3812 / 0.4077 Acc: 28.1250 (18.9289)\n",
      "[0/200][751/782] Loss_D: 0.5551 (0.9727) Loss_G: 1.2949 (0.9674) D(x): 0.7175 D(G(z)): 0.4420 / 0.2629 Acc: 23.4375 (18.9349)\n",
      "[0/200][752/782] Loss_D: 0.5180 (0.9721) Loss_G: 1.2934 (0.9678) D(x): 0.5965 D(G(z)): 0.3860 / 0.2370 Acc: 28.1250 (18.9471)\n",
      "[0/200][753/782] Loss_D: 0.7553 (0.9718) Loss_G: 0.6204 (0.9674) D(x): 0.4994 D(G(z)): 0.3287 / 0.4526 Acc: 17.1875 (18.9448)\n",
      "[0/200][754/782] Loss_D: 0.6491 (0.9714) Loss_G: 0.7561 (0.9671) D(x): 0.6622 D(G(z)): 0.4653 / 0.4020 Acc: 26.5625 (18.9549)\n",
      "[0/200][755/782] Loss_D: 0.7839 (0.9711) Loss_G: 0.9211 (0.9670) D(x): 0.5983 D(G(z)): 0.4326 / 0.3420 Acc: 17.1875 (18.9525)\n",
      "[0/200][756/782] Loss_D: 0.7603 (0.9708) Loss_G: 0.9169 (0.9670) D(x): 0.5386 D(G(z)): 0.3832 / 0.3703 Acc: 26.5625 (18.9626)\n",
      "[0/200][757/782] Loss_D: 0.8802 (0.9707) Loss_G: 0.7165 (0.9666) D(x): 0.5541 D(G(z)): 0.4586 / 0.4434 Acc: 21.8750 (18.9664)\n",
      "[0/200][758/782] Loss_D: 0.7237 (0.9704) Loss_G: 1.0314 (0.9667) D(x): 0.6773 D(G(z)): 0.4780 / 0.3231 Acc: 20.3125 (18.9682)\n",
      "[0/200][759/782] Loss_D: 0.9293 (0.9703) Loss_G: 1.0924 (0.9669) D(x): 0.5235 D(G(z)): 0.4067 / 0.3341 Acc: 20.3125 (18.9700)\n",
      "[0/200][760/782] Loss_D: 1.1221 (0.9705) Loss_G: 0.8303 (0.9667) D(x): 0.4986 D(G(z)): 0.4788 / 0.3754 Acc: 14.0625 (18.9635)\n",
      "[0/200][761/782] Loss_D: 1.2538 (0.9709) Loss_G: 0.9760 (0.9667) D(x): 0.5344 D(G(z)): 0.5664 / 0.3562 Acc: 21.8750 (18.9674)\n",
      "[0/200][762/782] Loss_D: 0.9618 (0.9709) Loss_G: 0.9457 (0.9667) D(x): 0.5169 D(G(z)): 0.4553 / 0.3328 Acc: 21.8750 (18.9712)\n",
      "[0/200][763/782] Loss_D: 1.1197 (0.9711) Loss_G: 0.8524 (0.9665) D(x): 0.4587 D(G(z)): 0.4440 / 0.3871 Acc: 15.6250 (18.9668)\n",
      "[0/200][764/782] Loss_D: 1.0206 (0.9712) Loss_G: 0.9413 (0.9665) D(x): 0.5516 D(G(z)): 0.5202 / 0.3360 Acc: 23.4375 (18.9726)\n",
      "[0/200][765/782] Loss_D: 1.2149 (0.9715) Loss_G: 1.0893 (0.9667) D(x): 0.4782 D(G(z)): 0.5040 / 0.3115 Acc: 23.4375 (18.9785)\n",
      "[0/200][766/782] Loss_D: 0.8551 (0.9713) Loss_G: 0.6779 (0.9663) D(x): 0.4746 D(G(z)): 0.4336 / 0.4045 Acc: 28.1250 (18.9904)\n",
      "[0/200][767/782] Loss_D: 0.7096 (0.9710) Loss_G: 0.8443 (0.9661) D(x): 0.5691 D(G(z)): 0.4387 / 0.3463 Acc: 21.8750 (18.9941)\n",
      "[0/200][768/782] Loss_D: 0.9322 (0.9709) Loss_G: 0.9754 (0.9662) D(x): 0.5415 D(G(z)): 0.4523 / 0.3425 Acc: 23.4375 (18.9999)\n",
      "[0/200][769/782] Loss_D: 1.1246 (0.9711) Loss_G: 0.7753 (0.9659) D(x): 0.4590 D(G(z)): 0.4552 / 0.4078 Acc: 21.8750 (19.0037)\n",
      "[0/200][770/782] Loss_D: 1.2085 (0.9714) Loss_G: 0.8375 (0.9657) D(x): 0.5414 D(G(z)): 0.5593 / 0.3933 Acc: 20.3125 (19.0054)\n",
      "[0/200][771/782] Loss_D: 0.9401 (0.9714) Loss_G: 0.8828 (0.9656) D(x): 0.5410 D(G(z)): 0.4577 / 0.3503 Acc: 14.0625 (18.9989)\n",
      "[0/200][772/782] Loss_D: 0.7721 (0.9712) Loss_G: 0.7784 (0.9654) D(x): 0.5363 D(G(z)): 0.4479 / 0.3819 Acc: 28.1250 (19.0108)\n",
      "[0/200][773/782] Loss_D: 1.0661 (0.9713) Loss_G: 0.7505 (0.9651) D(x): 0.5240 D(G(z)): 0.5141 / 0.3983 Acc: 23.4375 (19.0165)\n",
      "[0/200][774/782] Loss_D: 0.8692 (0.9711) Loss_G: 0.9575 (0.9651) D(x): 0.5551 D(G(z)): 0.4380 / 0.3618 Acc: 21.8750 (19.0202)\n",
      "[0/200][775/782] Loss_D: 1.0048 (0.9712) Loss_G: 1.0723 (0.9652) D(x): 0.6055 D(G(z)): 0.5512 / 0.3242 Acc: 21.8750 (19.0238)\n",
      "[0/200][776/782] Loss_D: 0.6152 (0.9707) Loss_G: 1.1226 (0.9654) D(x): 0.5544 D(G(z)): 0.3785 / 0.2896 Acc: 34.3750 (19.0436)\n",
      "[0/200][777/782] Loss_D: 0.7961 (0.9705) Loss_G: 0.9310 (0.9654) D(x): 0.5606 D(G(z)): 0.4142 / 0.3631 Acc: 21.8750 (19.0472)\n",
      "[0/200][778/782] Loss_D: 0.6839 (0.9701) Loss_G: 0.8465 (0.9652) D(x): 0.6057 D(G(z)): 0.3931 / 0.3868 Acc: 18.7500 (19.0469)\n",
      "[0/200][779/782] Loss_D: 0.7142 (0.9698) Loss_G: 0.7801 (0.9650) D(x): 0.6180 D(G(z)): 0.4834 / 0.3813 Acc: 23.4375 (19.0525)\n",
      "[0/200][780/782] Loss_D: 0.8056 (0.9696) Loss_G: 1.0025 (0.9651) D(x): 0.5755 D(G(z)): 0.4284 / 0.3258 Acc: 18.7500 (19.0521)\n",
      "[0/200][781/782] Loss_D: 1.1855 (0.9699) Loss_G: 0.9116 (0.9650) D(x): 0.4196 D(G(z)): 0.3832 / 0.3877 Acc: 18.7500 (19.0517)\n",
      "[1/200][0/782] Loss_D: 0.9754 (0.9699) Loss_G: 0.6950 (0.9646) D(x): 0.6187 D(G(z)): 0.5518 / 0.4287 Acc: 25.0000 (19.0593)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[1/200][1/782] Loss_D: 0.7408 (0.9696) Loss_G: 1.0574 (0.9648) D(x): 0.6220 D(G(z)): 0.5104 / 0.3117 Acc: 29.6875 (19.0729)\n",
      "[1/200][2/782] Loss_D: 0.8720 (0.9695) Loss_G: 0.9404 (0.9647) D(x): 0.5350 D(G(z)): 0.4546 / 0.3468 Acc: 21.8750 (19.0764)\n",
      "[1/200][3/782] Loss_D: 0.8089 (0.9693) Loss_G: 0.9051 (0.9647) D(x): 0.5522 D(G(z)): 0.4369 / 0.3406 Acc: 23.4375 (19.0820)\n",
      "[1/200][4/782] Loss_D: 0.9483 (0.9692) Loss_G: 0.6279 (0.9642) D(x): 0.4601 D(G(z)): 0.4181 / 0.4495 Acc: 21.8750 (19.0855)\n",
      "[1/200][5/782] Loss_D: 1.3001 (0.9697) Loss_G: 0.4853 (0.9636) D(x): 0.4933 D(G(z)): 0.5835 / 0.5197 Acc: 14.0625 (19.0792)\n",
      "[1/200][6/782] Loss_D: 0.9494 (0.9696) Loss_G: 0.5836 (0.9631) D(x): 0.5272 D(G(z)): 0.5473 / 0.4388 Acc: 29.6875 (19.0926)\n",
      "[1/200][7/782] Loss_D: 1.1226 (0.9698) Loss_G: 0.9088 (0.9631) D(x): 0.5143 D(G(z)): 0.5227 / 0.3466 Acc: 18.7500 (19.0922)\n",
      "[1/200][8/782] Loss_D: 0.7411 (0.9695) Loss_G: 0.7794 (0.9628) D(x): 0.4862 D(G(z)): 0.4456 / 0.3503 Acc: 31.2500 (19.1075)\n",
      "[1/200][9/782] Loss_D: 0.8379 (0.9694) Loss_G: 0.5375 (0.9623) D(x): 0.4607 D(G(z)): 0.4078 / 0.4269 Acc: 18.7500 (19.1071)\n",
      "[1/200][10/782] Loss_D: 1.0036 (0.9694) Loss_G: 0.7357 (0.9620) D(x): 0.5484 D(G(z)): 0.4895 / 0.4331 Acc: 18.7500 (19.1066)\n",
      "[1/200][11/782] Loss_D: 0.8113 (0.9692) Loss_G: 0.7878 (0.9618) D(x): 0.5791 D(G(z)): 0.4885 / 0.3850 Acc: 28.1250 (19.1180)\n",
      "[1/200][12/782] Loss_D: 0.9650 (0.9692) Loss_G: 0.9052 (0.9617) D(x): 0.4547 D(G(z)): 0.3691 / 0.3675 Acc: 21.8750 (19.1215)\n",
      "[1/200][13/782] Loss_D: 0.7860 (0.9690) Loss_G: 0.9015 (0.9616) D(x): 0.6146 D(G(z)): 0.4444 / 0.3666 Acc: 15.6250 (19.1171)\n",
      "[1/200][14/782] Loss_D: 0.7782 (0.9687) Loss_G: 0.9525 (0.9616) D(x): 0.5588 D(G(z)): 0.4684 / 0.3031 Acc: 17.1875 (19.1146)\n",
      "[1/200][15/782] Loss_D: 0.5501 (0.9682) Loss_G: 1.0705 (0.9618) D(x): 0.5887 D(G(z)): 0.3646 / 0.3060 Acc: 26.5625 (19.1240)\n",
      "[1/200][16/782] Loss_D: 0.6677 (0.9678) Loss_G: 0.5917 (0.9613) D(x): 0.5563 D(G(z)): 0.3985 / 0.4344 Acc: 21.8750 (19.1274)\n",
      "[1/200][17/782] Loss_D: 0.7243 (0.9675) Loss_G: 0.6476 (0.9609) D(x): 0.6056 D(G(z)): 0.4619 / 0.4262 Acc: 25.0000 (19.1348)\n",
      "[1/200][18/782] Loss_D: 0.7666 (0.9673) Loss_G: 0.7530 (0.9607) D(x): 0.5916 D(G(z)): 0.5022 / 0.3682 Acc: 21.8750 (19.1382)\n",
      "[1/200][19/782] Loss_D: 0.8104 (0.9671) Loss_G: 1.0164 (0.9607) D(x): 0.5526 D(G(z)): 0.4132 / 0.3365 Acc: 25.0000 (19.1455)\n",
      "[1/200][20/782] Loss_D: 0.9674 (0.9671) Loss_G: 0.4988 (0.9601) D(x): 0.4555 D(G(z)): 0.3973 / 0.4803 Acc: 20.3125 (19.1469)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][21/782] Loss_D: 0.9380 (0.9670) Loss_G: 0.6686 (0.9598) D(x): 0.6250 D(G(z)): 0.5469 / 0.4270 Acc: 20.3125 (19.1484)\n",
      "[1/200][22/782] Loss_D: 1.0540 (0.9672) Loss_G: 0.7203 (0.9595) D(x): 0.4488 D(G(z)): 0.4235 / 0.4235 Acc: 20.3125 (19.1498)\n",
      "[1/200][23/782] Loss_D: 0.8453 (0.9670) Loss_G: 0.5723 (0.9590) D(x): 0.5467 D(G(z)): 0.4790 / 0.4806 Acc: 25.0000 (19.1571)\n",
      "[1/200][24/782] Loss_D: 0.9374 (0.9670) Loss_G: 0.4873 (0.9584) D(x): 0.5795 D(G(z)): 0.5772 / 0.4836 Acc: 28.1250 (19.1682)\n",
      "[1/200][25/782] Loss_D: 0.7582 (0.9667) Loss_G: 0.8685 (0.9583) D(x): 0.5168 D(G(z)): 0.4390 / 0.3457 Acc: 26.5625 (19.1774)\n",
      "[1/200][26/782] Loss_D: 0.8943 (0.9666) Loss_G: 0.7232 (0.9580) D(x): 0.4892 D(G(z)): 0.4594 / 0.3825 Acc: 26.5625 (19.1865)\n",
      "[1/200][27/782] Loss_D: 1.0436 (0.9667) Loss_G: 0.7312 (0.9577) D(x): 0.4740 D(G(z)): 0.4762 / 0.4126 Acc: 23.4375 (19.1917)\n",
      "[1/200][28/782] Loss_D: 0.8203 (0.9665) Loss_G: 0.7086 (0.9574) D(x): 0.5815 D(G(z)): 0.5113 / 0.4194 Acc: 29.6875 (19.2047)\n",
      "[1/200][29/782] Loss_D: 0.9021 (0.9665) Loss_G: 0.7393 (0.9572) D(x): 0.5322 D(G(z)): 0.4529 / 0.3857 Acc: 12.5000 (19.1964)\n",
      "[1/200][30/782] Loss_D: 0.8735 (0.9663) Loss_G: 0.6714 (0.9568) D(x): 0.4797 D(G(z)): 0.4151 / 0.4152 Acc: 25.0000 (19.2036)\n",
      "[1/200][31/782] Loss_D: 0.7214 (0.9660) Loss_G: 0.7846 (0.9566) D(x): 0.6172 D(G(z)): 0.4861 / 0.3993 Acc: 29.6875 (19.2164)\n",
      "[1/200][32/782] Loss_D: 0.7827 (0.9658) Loss_G: 0.8932 (0.9565) D(x): 0.6142 D(G(z)): 0.5288 / 0.3393 Acc: 34.3750 (19.2350)\n",
      "[1/200][33/782] Loss_D: 1.0566 (0.9659) Loss_G: 1.0597 (0.9567) D(x): 0.4802 D(G(z)): 0.4499 / 0.3165 Acc: 21.8750 (19.2383)\n",
      "[1/200][34/782] Loss_D: 0.9167 (0.9659) Loss_G: 0.8860 (0.9566) D(x): 0.4874 D(G(z)): 0.4402 / 0.3670 Acc: 31.2500 (19.2530)\n",
      "[1/200][35/782] Loss_D: 0.7942 (0.9657) Loss_G: 0.7098 (0.9563) D(x): 0.5692 D(G(z)): 0.4047 / 0.4394 Acc: 20.3125 (19.2543)\n",
      "[1/200][36/782] Loss_D: 1.2277 (0.9660) Loss_G: 0.6955 (0.9559) D(x): 0.4884 D(G(z)): 0.5374 / 0.4376 Acc: 21.8750 (19.2575)\n",
      "[1/200][37/782] Loss_D: 0.9238 (0.9659) Loss_G: 0.8761 (0.9558) D(x): 0.5859 D(G(z)): 0.4821 / 0.3987 Acc: 25.0000 (19.2645)\n",
      "[1/200][38/782] Loss_D: 0.8505 (0.9658) Loss_G: 0.7247 (0.9556) D(x): 0.5568 D(G(z)): 0.4413 / 0.4021 Acc: 17.1875 (19.2620)\n",
      "[1/200][39/782] Loss_D: 0.7625 (0.9655) Loss_G: 0.9575 (0.9556) D(x): 0.6036 D(G(z)): 0.4449 / 0.3313 Acc: 17.1875 (19.2594)\n",
      "[1/200][40/782] Loss_D: 0.9083 (0.9655) Loss_G: 1.2093 (0.9559) D(x): 0.6226 D(G(z)): 0.5108 / 0.2836 Acc: 26.5625 (19.2683)\n",
      "[1/200][41/782] Loss_D: 1.0403 (0.9656) Loss_G: 0.8561 (0.9558) D(x): 0.4107 D(G(z)): 0.3843 / 0.3798 Acc: 20.3125 (19.2696)\n",
      "[1/200][42/782] Loss_D: 0.9525 (0.9655) Loss_G: 0.5041 (0.9552) D(x): 0.4664 D(G(z)): 0.4888 / 0.4629 Acc: 26.5625 (19.2784)\n",
      "[1/200][43/782] Loss_D: 0.9767 (0.9656) Loss_G: 0.7079 (0.9549) D(x): 0.6071 D(G(z)): 0.5198 / 0.4359 Acc: 15.6250 (19.2740)\n",
      "[1/200][44/782] Loss_D: 1.0586 (0.9657) Loss_G: 0.9025 (0.9548) D(x): 0.5040 D(G(z)): 0.4432 / 0.3730 Acc: 15.6250 (19.2696)\n",
      "[1/200][45/782] Loss_D: 0.6869 (0.9653) Loss_G: 0.9310 (0.9548) D(x): 0.5653 D(G(z)): 0.4149 / 0.3590 Acc: 26.5625 (19.2784)\n",
      "[1/200][46/782] Loss_D: 0.8535 (0.9652) Loss_G: 0.7157 (0.9545) D(x): 0.5879 D(G(z)): 0.4826 / 0.3950 Acc: 20.3125 (19.2796)\n",
      "[1/200][47/782] Loss_D: 0.9007 (0.9651) Loss_G: 0.7273 (0.9543) D(x): 0.5265 D(G(z)): 0.3849 / 0.4173 Acc: 18.7500 (19.2790)\n",
      "[1/200][48/782] Loss_D: 0.7758 (0.9649) Loss_G: 0.6141 (0.9538) D(x): 0.5830 D(G(z)): 0.4466 / 0.4693 Acc: 26.5625 (19.2878)\n",
      "[1/200][49/782] Loss_D: 0.8953 (0.9648) Loss_G: 0.6205 (0.9534) D(x): 0.6155 D(G(z)): 0.4869 / 0.4419 Acc: 10.9375 (19.2777)\n",
      "[1/200][50/782] Loss_D: 0.6826 (0.9645) Loss_G: 0.9329 (0.9534) D(x): 0.6013 D(G(z)): 0.4212 / 0.3481 Acc: 23.4375 (19.2827)\n",
      "[1/200][51/782] Loss_D: 0.8471 (0.9643) Loss_G: 0.8090 (0.9532) D(x): 0.5608 D(G(z)): 0.4362 / 0.3808 Acc: 18.7500 (19.2821)\n",
      "[1/200][52/782] Loss_D: 0.8045 (0.9641) Loss_G: 0.8312 (0.9531) D(x): 0.5711 D(G(z)): 0.4332 / 0.3878 Acc: 17.1875 (19.2796)\n",
      "[1/200][53/782] Loss_D: 0.8138 (0.9640) Loss_G: 0.7993 (0.9529) D(x): 0.5186 D(G(z)): 0.4297 / 0.3912 Acc: 26.5625 (19.2883)\n",
      "[1/200][54/782] Loss_D: 0.8573 (0.9638) Loss_G: 0.7198 (0.9526) D(x): 0.5453 D(G(z)): 0.4004 / 0.4023 Acc: 14.0625 (19.2820)\n",
      "[1/200][55/782] Loss_D: 0.8529 (0.9637) Loss_G: 0.6277 (0.9523) D(x): 0.5698 D(G(z)): 0.4806 / 0.4621 Acc: 26.5625 (19.2907)\n",
      "[1/200][56/782] Loss_D: 0.9812 (0.9637) Loss_G: 0.7437 (0.9520) D(x): 0.5505 D(G(z)): 0.5228 / 0.4037 Acc: 25.0000 (19.2975)\n",
      "[1/200][57/782] Loss_D: 0.9388 (0.9637) Loss_G: 0.8559 (0.9519) D(x): 0.5690 D(G(z)): 0.4924 / 0.3820 Acc: 18.7500 (19.2969)\n",
      "[1/200][58/782] Loss_D: 0.9024 (0.9636) Loss_G: 0.7013 (0.9516) D(x): 0.5479 D(G(z)): 0.5220 / 0.3903 Acc: 20.3125 (19.2981)\n",
      "[1/200][59/782] Loss_D: 1.0330 (0.9637) Loss_G: 0.6362 (0.9512) D(x): 0.4758 D(G(z)): 0.3950 / 0.4783 Acc: 14.0625 (19.2919)\n",
      "[1/200][60/782] Loss_D: 1.0894 (0.9638) Loss_G: 0.6657 (0.9509) D(x): 0.5707 D(G(z)): 0.5497 / 0.4280 Acc: 20.3125 (19.2931)\n",
      "[1/200][61/782] Loss_D: 0.9769 (0.9639) Loss_G: 0.7565 (0.9506) D(x): 0.4945 D(G(z)): 0.4896 / 0.3700 Acc: 25.0000 (19.2998)\n",
      "[1/200][62/782] Loss_D: 1.1372 (0.9641) Loss_G: 0.5538 (0.9502) D(x): 0.4023 D(G(z)): 0.4393 / 0.4710 Acc: 23.4375 (19.3047)\n",
      "[1/200][63/782] Loss_D: 0.9846 (0.9641) Loss_G: 0.5367 (0.9497) D(x): 0.5361 D(G(z)): 0.5211 / 0.4855 Acc: 28.1250 (19.3152)\n",
      "[1/200][64/782] Loss_D: 0.9244 (0.9640) Loss_G: 0.7215 (0.9494) D(x): 0.5576 D(G(z)): 0.4866 / 0.4316 Acc: 18.7500 (19.3145)\n",
      "[1/200][65/782] Loss_D: 1.0341 (0.9641) Loss_G: 0.7627 (0.9492) D(x): 0.5519 D(G(z)): 0.5040 / 0.3828 Acc: 15.6250 (19.3101)\n",
      "[1/200][66/782] Loss_D: 0.7583 (0.9639) Loss_G: 0.6898 (0.9489) D(x): 0.4788 D(G(z)): 0.3716 / 0.4032 Acc: 23.4375 (19.3150)\n",
      "[1/200][67/782] Loss_D: 0.8602 (0.9638) Loss_G: 0.6883 (0.9486) D(x): 0.5143 D(G(z)): 0.4140 / 0.4223 Acc: 17.1875 (19.3125)\n",
      "[1/200][68/782] Loss_D: 0.8763 (0.9637) Loss_G: 0.5064 (0.9481) D(x): 0.5521 D(G(z)): 0.4646 / 0.4894 Acc: 15.6250 (19.3082)\n",
      "[1/200][69/782] Loss_D: 0.8996 (0.9636) Loss_G: 0.7258 (0.9478) D(x): 0.5213 D(G(z)): 0.4922 / 0.3742 Acc: 20.3125 (19.3093)\n",
      "[1/200][70/782] Loss_D: 0.8086 (0.9634) Loss_G: 0.5079 (0.9473) D(x): 0.4652 D(G(z)): 0.4002 / 0.4506 Acc: 25.0000 (19.3160)\n",
      "[1/200][71/782] Loss_D: 0.9267 (0.9634) Loss_G: 0.5638 (0.9468) D(x): 0.5261 D(G(z)): 0.4674 / 0.4842 Acc: 25.0000 (19.3227)\n",
      "[1/200][72/782] Loss_D: 0.8808 (0.9633) Loss_G: 0.7002 (0.9466) D(x): 0.5669 D(G(z)): 0.4895 / 0.4303 Acc: 26.5625 (19.3311)\n",
      "[1/200][73/782] Loss_D: 1.0885 (0.9634) Loss_G: 0.7250 (0.9463) D(x): 0.4848 D(G(z)): 0.4962 / 0.4271 Acc: 25.0000 (19.3378)\n",
      "[1/200][74/782] Loss_D: 0.8798 (0.9633) Loss_G: 0.7375 (0.9460) D(x): 0.5442 D(G(z)): 0.4559 / 0.3817 Acc: 20.3125 (19.3389)\n",
      "[1/200][75/782] Loss_D: 0.8833 (0.9632) Loss_G: 0.7990 (0.9459) D(x): 0.4985 D(G(z)): 0.4831 / 0.3655 Acc: 25.0000 (19.3455)\n",
      "[1/200][76/782] Loss_D: 0.8754 (0.9631) Loss_G: 0.7303 (0.9456) D(x): 0.4895 D(G(z)): 0.5038 / 0.3633 Acc: 32.8125 (19.3612)\n",
      "[1/200][77/782] Loss_D: 1.0067 (0.9632) Loss_G: 0.8079 (0.9455) D(x): 0.5061 D(G(z)): 0.4666 / 0.3753 Acc: 15.6250 (19.3568)\n",
      "[1/200][78/782] Loss_D: 0.8727 (0.9631) Loss_G: 0.5195 (0.9450) D(x): 0.4192 D(G(z)): 0.4122 / 0.4463 Acc: 28.1250 (19.3670)\n",
      "[1/200][79/782] Loss_D: 0.4948 (0.9625) Loss_G: 0.5771 (0.9445) D(x): 0.6494 D(G(z)): 0.3962 / 0.4496 Acc: 23.4375 (19.3717)\n",
      "[1/200][80/782] Loss_D: 0.6489 (0.9622) Loss_G: 0.8545 (0.9444) D(x): 0.6419 D(G(z)): 0.4691 / 0.3691 Acc: 29.6875 (19.3837)\n",
      "[1/200][81/782] Loss_D: 0.7000 (0.9619) Loss_G: 0.8916 (0.9444) D(x): 0.5423 D(G(z)): 0.3773 / 0.3528 Acc: 21.8750 (19.3866)\n",
      "[1/200][82/782] Loss_D: 0.7058 (0.9616) Loss_G: 0.5709 (0.9439) D(x): 0.4740 D(G(z)): 0.3738 / 0.3974 Acc: 20.3125 (19.3876)\n",
      "[1/200][83/782] Loss_D: 0.6823 (0.9612) Loss_G: 0.6739 (0.9436) D(x): 0.6349 D(G(z)): 0.5243 / 0.3956 Acc: 23.4375 (19.3923)\n",
      "[1/200][84/782] Loss_D: 0.7076 (0.9609) Loss_G: 0.8809 (0.9436) D(x): 0.5976 D(G(z)): 0.4279 / 0.3676 Acc: 21.8750 (19.3952)\n",
      "[1/200][85/782] Loss_D: 0.7007 (0.9606) Loss_G: 0.8205 (0.9434) D(x): 0.4630 D(G(z)): 0.3522 / 0.3834 Acc: 34.3750 (19.4124)\n",
      "[1/200][86/782] Loss_D: 0.5562 (0.9602) Loss_G: 0.5930 (0.9430) D(x): 0.5830 D(G(z)): 0.4277 / 0.4111 Acc: 26.5625 (19.4207)\n",
      "[1/200][87/782] Loss_D: 0.6382 (0.9598) Loss_G: 0.6236 (0.9427) D(x): 0.5951 D(G(z)): 0.4185 / 0.4054 Acc: 17.1875 (19.4181)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][88/782] Loss_D: 0.6100 (0.9594) Loss_G: 0.9032 (0.9426) D(x): 0.6205 D(G(z)): 0.3906 / 0.3562 Acc: 21.8750 (19.4209)\n",
      "[1/200][89/782] Loss_D: 0.7604 (0.9592) Loss_G: 0.7353 (0.9424) D(x): 0.5209 D(G(z)): 0.4259 / 0.4105 Acc: 31.2500 (19.4345)\n",
      "[1/200][90/782] Loss_D: 0.7012 (0.9589) Loss_G: 0.6170 (0.9420) D(x): 0.5658 D(G(z)): 0.4333 / 0.4366 Acc: 26.5625 (19.4427)\n",
      "[1/200][91/782] Loss_D: 0.6186 (0.9585) Loss_G: 0.7280 (0.9418) D(x): 0.6342 D(G(z)): 0.4136 / 0.4098 Acc: 18.7500 (19.4419)\n",
      "[1/200][92/782] Loss_D: 1.0329 (0.9586) Loss_G: 0.5764 (0.9413) D(x): 0.4919 D(G(z)): 0.4841 / 0.4400 Acc: 20.3125 (19.4429)\n",
      "[1/200][93/782] Loss_D: 0.8274 (0.9584) Loss_G: 0.5015 (0.9408) D(x): 0.4938 D(G(z)): 0.4306 / 0.4707 Acc: 26.5625 (19.4510)\n",
      "[1/200][94/782] Loss_D: 0.8005 (0.9582) Loss_G: 0.9182 (0.9408) D(x): 0.6084 D(G(z)): 0.4972 / 0.3447 Acc: 21.8750 (19.4537)\n",
      "[1/200][95/782] Loss_D: 0.6358 (0.9579) Loss_G: 0.8663 (0.9407) D(x): 0.6054 D(G(z)): 0.4499 / 0.3350 Acc: 26.5625 (19.4618)\n",
      "[1/200][96/782] Loss_D: 0.7527 (0.9576) Loss_G: 0.7987 (0.9406) D(x): 0.5026 D(G(z)): 0.3649 / 0.3672 Acc: 25.0000 (19.4681)\n",
      "[1/200][97/782] Loss_D: 0.8073 (0.9575) Loss_G: 0.4700 (0.9400) D(x): 0.5372 D(G(z)): 0.4352 / 0.4980 Acc: 20.3125 (19.4691)\n",
      "[1/200][98/782] Loss_D: 0.7460 (0.9572) Loss_G: 0.8381 (0.9399) D(x): 0.6416 D(G(z)): 0.4938 / 0.3430 Acc: 14.0625 (19.4630)\n",
      "[1/200][99/782] Loss_D: 0.7385 (0.9570) Loss_G: 1.1547 (0.9402) D(x): 0.5801 D(G(z)): 0.4471 / 0.2948 Acc: 26.5625 (19.4710)\n",
      "[1/200][100/782] Loss_D: 0.7084 (0.9567) Loss_G: 0.9803 (0.9402) D(x): 0.5655 D(G(z)): 0.3417 / 0.3100 Acc: 9.3750 (19.4596)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[1/200][101/782] Loss_D: 0.6435 (0.9564) Loss_G: 0.7085 (0.9399) D(x): 0.5510 D(G(z)): 0.3648 / 0.4094 Acc: 28.1250 (19.4694)\n",
      "[1/200][102/782] Loss_D: 0.6308 (0.9560) Loss_G: 0.6314 (0.9396) D(x): 0.5626 D(G(z)): 0.4249 / 0.3929 Acc: 23.4375 (19.4739)\n",
      "[1/200][103/782] Loss_D: 0.6690 (0.9557) Loss_G: 0.8319 (0.9395) D(x): 0.6332 D(G(z)): 0.4488 / 0.3614 Acc: 20.3125 (19.4748)\n",
      "[1/200][104/782] Loss_D: 0.6020 (0.9553) Loss_G: 0.7595 (0.9393) D(x): 0.5699 D(G(z)): 0.4388 / 0.3567 Acc: 25.0000 (19.4810)\n",
      "[1/200][105/782] Loss_D: 0.7611 (0.9550) Loss_G: 0.7806 (0.9391) D(x): 0.5866 D(G(z)): 0.4663 / 0.3583 Acc: 20.3125 (19.4820)\n",
      "[1/200][106/782] Loss_D: 0.6033 (0.9546) Loss_G: 0.6570 (0.9388) D(x): 0.5514 D(G(z)): 0.4294 / 0.3873 Acc: 29.6875 (19.4935)\n",
      "[1/200][107/782] Loss_D: 0.5885 (0.9542) Loss_G: 0.6452 (0.9384) D(x): 0.5532 D(G(z)): 0.3785 / 0.3721 Acc: 18.7500 (19.4926)\n",
      "[1/200][108/782] Loss_D: 0.7090 (0.9540) Loss_G: 0.7814 (0.9383) D(x): 0.5523 D(G(z)): 0.4017 / 0.3610 Acc: 21.8750 (19.4953)\n",
      "[1/200][109/782] Loss_D: 0.8523 (0.9538) Loss_G: 0.7816 (0.9381) D(x): 0.5684 D(G(z)): 0.4686 / 0.3893 Acc: 15.6250 (19.4910)\n",
      "[1/200][110/782] Loss_D: 0.6229 (0.9535) Loss_G: 1.1745 (0.9383) D(x): 0.5976 D(G(z)): 0.4476 / 0.2696 Acc: 26.5625 (19.4989)\n",
      "[1/200][111/782] Loss_D: 0.5450 (0.9530) Loss_G: 0.6771 (0.9381) D(x): 0.5408 D(G(z)): 0.3275 / 0.3731 Acc: 18.7500 (19.4980)\n",
      "[1/200][112/782] Loss_D: 0.3883 (0.9524) Loss_G: 0.8084 (0.9379) D(x): 0.6130 D(G(z)): 0.3696 / 0.3582 Acc: 31.2500 (19.5112)\n",
      "[1/200][113/782] Loss_D: 0.5798 (0.9520) Loss_G: 0.7826 (0.9377) D(x): 0.6158 D(G(z)): 0.5059 / 0.3523 Acc: 37.5000 (19.5312)\n",
      "[1/200][114/782] Loss_D: 0.7353 (0.9517) Loss_G: 0.8774 (0.9377) D(x): 0.6087 D(G(z)): 0.4382 / 0.3394 Acc: 14.0625 (19.5252)\n",
      "[1/200][115/782] Loss_D: 0.8464 (0.9516) Loss_G: 0.6834 (0.9374) D(x): 0.5252 D(G(z)): 0.4156 / 0.4174 Acc: 25.0000 (19.5312)\n",
      "[1/200][116/782] Loss_D: 0.9404 (0.9516) Loss_G: 0.6019 (0.9370) D(x): 0.5676 D(G(z)): 0.5350 / 0.4287 Acc: 23.4375 (19.5356)\n",
      "[1/200][117/782] Loss_D: 1.0074 (0.9517) Loss_G: 0.7948 (0.9369) D(x): 0.5477 D(G(z)): 0.4999 / 0.3763 Acc: 15.6250 (19.5312)\n",
      "[1/200][118/782] Loss_D: 0.9830 (0.9517) Loss_G: 0.5271 (0.9364) D(x): 0.4776 D(G(z)): 0.5301 / 0.4405 Acc: 21.8750 (19.5339)\n",
      "[1/200][119/782] Loss_D: 0.9742 (0.9517) Loss_G: 0.5292 (0.9360) D(x): 0.4689 D(G(z)): 0.4939 / 0.4311 Acc: 18.7500 (19.5330)\n",
      "[1/200][120/782] Loss_D: 1.1104 (0.9519) Loss_G: 0.6476 (0.9356) D(x): 0.5467 D(G(z)): 0.5829 / 0.4194 Acc: 21.8750 (19.5356)\n",
      "[1/200][121/782] Loss_D: 1.1929 (0.9522) Loss_G: 0.5556 (0.9352) D(x): 0.4471 D(G(z)): 0.5131 / 0.4511 Acc: 20.3125 (19.5364)\n",
      "[1/200][122/782] Loss_D: 1.0504 (0.9523) Loss_G: 0.8035 (0.9351) D(x): 0.4934 D(G(z)): 0.5094 / 0.4051 Acc: 29.6875 (19.5477)\n",
      "[1/200][123/782] Loss_D: 1.1340 (0.9525) Loss_G: 0.7833 (0.9349) D(x): 0.5313 D(G(z)): 0.5342 / 0.4026 Acc: 15.6250 (19.5433)\n",
      "[1/200][124/782] Loss_D: 0.9702 (0.9525) Loss_G: 0.8495 (0.9348) D(x): 0.4545 D(G(z)): 0.4436 / 0.3315 Acc: 15.6250 (19.5390)\n",
      "[1/200][125/782] Loss_D: 0.7233 (0.9522) Loss_G: 0.7848 (0.9346) D(x): 0.5503 D(G(z)): 0.4713 / 0.3650 Acc: 26.5625 (19.5467)\n",
      "[1/200][126/782] Loss_D: 0.9043 (0.9522) Loss_G: 0.8467 (0.9345) D(x): 0.4769 D(G(z)): 0.4557 / 0.3542 Acc: 28.1250 (19.5562)\n",
      "[1/200][127/782] Loss_D: 0.5606 (0.9518) Loss_G: 0.6455 (0.9342) D(x): 0.5784 D(G(z)): 0.4156 / 0.3852 Acc: 25.0000 (19.5622)\n",
      "[1/200][128/782] Loss_D: 0.6548 (0.9514) Loss_G: 0.7925 (0.9341) D(x): 0.5560 D(G(z)): 0.4551 / 0.3139 Acc: 23.4375 (19.5664)\n",
      "[1/200][129/782] Loss_D: 0.7475 (0.9512) Loss_G: 0.6856 (0.9338) D(x): 0.5081 D(G(z)): 0.3966 / 0.3886 Acc: 17.1875 (19.5638)\n",
      "[1/200][130/782] Loss_D: 0.7959 (0.9510) Loss_G: 0.5733 (0.9334) D(x): 0.5490 D(G(z)): 0.4707 / 0.4516 Acc: 28.1250 (19.5732)\n",
      "[1/200][131/782] Loss_D: 0.9193 (0.9510) Loss_G: 0.8591 (0.9333) D(x): 0.5900 D(G(z)): 0.5236 / 0.3514 Acc: 14.0625 (19.5671)\n",
      "[1/200][132/782] Loss_D: 0.7496 (0.9508) Loss_G: 0.6355 (0.9330) D(x): 0.5478 D(G(z)): 0.4267 / 0.4270 Acc: 20.3125 (19.5680)\n",
      "[1/200][133/782] Loss_D: 0.7566 (0.9506) Loss_G: 0.7770 (0.9328) D(x): 0.5092 D(G(z)): 0.4274 / 0.3920 Acc: 31.2500 (19.5807)\n",
      "[1/200][134/782] Loss_D: 0.9370 (0.9506) Loss_G: 0.7795 (0.9327) D(x): 0.5688 D(G(z)): 0.5167 / 0.3840 Acc: 25.0000 (19.5866)\n",
      "[1/200][135/782] Loss_D: 0.8516 (0.9504) Loss_G: 0.7635 (0.9325) D(x): 0.4542 D(G(z)): 0.4379 / 0.3653 Acc: 25.0000 (19.5925)\n",
      "[1/200][136/782] Loss_D: 0.9313 (0.9504) Loss_G: 0.6534 (0.9322) D(x): 0.4855 D(G(z)): 0.5164 / 0.4180 Acc: 35.9375 (19.6103)\n",
      "[1/200][137/782] Loss_D: 1.0487 (0.9505) Loss_G: 0.5629 (0.9318) D(x): 0.4765 D(G(z)): 0.4683 / 0.4770 Acc: 26.5625 (19.6179)\n",
      "[1/200][138/782] Loss_D: 0.7681 (0.9503) Loss_G: 0.6341 (0.9314) D(x): 0.5764 D(G(z)): 0.4442 / 0.4157 Acc: 14.0625 (19.6118)\n",
      "[1/200][139/782] Loss_D: 1.0744 (0.9505) Loss_G: 0.6256 (0.9311) D(x): 0.5411 D(G(z)): 0.5455 / 0.4210 Acc: 17.1875 (19.6092)\n",
      "[1/200][140/782] Loss_D: 1.0218 (0.9505) Loss_G: 0.4932 (0.9306) D(x): 0.5079 D(G(z)): 0.5141 / 0.4617 Acc: 18.7500 (19.6083)\n",
      "[1/200][141/782] Loss_D: 0.7810 (0.9504) Loss_G: 0.6325 (0.9303) D(x): 0.5044 D(G(z)): 0.4757 / 0.4148 Acc: 29.6875 (19.6192)\n",
      "[1/200][142/782] Loss_D: 0.6863 (0.9501) Loss_G: 0.7522 (0.9301) D(x): 0.5549 D(G(z)): 0.4338 / 0.3465 Acc: 21.8750 (19.6216)\n",
      "[1/200][143/782] Loss_D: 0.7415 (0.9499) Loss_G: 0.6585 (0.9298) D(x): 0.5984 D(G(z)): 0.5018 / 0.3929 Acc: 18.7500 (19.6207)\n",
      "[1/200][144/782] Loss_D: 0.7244 (0.9496) Loss_G: 0.7135 (0.9296) D(x): 0.5200 D(G(z)): 0.3895 / 0.3661 Acc: 14.0625 (19.6147)\n",
      "[1/200][145/782] Loss_D: 0.6017 (0.9492) Loss_G: 0.6779 (0.9293) D(x): 0.6367 D(G(z)): 0.5028 / 0.3826 Acc: 21.8750 (19.6171)\n",
      "[1/200][146/782] Loss_D: 0.8697 (0.9491) Loss_G: 0.6083 (0.9290) D(x): 0.5330 D(G(z)): 0.4550 / 0.4619 Acc: 26.5625 (19.6246)\n",
      "[1/200][147/782] Loss_D: 0.9502 (0.9492) Loss_G: 0.6658 (0.9287) D(x): 0.5611 D(G(z)): 0.5385 / 0.3994 Acc: 25.0000 (19.6304)\n",
      "[1/200][148/782] Loss_D: 0.8132 (0.9490) Loss_G: 0.7660 (0.9285) D(x): 0.4676 D(G(z)): 0.4206 / 0.3532 Acc: 23.4375 (19.6345)\n",
      "[1/200][149/782] Loss_D: 0.9795 (0.9490) Loss_G: 0.6562 (0.9282) D(x): 0.5735 D(G(z)): 0.5169 / 0.4400 Acc: 21.8750 (19.6369)\n",
      "[1/200][150/782] Loss_D: 0.9712 (0.9491) Loss_G: 0.5751 (0.9279) D(x): 0.4805 D(G(z)): 0.5047 / 0.4275 Acc: 29.6875 (19.6476)\n",
      "[1/200][151/782] Loss_D: 0.9481 (0.9491) Loss_G: 0.4893 (0.9274) D(x): 0.4671 D(G(z)): 0.4919 / 0.4474 Acc: 29.6875 (19.6584)\n",
      "[1/200][152/782] Loss_D: 0.6687 (0.9488) Loss_G: 0.3873 (0.9268) D(x): 0.5775 D(G(z)): 0.5224 / 0.4690 Acc: 29.6875 (19.6691)\n",
      "[1/200][153/782] Loss_D: 0.8212 (0.9486) Loss_G: 0.4773 (0.9263) D(x): 0.5050 D(G(z)): 0.5120 / 0.4500 Acc: 31.2500 (19.6815)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][154/782] Loss_D: 1.1880 (0.9489) Loss_G: 0.4504 (0.9258) D(x): 0.4870 D(G(z)): 0.5814 / 0.4999 Acc: 21.8750 (19.6838)\n",
      "[1/200][155/782] Loss_D: 1.0359 (0.9490) Loss_G: 0.6082 (0.9255) D(x): 0.5427 D(G(z)): 0.5347 / 0.4358 Acc: 18.7500 (19.6828)\n",
      "[1/200][156/782] Loss_D: 0.9781 (0.9490) Loss_G: 0.7910 (0.9253) D(x): 0.4726 D(G(z)): 0.4602 / 0.3957 Acc: 23.4375 (19.6868)\n",
      "[1/200][157/782] Loss_D: 1.1889 (0.9493) Loss_G: 0.5578 (0.9249) D(x): 0.4867 D(G(z)): 0.4773 / 0.4820 Acc: 9.3750 (19.6759)\n",
      "[1/200][158/782] Loss_D: 0.9651 (0.9493) Loss_G: 0.4238 (0.9244) D(x): 0.5279 D(G(z)): 0.5383 / 0.4824 Acc: 15.6250 (19.6716)\n",
      "[1/200][159/782] Loss_D: 1.0187 (0.9493) Loss_G: 0.6062 (0.9241) D(x): 0.5039 D(G(z)): 0.4798 / 0.4404 Acc: 20.3125 (19.6722)\n",
      "[1/200][160/782] Loss_D: 0.6281 (0.9490) Loss_G: 0.4149 (0.9235) D(x): 0.5171 D(G(z)): 0.4024 / 0.4690 Acc: 23.4375 (19.6762)\n",
      "[1/200][161/782] Loss_D: 0.6723 (0.9487) Loss_G: 0.7330 (0.9233) D(x): 0.6082 D(G(z)): 0.5070 / 0.3684 Acc: 26.5625 (19.6835)\n",
      "[1/200][162/782] Loss_D: 0.6348 (0.9484) Loss_G: 0.8620 (0.9233) D(x): 0.5664 D(G(z)): 0.4096 / 0.3121 Acc: 17.1875 (19.6809)\n",
      "[1/200][163/782] Loss_D: 0.7614 (0.9482) Loss_G: 0.9921 (0.9233) D(x): 0.5635 D(G(z)): 0.4488 / 0.3164 Acc: 23.4375 (19.6849)\n",
      "[1/200][164/782] Loss_D: 0.8936 (0.9481) Loss_G: 0.6622 (0.9231) D(x): 0.4356 D(G(z)): 0.3638 / 0.4075 Acc: 20.3125 (19.6855)\n",
      "[1/200][165/782] Loss_D: 0.6554 (0.9478) Loss_G: 0.5420 (0.9227) D(x): 0.5849 D(G(z)): 0.4505 / 0.4659 Acc: 28.1250 (19.6944)\n",
      "[1/200][166/782] Loss_D: 0.7801 (0.9476) Loss_G: 0.5394 (0.9223) D(x): 0.6256 D(G(z)): 0.5287 / 0.4298 Acc: 20.3125 (19.6951)\n",
      "[1/200][167/782] Loss_D: 0.7497 (0.9474) Loss_G: 0.7068 (0.9220) D(x): 0.5323 D(G(z)): 0.4772 / 0.3863 Acc: 32.8125 (19.7089)\n",
      "[1/200][168/782] Loss_D: 0.8510 (0.9473) Loss_G: 0.7352 (0.9218) D(x): 0.4849 D(G(z)): 0.4343 / 0.3791 Acc: 26.5625 (19.7161)\n",
      "[1/200][169/782] Loss_D: 0.8129 (0.9472) Loss_G: 0.5821 (0.9215) D(x): 0.5309 D(G(z)): 0.4552 / 0.4258 Acc: 20.3125 (19.7167)\n",
      "[1/200][170/782] Loss_D: 0.7853 (0.9470) Loss_G: 0.5735 (0.9211) D(x): 0.5402 D(G(z)): 0.4642 / 0.4249 Acc: 26.5625 (19.7239)\n",
      "[1/200][171/782] Loss_D: 0.7745 (0.9468) Loss_G: 0.5100 (0.9207) D(x): 0.5001 D(G(z)): 0.4095 / 0.4438 Acc: 18.7500 (19.7229)\n",
      "[1/200][172/782] Loss_D: 0.8471 (0.9467) Loss_G: 0.5860 (0.9203) D(x): 0.5175 D(G(z)): 0.4597 / 0.4422 Acc: 28.1250 (19.7317)\n",
      "[1/200][173/782] Loss_D: 0.6178 (0.9464) Loss_G: 0.5337 (0.9199) D(x): 0.5282 D(G(z)): 0.4371 / 0.3886 Acc: 21.8750 (19.7339)\n",
      "[1/200][174/782] Loss_D: 0.9458 (0.9464) Loss_G: 0.6696 (0.9197) D(x): 0.5154 D(G(z)): 0.5287 / 0.4134 Acc: 29.6875 (19.7443)\n",
      "[1/200][175/782] Loss_D: 0.8209 (0.9463) Loss_G: 0.5324 (0.9193) D(x): 0.5086 D(G(z)): 0.5118 / 0.4277 Acc: 28.1250 (19.7531)\n",
      "[1/200][176/782] Loss_D: 0.7129 (0.9460) Loss_G: 0.4981 (0.9188) D(x): 0.4940 D(G(z)): 0.3859 / 0.4700 Acc: 28.1250 (19.7618)\n",
      "[1/200][177/782] Loss_D: 0.7383 (0.9458) Loss_G: 0.4719 (0.9184) D(x): 0.4799 D(G(z)): 0.4322 / 0.4639 Acc: 26.5625 (19.7689)\n",
      "[1/200][178/782] Loss_D: 0.7167 (0.9456) Loss_G: 0.4928 (0.9179) D(x): 0.5943 D(G(z)): 0.4806 / 0.4645 Acc: 20.3125 (19.7694)\n",
      "[1/200][179/782] Loss_D: 0.6394 (0.9452) Loss_G: 0.6192 (0.9176) D(x): 0.6117 D(G(z)): 0.4564 / 0.4029 Acc: 17.1875 (19.7668)\n",
      "[1/200][180/782] Loss_D: 0.5373 (0.9448) Loss_G: 0.6836 (0.9174) D(x): 0.5365 D(G(z)): 0.3781 / 0.3767 Acc: 23.4375 (19.7706)\n",
      "[1/200][181/782] Loss_D: 0.6084 (0.9445) Loss_G: 0.3742 (0.9168) D(x): 0.5256 D(G(z)): 0.4130 / 0.4785 Acc: 23.4375 (19.7744)\n",
      "[1/200][182/782] Loss_D: 0.7385 (0.9443) Loss_G: 0.6440 (0.9165) D(x): 0.5776 D(G(z)): 0.4725 / 0.4104 Acc: 23.4375 (19.7782)\n",
      "[1/200][183/782] Loss_D: 0.6580 (0.9440) Loss_G: 0.6119 (0.9162) D(x): 0.6099 D(G(z)): 0.4852 / 0.4206 Acc: 28.1250 (19.7868)\n",
      "[1/200][184/782] Loss_D: 0.5684 (0.9436) Loss_G: 0.5662 (0.9158) D(x): 0.5564 D(G(z)): 0.4366 / 0.4123 Acc: 28.1250 (19.7954)\n",
      "[1/200][185/782] Loss_D: 0.5951 (0.9432) Loss_G: 0.7149 (0.9156) D(x): 0.5564 D(G(z)): 0.4289 / 0.3605 Acc: 21.8750 (19.7976)\n",
      "[1/200][186/782] Loss_D: 0.7515 (0.9430) Loss_G: 0.5373 (0.9152) D(x): 0.5208 D(G(z)): 0.4297 / 0.4623 Acc: 26.5625 (19.8046)\n",
      "[1/200][187/782] Loss_D: 0.8146 (0.9429) Loss_G: 0.5122 (0.9148) D(x): 0.5112 D(G(z)): 0.4801 / 0.4579 Acc: 25.0000 (19.8099)\n",
      "[1/200][188/782] Loss_D: 0.6755 (0.9426) Loss_G: 0.5676 (0.9145) D(x): 0.6349 D(G(z)): 0.5063 / 0.4358 Acc: 25.0000 (19.8153)\n",
      "[1/200][189/782] Loss_D: 0.7720 (0.9424) Loss_G: 0.6726 (0.9142) D(x): 0.5538 D(G(z)): 0.4955 / 0.3718 Acc: 25.0000 (19.8206)\n",
      "[1/200][190/782] Loss_D: 0.7623 (0.9422) Loss_G: 0.5674 (0.9139) D(x): 0.5240 D(G(z)): 0.4558 / 0.4008 Acc: 18.7500 (19.8195)\n",
      "[1/200][191/782] Loss_D: 0.8015 (0.9421) Loss_G: 0.6106 (0.9135) D(x): 0.4940 D(G(z)): 0.4174 / 0.4248 Acc: 23.4375 (19.8232)\n",
      "[1/200][192/782] Loss_D: 0.8517 (0.9420) Loss_G: 0.4613 (0.9131) D(x): 0.5315 D(G(z)): 0.4606 / 0.4791 Acc: 20.3125 (19.8237)\n",
      "[1/200][193/782] Loss_D: 0.6202 (0.9417) Loss_G: 0.5806 (0.9127) D(x): 0.5955 D(G(z)): 0.4694 / 0.3950 Acc: 21.8750 (19.8258)\n",
      "[1/200][194/782] Loss_D: 0.7757 (0.9415) Loss_G: 0.7192 (0.9125) D(x): 0.6075 D(G(z)): 0.4956 / 0.3917 Acc: 20.3125 (19.8263)\n",
      "[1/200][195/782] Loss_D: 0.7582 (0.9413) Loss_G: 0.4788 (0.9121) D(x): 0.4878 D(G(z)): 0.4109 / 0.4534 Acc: 21.8750 (19.8284)\n",
      "[1/200][196/782] Loss_D: 0.8794 (0.9413) Loss_G: 0.5311 (0.9117) D(x): 0.4995 D(G(z)): 0.4850 / 0.4452 Acc: 26.5625 (19.8353)\n",
      "[1/200][197/782] Loss_D: 1.1314 (0.9415) Loss_G: 0.5839 (0.9114) D(x): 0.4788 D(G(z)): 0.4939 / 0.4575 Acc: 12.5000 (19.8278)\n",
      "[1/200][198/782] Loss_D: 0.8836 (0.9414) Loss_G: 0.6417 (0.9111) D(x): 0.5463 D(G(z)): 0.4880 / 0.4364 Acc: 18.7500 (19.8267)\n",
      "[1/200][199/782] Loss_D: 0.5280 (0.9410) Loss_G: 0.6149 (0.9108) D(x): 0.5722 D(G(z)): 0.4238 / 0.4177 Acc: 32.8125 (19.8399)\n",
      "[1/200][200/782] Loss_D: 0.9525 (0.9410) Loss_G: 0.5930 (0.9105) D(x): 0.5220 D(G(z)): 0.4912 / 0.4287 Acc: 17.1875 (19.8372)\n",
      "Label for eval = [8 0 5 7 3 1 1 3 6 8 1 5 2 5 4 7 8 4 7 1 6 1 8 4 0 0 4 9 9 6 9 9 9 9 1 6 8\n",
      " 4 7 7 0 1 6 7 8 5 8 9 7 3 5 7 9 0 8 0 4 0 4 9 1 1 0 2]\n",
      "[1/200][201/782] Loss_D: 0.4363 (0.9405) Loss_G: 0.4613 (0.9100) D(x): 0.5730 D(G(z)): 0.3984 / 0.4188 Acc: 21.8750 (19.8393)\n",
      "[1/200][202/782] Loss_D: 0.7037 (0.9402) Loss_G: 0.5603 (0.9097) D(x): 0.5691 D(G(z)): 0.4740 / 0.4025 Acc: 18.7500 (19.8382)\n",
      "[1/200][203/782] Loss_D: 0.7503 (0.9400) Loss_G: 0.5000 (0.9093) D(x): 0.5495 D(G(z)): 0.4896 / 0.4420 Acc: 20.3125 (19.8387)\n",
      "[1/200][204/782] Loss_D: 0.5554 (0.9396) Loss_G: 0.6245 (0.9090) D(x): 0.5499 D(G(z)): 0.4216 / 0.4254 Acc: 39.0625 (19.8582)\n",
      "[1/200][205/782] Loss_D: 0.6249 (0.9393) Loss_G: 0.6352 (0.9087) D(x): 0.6021 D(G(z)): 0.4166 / 0.4095 Acc: 15.6250 (19.8539)\n",
      "[1/200][206/782] Loss_D: 0.8665 (0.9393) Loss_G: 0.7500 (0.9085) D(x): 0.5430 D(G(z)): 0.5233 / 0.3924 Acc: 32.8125 (19.8670)\n",
      "[1/200][207/782] Loss_D: 0.6671 (0.9390) Loss_G: 0.8328 (0.9085) D(x): 0.5172 D(G(z)): 0.3880 / 0.3278 Acc: 20.3125 (19.8674)\n",
      "[1/200][208/782] Loss_D: 0.4452 (0.9385) Loss_G: 0.4847 (0.9080) D(x): 0.5014 D(G(z)): 0.3534 / 0.4145 Acc: 23.4375 (19.8710)\n",
      "[1/200][209/782] Loss_D: 0.4198 (0.9380) Loss_G: 0.5548 (0.9077) D(x): 0.6186 D(G(z)): 0.4049 / 0.4321 Acc: 31.2500 (19.8825)\n",
      "[1/200][210/782] Loss_D: 0.7966 (0.9378) Loss_G: 0.5765 (0.9073) D(x): 0.5946 D(G(z)): 0.4682 / 0.4410 Acc: 10.9375 (19.8735)\n",
      "[1/200][211/782] Loss_D: 0.5932 (0.9375) Loss_G: 0.7119 (0.9071) D(x): 0.6069 D(G(z)): 0.4366 / 0.3847 Acc: 28.1250 (19.8818)\n",
      "[1/200][212/782] Loss_D: 0.7850 (0.9373) Loss_G: 0.7107 (0.9069) D(x): 0.5495 D(G(z)): 0.4748 / 0.3779 Acc: 20.3125 (19.8822)\n",
      "[1/200][213/782] Loss_D: 0.6053 (0.9370) Loss_G: 0.5858 (0.9066) D(x): 0.5207 D(G(z)): 0.3772 / 0.4148 Acc: 23.4375 (19.8858)\n",
      "[1/200][214/782] Loss_D: 0.3635 (0.9364) Loss_G: 0.7436 (0.9065) D(x): 0.6080 D(G(z)): 0.4195 / 0.3530 Acc: 32.8125 (19.8988)\n",
      "[1/200][215/782] Loss_D: 0.5974 (0.9361) Loss_G: 0.6197 (0.9062) D(x): 0.6187 D(G(z)): 0.4694 / 0.4001 Acc: 20.3125 (19.8992)\n",
      "[1/200][216/782] Loss_D: 0.5556 (0.9357) Loss_G: 0.8856 (0.9061) D(x): 0.6050 D(G(z)): 0.4042 / 0.3406 Acc: 23.4375 (19.9027)\n",
      "[1/200][217/782] Loss_D: 0.7478 (0.9355) Loss_G: 0.4529 (0.9057) D(x): 0.5031 D(G(z)): 0.3792 / 0.4840 Acc: 15.6250 (19.8984)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/200][218/782] Loss_D: 0.6224 (0.9352) Loss_G: 0.6971 (0.9055) D(x): 0.6305 D(G(z)): 0.4694 / 0.3807 Acc: 18.7500 (19.8973)\n",
      "[1/200][219/782] Loss_D: 0.6275 (0.9349) Loss_G: 0.7472 (0.9053) D(x): 0.5323 D(G(z)): 0.4096 / 0.3676 Acc: 28.1250 (19.9055)\n",
      "[1/200][220/782] Loss_D: 0.9234 (0.9349) Loss_G: 0.6329 (0.9051) D(x): 0.5384 D(G(z)): 0.5026 / 0.4089 Acc: 21.8750 (19.9075)\n",
      "[1/200][221/782] Loss_D: 0.8493 (0.9348) Loss_G: 0.8294 (0.9050) D(x): 0.5875 D(G(z)): 0.4777 / 0.3864 Acc: 15.6250 (19.9032)\n",
      "[1/200][222/782] Loss_D: 0.8516 (0.9347) Loss_G: 0.6145 (0.9047) D(x): 0.4690 D(G(z)): 0.4428 / 0.4075 Acc: 21.8750 (19.9052)\n",
      "[1/200][223/782] Loss_D: 0.9829 (0.9348) Loss_G: 0.6954 (0.9045) D(x): 0.5239 D(G(z)): 0.4702 / 0.4480 Acc: 23.4375 (19.9087)\n",
      "[1/200][224/782] Loss_D: 0.6550 (0.9345) Loss_G: 0.6749 (0.9043) D(x): 0.5405 D(G(z)): 0.4195 / 0.3546 Acc: 18.7500 (19.9075)\n",
      "[1/200][225/782] Loss_D: 1.0293 (0.9346) Loss_G: 0.5813 (0.9039) D(x): 0.4670 D(G(z)): 0.5175 / 0.4289 Acc: 23.4375 (19.9110)\n",
      "[1/200][226/782] Loss_D: 1.0298 (0.9347) Loss_G: 0.5561 (0.9036) D(x): 0.4911 D(G(z)): 0.5165 / 0.4228 Acc: 21.8750 (19.9130)\n",
      "[1/200][227/782] Loss_D: 1.0104 (0.9347) Loss_G: 0.5412 (0.9032) D(x): 0.4458 D(G(z)): 0.4904 / 0.4537 Acc: 23.4375 (19.9165)\n",
      "[1/200][228/782] Loss_D: 0.8307 (0.9346) Loss_G: 0.5783 (0.9029) D(x): 0.5298 D(G(z)): 0.4819 / 0.4286 Acc: 23.4375 (19.9199)\n",
      "[1/200][229/782] Loss_D: 0.5867 (0.9343) Loss_G: 0.6885 (0.9027) D(x): 0.5558 D(G(z)): 0.4860 / 0.3595 Acc: 32.8125 (19.9327)\n",
      "[1/200][230/782] Loss_D: 0.8538 (0.9342) Loss_G: 0.8408 (0.9026) D(x): 0.5203 D(G(z)): 0.4690 / 0.3547 Acc: 25.0000 (19.9377)\n",
      "[1/200][231/782] Loss_D: 0.6957 (0.9340) Loss_G: 0.8619 (0.9026) D(x): 0.5554 D(G(z)): 0.4449 / 0.3370 Acc: 23.4375 (19.9411)\n",
      "[1/200][232/782] Loss_D: 0.5970 (0.9336) Loss_G: 0.5924 (0.9023) D(x): 0.5481 D(G(z)): 0.4230 / 0.4079 Acc: 28.1250 (19.9492)\n",
      "[1/200][233/782] Loss_D: 0.6763 (0.9334) Loss_G: 0.7161 (0.9021) D(x): 0.5578 D(G(z)): 0.4247 / 0.3725 Acc: 18.7500 (19.9480)\n",
      "[1/200][234/782] Loss_D: 0.6617 (0.9331) Loss_G: 0.8682 (0.9021) D(x): 0.5658 D(G(z)): 0.4575 / 0.3272 Acc: 29.6875 (19.9576)\n",
      "[1/200][235/782] Loss_D: 0.7348 (0.9329) Loss_G: 0.8360 (0.9020) D(x): 0.6082 D(G(z)): 0.4897 / 0.3491 Acc: 21.8750 (19.9595)\n",
      "[1/200][236/782] Loss_D: 0.8277 (0.9328) Loss_G: 0.9110 (0.9020) D(x): 0.5288 D(G(z)): 0.4488 / 0.3274 Acc: 26.5625 (19.9660)\n",
      "[1/200][237/782] Loss_D: 0.8322 (0.9327) Loss_G: 0.5644 (0.9017) D(x): 0.4634 D(G(z)): 0.3817 / 0.4403 Acc: 20.3125 (19.9663)\n",
      "[1/200][238/782] Loss_D: 0.8296 (0.9326) Loss_G: 0.2916 (0.9011) D(x): 0.5381 D(G(z)): 0.4936 / 0.5456 Acc: 29.6875 (19.9758)\n",
      "[1/200][239/782] Loss_D: 0.9884 (0.9327) Loss_G: 0.5368 (0.9007) D(x): 0.5460 D(G(z)): 0.5194 / 0.4622 Acc: 17.1875 (19.9731)\n",
      "[1/200][240/782] Loss_D: 0.7976 (0.9325) Loss_G: 0.2891 (0.9001) D(x): 0.4818 D(G(z)): 0.5020 / 0.4954 Acc: 25.0000 (19.9780)\n",
      "[1/200][241/782] Loss_D: 0.6851 (0.9323) Loss_G: 0.7257 (0.9000) D(x): 0.5840 D(G(z)): 0.4743 / 0.3641 Acc: 25.0000 (19.9829)\n",
      "[1/200][242/782] Loss_D: 0.7408 (0.9321) Loss_G: 0.6286 (0.8997) D(x): 0.5487 D(G(z)): 0.4639 / 0.4117 Acc: 25.0000 (19.9878)\n",
      "[1/200][243/782] Loss_D: 0.9474 (0.9321) Loss_G: 0.5486 (0.8994) D(x): 0.4637 D(G(z)): 0.4612 / 0.4305 Acc: 21.8750 (19.9896)\n",
      "[1/200][244/782] Loss_D: 0.7502 (0.9320) Loss_G: 0.3756 (0.8988) D(x): 0.5094 D(G(z)): 0.4649 / 0.4866 Acc: 28.1250 (19.9976)\n",
      "[1/200][245/782] Loss_D: 0.9262 (0.9320) Loss_G: 0.6517 (0.8986) D(x): 0.5445 D(G(z)): 0.5519 / 0.3941 Acc: 25.0000 (20.0024)\n",
      "[1/200][246/782] Loss_D: 0.6470 (0.9317) Loss_G: 0.4766 (0.8982) D(x): 0.5447 D(G(z)): 0.4747 / 0.4294 Acc: 23.4375 (20.0058)\n",
      "[1/200][247/782] Loss_D: 0.8271 (0.9316) Loss_G: 0.5488 (0.8979) D(x): 0.4904 D(G(z)): 0.3973 / 0.4418 Acc: 18.7500 (20.0046)\n",
      "[1/200][248/782] Loss_D: 0.7605 (0.9314) Loss_G: 0.5668 (0.8975) D(x): 0.5986 D(G(z)): 0.5234 / 0.3965 Acc: 18.7500 (20.0033)\n",
      "[1/200][249/782] Loss_D: 0.9201 (0.9314) Loss_G: 0.7936 (0.8974) D(x): 0.5088 D(G(z)): 0.4479 / 0.3753 Acc: 14.0625 (19.9976)\n",
      "[1/200][250/782] Loss_D: 0.7770 (0.9312) Loss_G: 0.6802 (0.8972) D(x): 0.4912 D(G(z)): 0.4543 / 0.4160 Acc: 28.1250 (20.0054)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-00f2d34846bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdis_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0maux_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdis_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maux_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdis_errD_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdis_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdis_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdis_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/umangsharma/CIFAR_DCGAN/ACGAN-PyTorch/network.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mfc_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_aux\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mrealfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc_dis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrealfake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/py27/lib/python2.7/site-packages/torch/nn/modules/activation.pyc\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu, label = data\n",
    "        batch_size = real_cpu.size(0)\n",
    "        if cuda:\n",
    "            real_cpu = real_cpu.cuda()\n",
    "        with torch.no_grad():\n",
    "            input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "            dis_label.resize_(batch_size).fill_(real_label)\n",
    "            aux_label.resize_(batch_size).copy_(label)\n",
    "        dis_output, aux_output = netD(input)\n",
    "\n",
    "        dis_errD_real = dis_criterion(dis_output, dis_label)\n",
    "        aux_errD_real = aux_criterion(aux_output, aux_label)\n",
    "        errD_real = dis_errD_real + aux_errD_real\n",
    "        errD_real.backward()\n",
    "        D_x = dis_output.data.mean()\n",
    "\n",
    "        # compute the current classification accuracy\n",
    "        accuracy = compute_acc(aux_output, aux_label)\n",
    "\n",
    "        # train with fake\n",
    "        with torch.no_grad() :\n",
    "            noise.resize_(batch_size, nz, 1, 1).normal_(0, 1)\n",
    "        label = np.random.randint(0, num_classes, batch_size)\n",
    "        noise_ = np.random.normal(0, 1, (batch_size, nz))\n",
    "        class_onehot = np.zeros((batch_size, num_classes))\n",
    "        class_onehot[np.arange(batch_size), label] = 1\n",
    "        noise_[np.arange(batch_size), :num_classes] = class_onehot[np.arange(batch_size)]\n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        with torch.no_grad():\n",
    "            noise.data.copy_(noise_.view(batch_size, nz, 1, 1))\n",
    "            aux_label.resize_(batch_size).copy_(torch.from_numpy(label))\n",
    "            dis_label.fill_(fake_label)\n",
    "\n",
    "\n",
    "        fake = netG(noise)\n",
    "        dis_output, aux_output = netD(fake.detach())\n",
    "        dis_errD_fake = dis_criterion(dis_output, dis_label)\n",
    "        aux_errD_fake = aux_criterion(aux_output, aux_label)\n",
    "        errD_fake = dis_errD_fake + aux_errD_fake\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = dis_output.data.mean()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        dis_label.data.fill_(real_label)  # fake labels are real for generator cost\n",
    "        dis_output, aux_output = netD(fake)\n",
    "        dis_errG = dis_criterion(dis_output, dis_label)\n",
    "        aux_errG = aux_criterion(aux_output, aux_label)\n",
    "        errG = dis_errG + aux_errG\n",
    "        errG.backward()\n",
    "        D_G_z2 = dis_output.data.mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # compute the average loss\n",
    "        curr_iter = epoch * len(dataloader) + i\n",
    "        all_loss_G = avg_loss_G * curr_iter\n",
    "        all_loss_D = avg_loss_D * curr_iter\n",
    "        all_loss_A = avg_loss_A * curr_iter\n",
    "        all_loss_G += errG.data\n",
    "        all_loss_D += errD.data\n",
    "        all_loss_A += accuracy\n",
    "        avg_loss_G = all_loss_G / (curr_iter + 1)\n",
    "        avg_loss_D = all_loss_D / (curr_iter + 1)\n",
    "        avg_loss_A = all_loss_A / (curr_iter + 1)\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f (%.4f) Loss_G: %.4f (%.4f) D(x): %.4f D(G(z)): %.4f / %.4f Acc: %.4f (%.4f)'\n",
    "              % (epoch, niter, i, len(dataloader),\n",
    "                 errD.data, avg_loss_D, errG.data, avg_loss_G, D_x, D_G_z1, D_G_z2, accuracy, avg_loss_A))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(\n",
    "                real_cpu, '%s/real_samples.png' % outf)\n",
    "            print('Label for eval = {}'.format(eval_label))\n",
    "            fake = netG(eval_noise)\n",
    "            vutils.save_image(\n",
    "                fake.data,\n",
    "                '%s/fake_samples_epoch_%03d.png' % (outf, epoch)\n",
    "            )\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save(netG.state_dict(), '%s/netG.pth' % (outf))\n",
    "    torch.save(netD.state_dict(), '%s/netD.pth' % (outf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(G, num_samples) :\n",
    "    with torch.no_grad() :\n",
    "        noise = torch.FloatTensor(num_samples, nz, 1, 1).normal_(0, 1)\n",
    "        noise = noise.cuda()\n",
    "        noise = Variable(noise)\n",
    "        noise_ = np.random.normal(0, 1, (num_samples, nz))\n",
    "        random_label = np.random.randint(0, 10, num_samples)\n",
    "    #     label = np.random.randint(0, nb_label, batchSize)\n",
    "        label_onehot = np.zeros((num_samples, num_classes))\n",
    "        label_onehot[np.arange(num_samples), random_label] = 1\n",
    "        noise_[np.arange(num_samples), :num_classes] = label_onehot[np.arange(num_samples)]\n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise_ = noise_.resize_(num_samples, nz, 1, 1)\n",
    "        noise.copy_(noise_)\n",
    "        imgs = G(noise)\n",
    "        \n",
    "        \n",
    "        return imgs, random_label\n",
    "    \n",
    "def generate_single_img(G, label) :\n",
    "    with torch.no_grad() :\n",
    "        noise = torch.FloatTensor(1, nz, 1, 1).normal_(0, 1)\n",
    "        noise = noise.cuda()\n",
    "        noise = Variable(noise)\n",
    "        noise_ = np.random.normal(0, 1, (1, nz))\n",
    "        label_onehot = np.zeros(num_classes)\n",
    "        label_onehot[label] = 1\n",
    "        noise_[:,:num_classes] = label_onehot\n",
    "        noise_ = (torch.from_numpy(noise_))\n",
    "        noise_ = noise_.resize_(1, nz, 1, 1)\n",
    "        noise.copy_(noise_)\n",
    "        imgs = G(noise)\n",
    "        \n",
    "        \n",
    "        return imgs\n",
    "    \n",
    "def plt_images(images, rows, cols):\n",
    "    fig = plt.figure(figsize=(cols,rows))\n",
    "    fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0.05, hspace=0.05)\n",
    "    for i, x in enumerate(images[:cols * rows]):\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(np.rollaxis(x, 0, 3), norm=colors.NoNorm())\n",
    "    return fig\n",
    "\n",
    "def plt_single_gen_with_fixed_noise(G, img_num, class_gen):\n",
    "    plt_labels_sing = Variable(torch.zeros(100, 10).scatter_(1, torch.LongTensor([[class_gen]*10 for i in range(10)]).view(-1, 1), 1), volatile = True)\n",
    "    plt_z_sing = Variable(torch.randn(100, 100), volatile = True)\n",
    "    imgs = generate_single_img(G, class_gen)\n",
    "    fig = plt_images(imgs.data.cpu().numpy(), 1, 1)\n",
    "    if not os.path.exists(outf + '/img_gen'):\n",
    "        os.makedirs(outf + '/img_gen')\n",
    "    fig.savefig(outf + '/img_gen/%s/%s.png' % ((str(class_gen).zfill(3)), str(img_num).zfill(5)) )\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 32, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imgs, labels = generate_img(netG, 2)\n",
    "# print(imgs.shape)\n",
    "# fig = plt_images(imgs.data.cpu().numpy(), 2,2)\n",
    "# fig = plt_images(G(plt_z_sing, plt_labels_sing).data.cpu().numpy(), 1, 1)\n",
    "# if not os.path.exists(outf + '/img_gen'):\n",
    "#     os.makedirs(outf + '/img_gen')\n",
    "# fig.savefig(prefix + '/img_gen/%s/%s.png' % ((str(class_gen).zfill(3)), str(img_num).zfill(5)) )\n",
    "# plt.close(fig)\n",
    "generate_single_img(netG, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:47: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/opt/conda/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:48: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "num_images = 1000\n",
    "if not os.path.exists(outf + '/img_gen'):\n",
    "    os.makedirs(outf + '/img_gen')\n",
    "for j in range(10):\n",
    "    os.mkdir(outf + '/img_gen/' + str(j).zfill(3))\n",
    "    for i in range(num_images):\n",
    "        plt_single_gen_with_fixed_noise(netG,i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
